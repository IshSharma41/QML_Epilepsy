{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a93b61",
   "metadata": {},
   "source": [
    "# Data Loading and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919cb2ef",
   "metadata": {},
   "source": [
    "### library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image, input_data\n",
    "from nilearn.datasets import fetch_atlas_aal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pennylane as qml\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d22c5e",
   "metadata": {},
   "source": [
    "### Adding paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c76ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_pet_data_path = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\data\\processed\\mri_pet\\derivatives\"\n",
    "mri_data_path = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\data\\processed\\mri\\derivatives\\cleaned_skullstrip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c6985",
   "metadata": {},
   "source": [
    "### Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b63977",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='processing_log.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45931bc6",
   "metadata": {},
   "source": [
    "### Fetch the AAL Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a4643",
   "metadata": {},
   "outputs": [],
   "source": [
    "aal_atlas = fetch_atlas_aal()\n",
    "atlas_filename = aal_atlas.maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf0e23",
   "metadata": {},
   "source": [
    "### Function to extract ROI-based features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146db10",
   "metadata": {},
   "source": [
    "### Light imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead3ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import input_data\n",
    "from scipy.stats import skew, kurtosis # Add this import statement\n",
    "from skimage.feature import graycomatrix, graycoprops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1800d",
   "metadata": {},
   "source": [
    "## MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9aa48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_mri_features(mri_path, atlas_filename, surviving_labels=None):\n",
    "    try:\n",
    "        img = nib.load(mri_path)\n",
    "        masker = input_data.NiftiLabelsMasker(labels_img=atlas_filename, standardize=False)\n",
    "        roi_data = masker.fit_transform(img)\n",
    "\n",
    "        all_roi_features = []\n",
    "        if surviving_labels is None:\n",
    "            surviving_labels = range(roi_data.shape[1])\n",
    "\n",
    "        for i, roi_values in enumerate(roi_data.T):\n",
    "            if i in surviving_labels:\n",
    "                mean_val = np.mean(roi_values)\n",
    "                std_val = np.std(roi_values)\n",
    "                skew_val = np.skew(roi_values)\n",
    "                kurt_val = np.kurtosis(roi_values)\n",
    "\n",
    "                if roi_values.max() == roi_values.min():\n",
    "                    roi_features = [mean_val, std_val, skew_val, kurt_val, 0, 0, 0, 0]\n",
    "                else:\n",
    "                    roi_values_int = (roi_values - roi_values.min()) / (roi_values.max() - roi_values.min()) * 255\n",
    "                    roi_values_int = roi_values_int.astype(np.uint8)\n",
    "\n",
    "                    size = int(np.sqrt(len(roi_values_int)))\n",
    "                    if size * size != len(roi_values_int):\n",
    "                        size += 1\n",
    "                    reshaped_roi_values = np.pad(roi_values_int, (0, size * size - len(roi_values_int)), 'constant').reshape(size, size)\n",
    "\n",
    "                    glcm = graycomatrix(reshaped_roi_values, distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256, symmetric=True, normed=True)\n",
    "                    contrast = graycoprops(glcm, 'contrast').mean()\n",
    "                    correlation = graycoprops(glcm, 'correlation').mean()\n",
    "                    energy = graycoprops(glcm, 'energy').mean()\n",
    "                    homogeneity = graycoprops(glcm, 'homogeneity').mean()\n",
    "\n",
    "                    roi_features = [mean_val, std_val, skew_val, kurt_val, contrast, correlation, energy, homogeneity]\n",
    "                all_roi_features.extend(roi_features)\n",
    "\n",
    "        logging.info(f\"MRI features extracted from {mri_path}\")\n",
    "        return np.array(all_roi_features), surviving_labels\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing MRI file {mri_path}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b8d29",
   "metadata": {},
   "source": [
    "## PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e59a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_pet_features(pet_path, atlas_filename, surviving_labels=None):\n",
    "    try:\n",
    "        img = nib.load(pet_path)\n",
    "        masker = input_data.NiftiLabelsMasker(labels_img=atlas_filename, standardize=False)\n",
    "        roi_data = masker.fit_transform(img)\n",
    "\n",
    "        all_roi_features = []\n",
    "        if surviving_labels is None:\n",
    "            surviving_labels = range(roi_data.shape[1])\n",
    "\n",
    "        for i, roi_values in enumerate(roi_data.T):\n",
    "            if i in surviving_labels:\n",
    "                mean_val = np.mean(roi_values)\n",
    "                std_val = np.std(roi_values)\n",
    "                skew_val = np.skew(roi_values)\n",
    "                kurt_val = np.kurtosis(roi_values)\n",
    "\n",
    "                if roi_values.max() == roi_values.min():\n",
    "                    roi_features = [mean_val, std_val, skew_val, kurt_val, 0, 0, 0, 0]\n",
    "                else:\n",
    "                    roi_values_int = (roi_values - roi_values.min()) / (roi_values.max() - roi_values.min()) * 255\n",
    "                    roi_values_int = roi_values_int.astype(np.uint8)\n",
    "\n",
    "                    size = int(np.sqrt(len(roi_values_int)))\n",
    "                    if size * size != len(roi_values_int):\n",
    "                        size += 1\n",
    "                    reshaped_roi_values = np.pad(roi_values_int, (0, size * size - len(roi_values_int)), 'constant').reshape(size, size)\n",
    "\n",
    "                    glcm = graycomatrix(reshaped_roi_values, distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256, symmetric=True, normed=True)\n",
    "                    contrast = graycoprops(glcm, 'contrast').mean()\n",
    "                    correlation = graycoprops(glcm, 'correlation').mean()\n",
    "                    energy = graycoprops(glcm, 'energy').mean()\n",
    "                    homogeneity = graycoprops(glcm, 'homogeneity').mean()\n",
    "\n",
    "                    roi_features = [mean_val, std_val, skew_val, kurt_val, contrast, correlation, energy, homogeneity]\n",
    "                all_roi_features.extend(roi_features)\n",
    "\n",
    "        logging.info(f\"PET features extracted from {pet_path}\")\n",
    "        return np.array(all_roi_features), surviving_labels\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing PET file {pet_path}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb1975",
   "metadata": {},
   "source": [
    "### Find the reference MRI image for MRI-only data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b17050",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_mri_only_path = None\n",
    "max_rois_mri_only = 0\n",
    "\n",
    "for subject_folder in os.listdir(mri_data_path):\n",
    "    mri_path = os.path.join(mri_data_path, subject_folder, \"anat\", f\"{subject_folder}_T1w_skullstripped.nii.gz\")\n",
    "    img = nib.load(mri_path)\n",
    "    masker = input_data.NiftiLabelsMasker(labels_img=atlas_filename, standardize=False)\n",
    "    roi_data = masker.fit_transform(img)\n",
    "    num_rois = roi_data.shape[1]\n",
    "\n",
    "    if num_rois > max_rois_mri_only:\n",
    "        max_rois_mri_only = num_rois\n",
    "        reference_mri_only_path = mri_path\n",
    "\n",
    "_, surviving_mri_only_labels = extract_mri_features(reference_mri_only_path, atlas_filename)\n",
    "\n",
    "for subject_folder in os.listdir(os.path.join(mri_pet_data_path, \"cleaned_skullstrip\")):\n",
    "    mri_path = os.path.join(mri_pet_data_path, \"cleaned_skullstrip\", subject_folder, \"anat\", f\"{subject_folder}_T1w_skullstripped.nii.gz\")\n",
    "    pet_path = os.path.join(mri_pet_data_path, \"preprocessed_pet\", subject_folder, \"pet\", f\"{subject_folder}_space-MNI152NLin2009aPET.nii.gz\")\n",
    "\n",
    "    mri_features, _ = extract_mri_features(mri_path, atlas_filename, surviving_mri_pet_labels)\n",
    "    pet_features, _ = extract_pet_features(pet_path, atlas_filename, surviving_mri_pet_labels)\n",
    "\n",
    "    if mri_features is not None and pet_features is not None:\n",
    "        combined_features = np.concatenate((mri_features, pet_features))\n",
    "        mri_pet_features.append(combined_features)\n",
    "        mri_pet_labels.append(1 if \"patient\" in subject_folder else 0)\n",
    "\n",
    "mri_pet_features = np.array(mri_pet_features)\n",
    "mri_pet_labels = np.array(mri_pet_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f02d5",
   "metadata": {},
   "source": [
    "### Extract MRI-PET features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_pet_features = []\n",
    "mri_pet_labels = []\n",
    "surviving_mri_pet_labels = None\n",
    "\n",
    "for subject_folder in os.listdir(os.path.join(mri_pet_data_path, \"cleaned_skullstrip\")):\n",
    "    mri_path = os.path.join(mri_pet_data_path, \"cleaned_skullstrip\", subject_folder, \"anat\", f\"{subject_folder}_T1w_skullstripped.nii.gz\")\n",
    "    pet_path = os.path.join(mri_pet_data_path, \"preprocessed_pet\", subject_folder, \"pet\", f\"{subject_folder}_space-MNI152NLin2009aPET.nii.gz\")\n",
    "\n",
    "    mri_features, surviving_mri_pet_labels = extract_mri_features(mri_path, atlas_filename, surviving_mri_pet_labels)\n",
    "    pet_features, surviving_mri_pet_labels = extract_pet_features(pet_path, atlas_filename, surviving_mri_pet_labels)\n",
    "\n",
    "    combined_features = np.concatenate((mri_features, pet_features))\n",
    "    mri_pet_features.append(combined_features)\n",
    "    mri_pet_labels.append(1 if \"patient\" in subject_folder else 0)\n",
    "\n",
    "mri_pet_features = np.array(mri_pet_features)\n",
    "mri_pet_labels = np.array(mri_pet_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc3f0d",
   "metadata": {},
   "source": [
    "### Extract MRI-only features and corrected labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_only_features = []\n",
    "mri_only_labels = []\n",
    "surviving_mri_only_labels = None\n",
    "\n",
    "for subject_folder in os.listdir(mri_data_path):\n",
    "    mri_path = os.path.join(mri_data_path, subject_folder, \"anat\", f\"{subject_folder}_T1w_skullstripped.nii.gz\")\n",
    "    mri_features, surviving_mri_only_labels = extract_mri_features(mri_path, atlas_filename, surviving_mri_only_labels)\n",
    "    mri_only_features.append(mri_features)\n",
    "    subject_id = int(subject_folder.replace(\"sub-\", \"\"))\n",
    "    mri_only_labels.append(0 if subject_id > 4000 else 1)\n",
    "\n",
    "mri_only_features = np.array(mri_only_features)\n",
    "mri_only_labels = np.array(mri_only_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b667828",
   "metadata": {},
   "source": [
    "## Saving intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a915f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ... (Feature extraction code) ...\n",
    "\n",
    "# Save MRI-PET features and labels\n",
    "np.save(\"mri_pet_features.npy\", mri_pet_features)\n",
    "np.save(\"mri_pet_labels.npy\", mri_pet_labels)\n",
    "\n",
    "# Save MRI-only features and labels\n",
    "np.save(\"mri_only_features.npy\", mri_only_features)\n",
    "np.save(\"mri_only_labels.npy\", mri_only_labels)\n",
    "\n",
    "#Save the training and testing sets\n",
    "np.save(\"X_train_mri_pet.npy\", X_train_mri_pet)\n",
    "np.save(\"X_test_mri_pet.npy\", X_test_mri_pet)\n",
    "np.save(\"y_train_mri_pet.npy\", y_train_mri_pet)\n",
    "np.save(\"y_test_mri_pet.npy\", y_test_mri_pet)\n",
    "\n",
    "#Or using pandas\n",
    "df_mri_pet = pd.DataFrame(np.concatenate((mri_pet_features, mri_pet_labels.reshape(-1,1)), axis = 1))\n",
    "df_mri_pet.to_csv(\"mri_pet_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0104a3",
   "metadata": {},
   "source": [
    "# Quantum Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Function for PCA and angle encoding\n",
    "def quantum_encode(features, n_qubits):\n",
    "    pca = PCA(n_components=n_qubits)\n",
    "    reduced_features = pca.fit_transform(features)\n",
    "    normalized_features = reduced_features / np.linalg.norm(reduced_features, axis=1, keepdims=True)\n",
    "\n",
    "    angles = np.arcsin(normalized_features)\n",
    "    return angles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87d479",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb37c7",
   "metadata": {},
   "source": [
    "### Classical MRI and PET Encoders (Simple Neural Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ClassicalEncoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04660c87",
   "metadata": {},
   "source": [
    "### Quantum Variational Circuit (QVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1828ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qvc(inputs, weights):\n",
    "    n_qubits = len(inputs)\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(inputs[i], wires=i)\n",
    "    for i in range(n_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(n_qubits):\n",
    "        qml.Rot(weights[i, 0], weights[i, 1], weights[i, 2], wires=i)\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afbac9b",
   "metadata": {},
   "source": [
    "### Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57680202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, mri_input_size, pet_input_size, hidden_size, n_qubits):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.mri_encoder = ClassicalEncoder(mri_input_size, hidden_size)\n",
    "        self.pet_encoder = ClassicalEncoder(pet_input_size, hidden_size)\n",
    "        self.dev = qml.device('default.qubit', wires=n_qubits)\n",
    "        self.qnode = qml.QNode(qvc, self.dev)\n",
    "        self.weights = torch.nn.Parameter(torch.rand(n_qubits, 3))\n",
    "        self.fc_final = nn.Linear(n_qubits, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, mri_inputs, pet_inputs):\n",
    "        mri_encoded = self.mri_encoder(mri_inputs)\n",
    "        pet_encoded = self.pet_encoder(pet_inputs)\n",
    "        combined = torch.cat((mri_encoded, pet_encoded), dim=1)\n",
    "        quantum_inputs = quantum_encode(combined.detach().numpy(), len(self.weights))\n",
    "        quantum_inputs = torch.tensor(quantum_inputs, dtype=torch.float32)\n",
    "        quantum_output = torch.tensor(self.qnode(quantum_inputs), dtype=torch.float32)\n",
    "        output = self.fc_final(quantum_output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ef0c4",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb6b19",
   "metadata": {},
   "source": [
    "### light imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb8e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da065bf2",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_input_size = mri_only_features.shape[1]\n",
    "pet_input_size = mri_pet_features.shape[1] // 2\n",
    "hidden_size = 64\n",
    "n_qubits = 10\n",
    "learning_rate = 0.001\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011f304",
   "metadata": {},
   "source": [
    "# Saving Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11de6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ... (Hyperparameter definitions) ...\n",
    "\n",
    "hyperparameters = {\n",
    "    \"mri_input_size\": mri_input_size,\n",
    "    \"pet_input_size\": pet_input_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"n_qubits\": n_qubits,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": epochs,\n",
    "    # Add other hyperparameters\n",
    "}\n",
    "\n",
    "with open(\"hyperparameters.json\", \"w\") as f:\n",
    "    json.dump(hyperparameters, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31ea2b",
   "metadata": {},
   "source": [
    "### Initialize Model, Loss Function, and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cdf029",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HybridModel(mri_input_size, pet_input_size, hidden_size, n_qubits)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6c040d",
   "metadata": {},
   "source": [
    "## Pretrain MRI Encoder (using MRI-only data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d79f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_encoder_optimizer = optim.Adam(model.mri_encoder.parameters(), lr=learning_rate)\n",
    "for epoch in range(epochs):\n",
    "    model.mri_encoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    mri_inputs = torch.tensor(mri_only_features, dtype=torch.float32)\n",
    "    mri_outputs = model.mri_encoder(mri_inputs)\n",
    "    loss = criterion(mri_outputs.mean(axis=1).unsqueeze(1), torch.tensor(mri_only_labels, dtype=torch.float32).unsqueeze(1))\n",
    "    loss.backward()\n",
    "    mri_encoder_optimizer.step()\n",
    "    print(f\"Pretrain MRI Encoder Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd722722",
   "metadata": {},
   "source": [
    "## Fusion Training (MRI-PET data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d1ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    mri_inputs = torch.tensor(X_train_mri_pet[:, :pet_input_size], dtype=torch.float32)\n",
    "    pet_inputs = torch.tensor(X_train_mri_pet[:, pet_input_size:], dtype=torch.float32)\n",
    "    outputs = model(mri_inputs, pet_inputs)\n",
    "    loss = criterion(outputs, torch.tensor(y_train_mri_pet, dtype=torch.float32).unsqueeze(1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Fusion Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea9a28",
   "metadata": {},
   "source": [
    "## Saving Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fad666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (Model training code) ...\n",
    "\n",
    "train_losses = []\n",
    "#Pretrain MRI Encoder\n",
    "for epoch in range(epochs):\n",
    "    # ... (training code) ...\n",
    "    train_losses.append(loss.item())\n",
    "#Fusion Training\n",
    "for epoch in range(epochs):\n",
    "    # ... (training code) ...\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "np.save(\"train_losses.npy\", np.array(train_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf7f53",
   "metadata": {},
   "source": [
    "## Saving outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# ... (Model training code) ...\n",
    "\n",
    "# Save the trained models\n",
    "torch.save(model.mri_encoder.state_dict(), \"mri_encoder.pth\")\n",
    "torch.save(model.state_dict(), \"hybrid_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33e67c4",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a851b",
   "metadata": {},
   "source": [
    "### light imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d097e43",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec7e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mri_inputs_test = torch.tensor(X_test_mri_pet[:, :pet_input_size], dtype=torch.float32)\n",
    "    pet_inputs_test = torch.tensor(X_test_mri_pet[:, pet_input_size:], dtype=torch.float32)\n",
    "    test_outputs = model(mri_inputs_test, pet_inputs_test)\n",
    "    test_predictions = (test_outputs > 0.5).float()\n",
    "    accuracy = accuracy_score(y_test_mri_pet, test_predictions.numpy())\n",
    "    print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d33d135",
   "metadata": {},
   "source": [
    "## Saving Evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ... (Model evaluation code) ...\n",
    "\n",
    "# Save evaluation report\n",
    "evaluation_report = {\n",
    "    \"test_accuracy\": accuracy,\n",
    "    # Add other metrics as needed\n",
    "}\n",
    "\n",
    "with open(\"evaluation_report.txt\", \"w\") as f:\n",
    "    for key, value in evaluation_report.items():\n",
    "        f.write(f\"{key}: {value}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
