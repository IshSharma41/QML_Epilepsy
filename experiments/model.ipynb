{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ebdac53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Training Base Model on MRI-only dataset ---\n",
      "Data shapes after split and scaling: Train=(433, 10), Test=(109, 10)\n",
      "Creating a complex, regularized Dense network with input shape 10\n",
      "Compiled base model for binary classification.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m2,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,081</span> (180.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m46,081\u001b[0m (180.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,081</span> (180.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,081\u001b[0m (180.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Early Stopping with patience=15 monitoring 'val_loss' for initial training.\n",
      "Fitting the base model on MRI-only dataset (max 100 epochs)...\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6398 - loss: 0.6225 - val_accuracy: 0.8165 - val_loss: 0.4440\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8310 - loss: 0.4203 - val_accuracy: 0.8165 - val_loss: 0.4076\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8179 - loss: 0.3931 - val_accuracy: 0.8165 - val_loss: 0.4035\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7992 - loss: 0.4019 - val_accuracy: 0.8165 - val_loss: 0.4093\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8041 - loss: 0.3844 - val_accuracy: 0.8165 - val_loss: 0.4097\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8377 - loss: 0.3668 - val_accuracy: 0.8165 - val_loss: 0.4144\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8190 - loss: 0.3640 - val_accuracy: 0.8165 - val_loss: 0.4163\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8077 - loss: 0.3909 - val_accuracy: 0.8165 - val_loss: 0.4230\n",
      "Epoch 9/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8153 - loss: 0.3730 - val_accuracy: 0.8165 - val_loss: 0.4311\n",
      "Epoch 10/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8194 - loss: 0.3739 - val_accuracy: 0.8165 - val_loss: 0.4298\n",
      "Epoch 11/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8129 - loss: 0.3839 - val_accuracy: 0.8165 - val_loss: 0.4262\n",
      "Epoch 12/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8162 - loss: 0.3694 - val_accuracy: 0.8349 - val_loss: 0.4276\n",
      "Epoch 13/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8267 - loss: 0.3774 - val_accuracy: 0.8532 - val_loss: 0.4295\n",
      "Epoch 14/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8186 - loss: 0.3768 - val_accuracy: 0.8349 - val_loss: 0.4297\n",
      "Epoch 15/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8314 - loss: 0.3628 - val_accuracy: 0.8349 - val_loss: 0.4345\n",
      "Epoch 16/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8364 - loss: 0.3603 - val_accuracy: 0.8257 - val_loss: 0.4418\n",
      "Epoch 17/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8503 - loss: 0.3203 - val_accuracy: 0.8257 - val_loss: 0.4397\n",
      "Epoch 18/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8611 - loss: 0.3860 - val_accuracy: 0.8257 - val_loss: 0.4379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model training finished.\n",
      "Initial training stopped after 18 epochs.\n",
      "Base model saved as base_model_mri.h5\n",
      "\n",
      "Evaluating base model on MRI-only test set:\n",
      "MRI-only Test Loss: 0.4035, Accuracy: 0.8165\n",
      "\n",
      "--- Step 2: Fine-Tuning Model on MRI-PET dataset ---\n",
      "MRI-PET Data shapes after split and scaling: Train=(33, 10), Test=(9, 10)\n",
      "Loaded base model from base_model_mri.h5\n",
      "Freezing all layers except the last 2 for fine-tuning...\n",
      "  - Unfroze layer: dense_43\n",
      "  - Unfroze layer: dropout_11\n",
      "Compiled fine-tuning model for binary classification with LR 1e-05.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m2,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,081</span> (180.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m46,081\u001b[0m (180.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> (132.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33\u001b[0m (132.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,048</span> (179.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m46,048\u001b[0m (179.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Early Stopping with patience=15 monitoring 'val_loss' for fine-tuning.\n",
      "Fitting the fine-tuning model on MRI-PET dataset (max 100 epochs)...\n",
      "Epoch 1/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 214ms/step - accuracy: 0.5713 - loss: 0.7075 - val_accuracy: 0.5556 - val_loss: 0.5742\n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.5713 - loss: 0.6842 - val_accuracy: 0.5556 - val_loss: 0.5742\n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5713 - loss: 0.6361 - val_accuracy: 0.5556 - val_loss: 0.5742\n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.7300 - val_accuracy: 0.5556 - val_loss: 0.5742\n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5818 - loss: 0.6659 - val_accuracy: 0.5556 - val_loss: 0.5742\n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5818 - loss: 0.6489 - val_accuracy: 0.5556 - val_loss: 0.5742\n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5713 - loss: 0.6904 - val_accuracy: 0.5556 - val_loss: 0.5742\n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5818 - loss: 0.6588 - val_accuracy: 0.5556 - val_loss: 0.5742\n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5818 - loss: 0.7228 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5818 - loss: 0.7483 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5713 - loss: 0.6886 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5818 - loss: 0.6720 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5713 - loss: 0.7305 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5713 - loss: 0.6943 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5713 - loss: 0.7155 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5713 - loss: 0.6890 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5713 - loss: 0.6976 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5818 - loss: 0.6875 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5818 - loss: 0.6734 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5818 - loss: 0.6711 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5818 - loss: 0.7119 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.6689 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5713 - loss: 0.7930 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5818 - loss: 0.6279 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5818 - loss: 0.7250 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5818 - loss: 0.6555 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.6999 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.7290 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5713 - loss: 0.7909 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.7093 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5818 - loss: 0.6414 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5818 - loss: 0.6048 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5713 - loss: 0.7162 - val_accuracy: 0.5556 - val_loss: 0.5741\n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.7368 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5713 - loss: 0.6847 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5713 - loss: 0.6787 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5713 - loss: 0.7053 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5818 - loss: 0.6656 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5713 - loss: 0.6768 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.6530 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5713 - loss: 0.6550 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5713 - loss: 0.6822 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5818 - loss: 0.6729 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5713 - loss: 0.6839 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5818 - loss: 0.6668 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5713 - loss: 0.6810 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.6794 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5713 - loss: 0.7353 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5818 - loss: 0.6713 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5713 - loss: 0.6825 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5818 - loss: 0.6923 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.6639 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5818 - loss: 0.7320 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5713 - loss: 0.6796 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5818 - loss: 0.6663 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5713 - loss: 0.6712 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5713 - loss: 0.6728 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5713 - loss: 0.6529 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5713 - loss: 0.7515 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5713 - loss: 0.6753 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5818 - loss: 0.6525 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5818 - loss: 0.7040 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.6836 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5818 - loss: 0.6915 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.7219 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5713 - loss: 0.7299 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.7173 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.6719 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5713 - loss: 0.6928 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5818 - loss: 0.6585 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.7221 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5713 - loss: 0.7142 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5818 - loss: 0.6761 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5818 - loss: 0.7299 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5713 - loss: 0.6780 - val_accuracy: 0.5556 - val_loss: 0.5740\n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.5818 - loss: 0.6461 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5818 - loss: 0.7005 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5713 - loss: 0.6691 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5818 - loss: 0.6974 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.6891 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5713 - loss: 0.6707 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.5818 - loss: 0.6382 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5818 - loss: 0.6737 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5713 - loss: 0.6668 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5818 - loss: 0.6342 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.6549 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5818 - loss: 0.7016 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5713 - loss: 0.6683 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.7324 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.6802 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5818 - loss: 0.6947 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5713 - loss: 0.6975 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5818 - loss: 0.6410 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5713 - loss: 0.6973 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5818 - loss: 0.6386 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5818 - loss: 0.6989 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5713 - loss: 0.6847 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5818 - loss: 0.6561 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5713 - loss: 0.6132 - val_accuracy: 0.5556 - val_loss: 0.5739\n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.7401 - val_accuracy: 0.5556 - val_loss: 0.5738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning finished.\n",
      "Fine-tuning stopped after 100 epochs.\n",
      "Fine-tuned model saved as fine_tuned_model_mri_pet.h5\n",
      "\n",
      "--- Evaluating Fine-Tuned Model on MRI-PET Test Set ---\n",
      "MRI-PET Test Loss: 0.5738, Accuracy: 0.5556\n",
      "\n",
      "Script execution finished.\n",
      "The final accuracy/metric reported is on the MRI-PET test set after fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os # Import os for path joining\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use os.path.join for better path handling across different OS\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\encoded_data\"\n",
    "MRI_FILE = \"mri_only_pca_10.csv\"\n",
    "MRI_PET_FILE = \"mri_pet_pca_10.csv\"\n",
    "\n",
    "MRI_PATH = os.path.join(BASE_DATA_DIR, MRI_FILE)\n",
    "MRI_PET_PATH = os.path.join(BASE_DATA_DIR, MRI_PET_FILE)\n",
    "\n",
    "BASE_MODEL_SAVE_PATH = \"base_model_mri.h5\"\n",
    "FINE_TUNED_MODEL_SAVE_PATH = \"fine_tuned_model_mri_pet.h5\"\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression' - adjusts loss/metrics/activation\n",
    "\n",
    "# Training Hyperparameters\n",
    "INITIAL_LEARNING_RATE = 0.001\n",
    "FINE_TUNE_LEARNING_RATE = 1e-5 # Start low for fine-tuning\n",
    "BATCH_SIZE = 32\n",
    "MAX_INITIAL_EPOCHS = 100 # Set a max, Early Stopping will likely stop earlier\n",
    "MAX_FINE_TUNE_EPOCHS = 100 # Set a max for fine-tuning\n",
    "EARLY_STOPPING_PATIENCE = 15 # How many epochs to wait for validation improvement\n",
    "\n",
    "# Transfer Learning Configuration\n",
    "NUM_LAYERS_TO_UNFREEZE = 2 # Number of layers from the end to unfreeze (experiment: 1, 2, 3...)\n",
    "\n",
    "# --- Function to Create a Complex Model with Regularization ---\n",
    "# This model architecture is used for both base training and fine-tuning\n",
    "def create_complex_regularized_model(input_shape):\n",
    "    \"\"\"\n",
    "    Function to create a complex classical neural network model with regularization.\n",
    "    Designed for good performance and generalization.\n",
    "    \"\"\"\n",
    "    print(f\"Creating a complex, regularized Dense network with input shape {input_shape}\")\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.4), # Added Dropout layer (experiment with rate, e.g., 0.2 to 0.5)\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        # Add more layers/neurons/dropout as needed, but balance complexity with data\n",
    "        Dense(1, activation='sigmoid' if TASK_TYPE == 'classification' else None) # Sigmoid for binary classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# --- Helper Function to Load and Preprocess Data ---\n",
    "# This scaler will be fitted ONLY on the first dataset's training split\n",
    "global_scaler = None\n",
    "def load_and_preprocess(file_path, fit_scaler=False):\n",
    "    \"\"\"\n",
    "    Loads data, handles labels, and scales features using a global scaler.\n",
    "    fit_scaler=True only for the initial training data source.\n",
    "    \"\"\"\n",
    "    global global_scaler # Declare intent to use the global scaler\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None, None, None\n",
    "\n",
    "    X = data.iloc[:, :-1].values  # Features\n",
    "    y = data.iloc[:, -1].values  # Labels\n",
    "\n",
    "    # Scale features - Fit ONLY if fit_scaler is True, always transform\n",
    "    if fit_scaler:\n",
    "        global_scaler = StandardScaler()\n",
    "        X_scaled = global_scaler.fit_transform(X)\n",
    "        print(\"Fitted StandardScaler on this data.\")\n",
    "    else:\n",
    "        if global_scaler is None:\n",
    "             print(\"Error: Scaler not fitted yet! Fit scaler on the first dataset's training data.\")\n",
    "             return None, None, None\n",
    "        X_scaled = global_scaler.transform(X)\n",
    "        print(\"Transformed data using fitted StandardScaler.\")\n",
    "\n",
    "\n",
    "    # If classification and labels are not 0/1, convert them\n",
    "    if TASK_TYPE == 'classification' and np.unique(y).size == 2 and not all(np.isin(y, [0, 1])):\n",
    "         print(f\"Warning: Assuming binary classification, converting labels {np.unique(y)} to 0/1.\")\n",
    "         unique_labels = np.unique(y)\n",
    "         label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "         y = np.array([label_map[label] for label in y])\n",
    "\n",
    "\n",
    "    print(f\"Loaded and processed data from {file_path}. Scaled X Shape: {X_scaled.shape}, y Shape: {y.shape}\")\n",
    "    return X_scaled, y, X_scaled.shape[1]\n",
    "\n",
    "\n",
    "# --- Step 1: Train the Base Model on the MRI-only dataset ---\n",
    "\n",
    "print(\"--- Step 1: Training Base Model on MRI-only dataset ---\")\n",
    "\n",
    "# Load and preprocess MRI data - Fit scaler on the training split\n",
    "# Need to split *before* calling load_and_preprocess with fit_scaler=True\n",
    "mri_data_full = pd.read_csv(MRI_PATH)\n",
    "X_mri_full = mri_data_full.iloc[:, :-1].values\n",
    "y_mri_full = mri_data_full.iloc[:, -1].values\n",
    "\n",
    "X_train_mri_raw, X_test_mri_raw, y_train_mri, y_test_mri = train_test_split(\n",
    "    X_mri_full, y_mri_full, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_mri_full if TASK_TYPE == 'classification' else None\n",
    ")\n",
    "\n",
    "# Now scale - Fit scaler *only* on the training subset\n",
    "global_scaler = StandardScaler() # Initialize the global scaler\n",
    "X_train_mri = global_scaler.fit_transform(X_train_mri_raw)\n",
    "X_test_mri = global_scaler.transform(X_test_mri_raw) # Transform test set with fitted scaler\n",
    "\n",
    "input_shape_mri = X_train_mri.shape[1]\n",
    "print(f\"Data shapes after split and scaling: Train={X_train_mri.shape}, Test={X_test_mri.shape}\")\n",
    "\n",
    "\n",
    "# Initialize the complex, regularized model\n",
    "model_mri = create_complex_regularized_model(input_shape_mri)\n",
    "\n",
    "# Define loss and metrics based on task type\n",
    "if TASK_TYPE == 'classification':\n",
    "    model_mri.compile(optimizer=Adam(learning_rate=INITIAL_LEARNING_RATE),\n",
    "                       loss='binary_crossentropy', # Use binary_crossentropy for binary classification\n",
    "                       metrics=['accuracy'])\n",
    "    print(\"Compiled base model for binary classification.\")\n",
    "else: # Regression\n",
    "     model_mri.compile(optimizer=Adam(learning_rate=INITIAL_LEARNING_RATE),\n",
    "                       loss='mean_squared_error',\n",
    "                       metrics=['mae']) # Use MAE for regression monitoring\n",
    "     print(\"Compiled base model for regression.\")\n",
    "\n",
    "\n",
    "model_mri.summary()\n",
    "\n",
    "# Add Early Stopping for preventing overfitting during initial training\n",
    "early_stopping_initial = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for initial training.\")\n",
    "\n",
    "print(f\"Fitting the base model on MRI-only dataset (max {MAX_INITIAL_EPOCHS} epochs)...\")\n",
    "history_mri = model_mri.fit(X_train_mri, y_train_mri,\n",
    "                            epochs=MAX_INITIAL_EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            validation_data=(X_test_mri, y_test_mri),\n",
    "                            callbacks=[early_stopping_initial], # Add Early Stopping callback\n",
    "                            verbose=1)\n",
    "print(\"Base model training finished.\")\n",
    "print(f\"Initial training stopped after {len(history_mri.history['loss'])} epochs.\")\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model_mri.save(BASE_MODEL_SAVE_PATH)\n",
    "print(f\"Base model saved as {BASE_MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Evaluate base model performance on its test set\n",
    "print(\"\\nEvaluating base model on MRI-only test set:\")\n",
    "base_model_eval = model_mri.evaluate(X_test_mri, y_test_mri, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"MRI-only Test Loss: {base_model_eval[0]:.4f}, Accuracy: {base_model_eval[1]:.4f}\")\n",
    "else:\n",
    "     print(f\"MRI-only Test Loss (MSE): {base_model_eval[0]:.4f}, Test MAE: {base_model_eval[1]:.4f}\")\n",
    "\n",
    "\n",
    "# --- Step 2: Fine-Tune the Model on the MRI-PET dataset ---\n",
    "\n",
    "print(\"\\n--- Step 2: Fine-Tuning Model on MRI-PET dataset ---\")\n",
    "\n",
    "# Load and preprocess MRI-PET data - Use the *already fitted* global_scaler\n",
    "mri_pet_data_full = pd.read_csv(MRI_PET_PATH)\n",
    "X_mri_pet_full = mri_pet_data_full.iloc[:, :-1].values\n",
    "y_mri_pet_full = mri_pet_data_full.iloc[:, -1].values\n",
    "\n",
    "# Split *before* scaling\n",
    "X_train_mri_pet_raw, X_test_mri_pet_raw, y_train_mri_pet, y_test_mri_pet = train_test_split(\n",
    "    X_mri_pet_full, y_mri_pet_full, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_mri_pet_full if TASK_TYPE == 'classification' else None\n",
    ")\n",
    "\n",
    "# Transform both train and test sets using the *already fitted* scaler\n",
    "if global_scaler is None:\n",
    "    print(\"Error: Scaler was not fitted in Step 1. Cannot proceed with fine-tuning.\")\n",
    "    exit()\n",
    "\n",
    "X_train_mri_pet = global_scaler.transform(X_train_mri_pet_raw)\n",
    "X_test_mri_pet = global_scaler.transform(X_test_mri_pet_raw)\n",
    "\n",
    "input_shape_mri_pet = X_train_mri_pet.shape[1]\n",
    "print(f\"MRI-PET Data shapes after split and scaling: Train={X_train_mri_pet.shape}, Test={X_test_mri_pet.shape}\")\n",
    "\n",
    "\n",
    "# Load the pre-trained model (from Step 1)\n",
    "try:\n",
    "    # Need to compile=False when loading if you plan to modify trainable status and re-compile\n",
    "    base_model_mri = tf.keras.models.load_model(BASE_MODEL_SAVE_PATH, compile=False)\n",
    "    print(f\"Loaded base model from {BASE_MODEL_SAVE_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Base model file '{BASE_MODEL_SAVE_PATH}' not found. Run Step 1 first.\")\n",
    "    exit()\n",
    "\n",
    "# Ensure the input shape matches the loaded model's expected input shape\n",
    "if input_shape_mri_pet != base_model_mri.input_shape[1]:\n",
    "    print(f\"Error: Input shapes of processed MRI-PET data ({input_shape_mri_pet}) and base model input ({base_model_mri.input_shape[1]}) differ.\")\n",
    "    print(\"Transfer learning requires the input feature dimension to be the same.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Unfreeze layers for fine-tuning ---\n",
    "# Set layers to non-trainable except the last 'NUM_LAYERS_TO_UNFREEZE' layers\n",
    "print(f\"Freezing all layers except the last {NUM_LAYERS_TO_UNFREEZE} for fine-tuning...\")\n",
    "for layer in base_model_mri.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Explicitly set the last N layers as trainable\n",
    "for i in range(1, NUM_LAYERS_TO_UNFREEZE + 1):\n",
    "    if len(base_model_mri.layers) - i >= 0:\n",
    "        base_model_mri.layers[-i].trainable = True\n",
    "        print(f\"  - Unfroze layer: {base_model_mri.layers[-i].name}\")\n",
    "    else:\n",
    "        print(f\"Warning: Cannot unfreeze {NUM_LAYERS_TO_UNFREEZE} layers, model only has {len(base_model_mri.layers)}.\")\n",
    "        break\n",
    "\n",
    "# --- End of Unfreeze layers ---\n",
    "\n",
    "\n",
    "# Compile the model for fine-tuning (important to re-compile after changing trainable status)\n",
    "# Use a much lower learning rate for fine-tuning\n",
    "if TASK_TYPE == 'classification':\n",
    "    base_model_mri.compile(optimizer=Adam(learning_rate=FINE_TUNE_LEARNING_RATE),\n",
    "                           loss='binary_crossentropy', # Use binary_crossentropy for binary classification\n",
    "                           metrics=['accuracy'])\n",
    "    print(f\"Compiled fine-tuning model for binary classification with LR {FINE_TUNE_LEARNING_RATE}.\")\n",
    "else: # Regression\n",
    "    base_model_mri.compile(optimizer=Adam(learning_rate=FINE_TUNE_LEARNING_RATE),\n",
    "                           loss='mean_squared_error',\n",
    "                           metrics=['mae'])\n",
    "    print(f\"Compiled fine-tuning model for regression with LR {FINE_TUNE_LEARNING_RATE}.\")\n",
    "\n",
    "\n",
    "base_model_mri.summary() # See which layers are trainable\n",
    "\n",
    "# Add Early Stopping for fine-tuning\n",
    "early_stopping_ft = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for fine-tuning.\")\n",
    "\n",
    "\n",
    "print(f\"Fitting the fine-tuning model on MRI-PET dataset (max {MAX_FINE_TUNE_EPOCHS} epochs)...\")\n",
    "history_mri_pet = base_model_mri.fit(X_train_mri_pet, y_train_mri_pet,\n",
    "                                    epochs=MAX_FINE_TUNE_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    validation_data=(X_test_mri_pet, y_test_mri_pet),\n",
    "                                    callbacks=[early_stopping_ft], # Add Early Stopping callback\n",
    "                                    verbose=1)\n",
    "print(\"Fine-tuning finished.\")\n",
    "print(f\"Fine-tuning stopped after {len(history_mri_pet.history['loss'])} epochs.\")\n",
    "\n",
    "\n",
    "# Save the fine-tuned model\n",
    "base_model_mri.save(FINE_TUNED_MODEL_SAVE_PATH)\n",
    "print(f\"Fine-tuned model saved as {FINE_TUNED_MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Evaluate fine-tuned model performance on its test set (the primary goal)\n",
    "print(\"\\n--- Evaluating Fine-Tuned Model on MRI-PET Test Set ---\")\n",
    "fine_tuned_model_eval_pet = base_model_mri.evaluate(X_test_mri_pet, y_test_mri_pet, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"MRI-PET Test Loss: {fine_tuned_model_eval_pet[0]:.4f}, Accuracy: {fine_tuned_model_eval_pet[1]:.4f}\")\n",
    "else:\n",
    "     print(f\"MRI-PET Test Loss (MSE): {fine_tuned_model_eval_pet[0]:.4f}, Test MAE: {fine_tuned_model_eval_pet[1]:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nScript execution finished.\")\n",
    "print(\"The final accuracy/metric reported is on the MRI-PET test set after fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e1ff11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Training Process ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and processed data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\encoded_data\\mri_only_pca_10.csv. X Shape: (542, 10), y Shape: (542,)\n",
      "Data split into: Training (433, 10), Validation (109, 10)\n",
      "Creating a Dense network with input shape 10\n",
      "Compiled model for binary classification.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,777</span> (46.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,777\u001b[0m (46.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,777</span> (46.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,777\u001b[0m (46.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model for 150 epochs...\n",
      "Epoch 1/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8227 - loss: 0.5827 - val_accuracy: 0.8165 - val_loss: 0.4690\n",
      "Epoch 2/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8153 - loss: 0.4522 - val_accuracy: 0.8165 - val_loss: 0.4176\n",
      "Epoch 3/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8081 - loss: 0.4307 - val_accuracy: 0.8165 - val_loss: 0.4118\n",
      "Epoch 4/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8060 - loss: 0.4264 - val_accuracy: 0.8165 - val_loss: 0.4079\n",
      "Epoch 5/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8100 - loss: 0.4005 - val_accuracy: 0.8165 - val_loss: 0.4056\n",
      "Epoch 6/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7976 - loss: 0.3957 - val_accuracy: 0.8165 - val_loss: 0.4049\n",
      "Epoch 7/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8219 - loss: 0.3649 - val_accuracy: 0.8165 - val_loss: 0.4010\n",
      "Epoch 8/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8193 - loss: 0.3665 - val_accuracy: 0.8165 - val_loss: 0.4028\n",
      "Epoch 9/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8288 - loss: 0.3377 - val_accuracy: 0.8257 - val_loss: 0.3994\n",
      "Epoch 10/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8203 - loss: 0.3766 - val_accuracy: 0.8440 - val_loss: 0.4006\n",
      "Epoch 11/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8530 - loss: 0.3336 - val_accuracy: 0.8349 - val_loss: 0.4054\n",
      "Epoch 12/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8306 - loss: 0.3640 - val_accuracy: 0.8349 - val_loss: 0.3993\n",
      "Epoch 13/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8335 - loss: 0.3578 - val_accuracy: 0.8440 - val_loss: 0.4002\n",
      "Epoch 14/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8326 - loss: 0.3454 - val_accuracy: 0.8440 - val_loss: 0.4006\n",
      "Epoch 15/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8570 - loss: 0.3177 - val_accuracy: 0.8165 - val_loss: 0.4123\n",
      "Epoch 16/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8451 - loss: 0.3528 - val_accuracy: 0.8257 - val_loss: 0.4066\n",
      "Epoch 17/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8470 - loss: 0.3498 - val_accuracy: 0.8257 - val_loss: 0.4046\n",
      "Epoch 18/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8585 - loss: 0.3326 - val_accuracy: 0.8073 - val_loss: 0.4073\n",
      "Epoch 19/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8478 - loss: 0.3155 - val_accuracy: 0.8257 - val_loss: 0.4116\n",
      "Epoch 20/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8436 - loss: 0.3203 - val_accuracy: 0.8349 - val_loss: 0.4158\n",
      "Epoch 21/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8522 - loss: 0.3239 - val_accuracy: 0.8257 - val_loss: 0.4080\n",
      "Epoch 22/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8624 - loss: 0.3035 - val_accuracy: 0.8349 - val_loss: 0.4151\n",
      "Epoch 23/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8503 - loss: 0.3183 - val_accuracy: 0.8349 - val_loss: 0.4061\n",
      "Epoch 24/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8554 - loss: 0.3149 - val_accuracy: 0.8257 - val_loss: 0.4107\n",
      "Epoch 25/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8593 - loss: 0.3127 - val_accuracy: 0.8165 - val_loss: 0.4166\n",
      "Epoch 26/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8633 - loss: 0.2900 - val_accuracy: 0.7982 - val_loss: 0.4309\n",
      "Epoch 27/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8743 - loss: 0.2776 - val_accuracy: 0.8257 - val_loss: 0.4182\n",
      "Epoch 28/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8321 - loss: 0.3263 - val_accuracy: 0.8257 - val_loss: 0.4174\n",
      "Epoch 29/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8696 - loss: 0.2871 - val_accuracy: 0.8257 - val_loss: 0.4222\n",
      "Epoch 30/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8581 - loss: 0.3048 - val_accuracy: 0.8349 - val_loss: 0.4235\n",
      "Epoch 31/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8574 - loss: 0.3018 - val_accuracy: 0.8257 - val_loss: 0.4240\n",
      "Epoch 32/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8707 - loss: 0.2763 - val_accuracy: 0.8073 - val_loss: 0.4362\n",
      "Epoch 33/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8560 - loss: 0.2836 - val_accuracy: 0.8165 - val_loss: 0.4338\n",
      "Epoch 34/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8828 - loss: 0.2589 - val_accuracy: 0.8440 - val_loss: 0.4345\n",
      "Epoch 35/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8702 - loss: 0.2796 - val_accuracy: 0.8257 - val_loss: 0.4330\n",
      "Epoch 36/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8703 - loss: 0.2864 - val_accuracy: 0.8073 - val_loss: 0.4422\n",
      "Epoch 37/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8783 - loss: 0.2721 - val_accuracy: 0.8349 - val_loss: 0.4386\n",
      "Epoch 38/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8443 - loss: 0.3052 - val_accuracy: 0.8257 - val_loss: 0.4385\n",
      "Epoch 39/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8686 - loss: 0.2679 - val_accuracy: 0.8073 - val_loss: 0.4502\n",
      "Epoch 40/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8400 - loss: 0.2913 - val_accuracy: 0.8257 - val_loss: 0.4522\n",
      "Epoch 41/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8697 - loss: 0.2739 - val_accuracy: 0.7982 - val_loss: 0.4559\n",
      "Epoch 42/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8650 - loss: 0.2730 - val_accuracy: 0.8257 - val_loss: 0.4463\n",
      "Epoch 43/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8542 - loss: 0.2889 - val_accuracy: 0.8165 - val_loss: 0.4569\n",
      "Epoch 44/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8862 - loss: 0.2478 - val_accuracy: 0.8073 - val_loss: 0.4635\n",
      "Epoch 45/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8818 - loss: 0.2452 - val_accuracy: 0.8165 - val_loss: 0.4577\n",
      "Epoch 46/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8827 - loss: 0.2308 - val_accuracy: 0.8073 - val_loss: 0.4673\n",
      "Epoch 47/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8673 - loss: 0.2915 - val_accuracy: 0.8165 - val_loss: 0.4682\n",
      "Epoch 48/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8826 - loss: 0.2447 - val_accuracy: 0.7982 - val_loss: 0.4828\n",
      "Epoch 49/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8793 - loss: 0.2549 - val_accuracy: 0.8257 - val_loss: 0.4795\n",
      "Epoch 50/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8643 - loss: 0.2626 - val_accuracy: 0.8073 - val_loss: 0.4822\n",
      "Epoch 51/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8794 - loss: 0.2555 - val_accuracy: 0.8440 - val_loss: 0.4768\n",
      "Epoch 52/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8794 - loss: 0.2380 - val_accuracy: 0.8165 - val_loss: 0.4821\n",
      "Epoch 53/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8822 - loss: 0.2278 - val_accuracy: 0.7890 - val_loss: 0.5082\n",
      "Epoch 54/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8806 - loss: 0.2562 - val_accuracy: 0.8073 - val_loss: 0.4902\n",
      "Epoch 55/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8611 - loss: 0.2608 - val_accuracy: 0.8073 - val_loss: 0.4942\n",
      "Epoch 56/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8956 - loss: 0.2309 - val_accuracy: 0.8165 - val_loss: 0.4970\n",
      "Epoch 57/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8720 - loss: 0.2615 - val_accuracy: 0.8073 - val_loss: 0.5077\n",
      "Epoch 58/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8894 - loss: 0.2477 - val_accuracy: 0.8165 - val_loss: 0.5026\n",
      "Epoch 59/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8947 - loss: 0.2380 - val_accuracy: 0.8257 - val_loss: 0.5041\n",
      "Epoch 60/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8889 - loss: 0.2407 - val_accuracy: 0.8073 - val_loss: 0.5118\n",
      "Epoch 61/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8751 - loss: 0.2464 - val_accuracy: 0.8073 - val_loss: 0.5213\n",
      "Epoch 62/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8833 - loss: 0.2377 - val_accuracy: 0.8165 - val_loss: 0.5224\n",
      "Epoch 63/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9002 - loss: 0.2418 - val_accuracy: 0.8165 - val_loss: 0.5247\n",
      "Epoch 64/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8985 - loss: 0.2205 - val_accuracy: 0.7982 - val_loss: 0.5394\n",
      "Epoch 65/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9102 - loss: 0.2225 - val_accuracy: 0.8073 - val_loss: 0.5321\n",
      "Epoch 66/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8788 - loss: 0.2498 - val_accuracy: 0.7982 - val_loss: 0.5466\n",
      "Epoch 67/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8924 - loss: 0.2174 - val_accuracy: 0.8165 - val_loss: 0.5398\n",
      "Epoch 68/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9049 - loss: 0.2040 - val_accuracy: 0.7890 - val_loss: 0.5385\n",
      "Epoch 69/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9204 - loss: 0.1971 - val_accuracy: 0.8073 - val_loss: 0.5425\n",
      "Epoch 70/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9136 - loss: 0.2180 - val_accuracy: 0.8165 - val_loss: 0.5472\n",
      "Epoch 71/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9176 - loss: 0.1937 - val_accuracy: 0.7982 - val_loss: 0.5597\n",
      "Epoch 72/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8595 - loss: 0.2762 - val_accuracy: 0.8349 - val_loss: 0.5524\n",
      "Epoch 73/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9060 - loss: 0.2129 - val_accuracy: 0.8257 - val_loss: 0.5529\n",
      "Epoch 74/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9086 - loss: 0.1980 - val_accuracy: 0.8165 - val_loss: 0.5624\n",
      "Epoch 75/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9098 - loss: 0.1970 - val_accuracy: 0.8073 - val_loss: 0.5624\n",
      "Epoch 76/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9246 - loss: 0.1855 - val_accuracy: 0.8073 - val_loss: 0.5708\n",
      "Epoch 77/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9082 - loss: 0.2096 - val_accuracy: 0.8165 - val_loss: 0.5823\n",
      "Epoch 78/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9141 - loss: 0.1994 - val_accuracy: 0.7890 - val_loss: 0.5748\n",
      "Epoch 79/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8797 - loss: 0.2291 - val_accuracy: 0.8165 - val_loss: 0.5811\n",
      "Epoch 80/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9093 - loss: 0.1914 - val_accuracy: 0.8165 - val_loss: 0.5859\n",
      "Epoch 81/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8933 - loss: 0.1904 - val_accuracy: 0.8073 - val_loss: 0.5879\n",
      "Epoch 82/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9050 - loss: 0.2129 - val_accuracy: 0.8165 - val_loss: 0.5987\n",
      "Epoch 83/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9146 - loss: 0.1829 - val_accuracy: 0.7890 - val_loss: 0.5951\n",
      "Epoch 84/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9149 - loss: 0.1845 - val_accuracy: 0.7982 - val_loss: 0.6137\n",
      "Epoch 85/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8942 - loss: 0.2062 - val_accuracy: 0.8165 - val_loss: 0.6095\n",
      "Epoch 86/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8786 - loss: 0.2296 - val_accuracy: 0.8165 - val_loss: 0.6157\n",
      "Epoch 87/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8950 - loss: 0.2078 - val_accuracy: 0.8073 - val_loss: 0.6338\n",
      "Epoch 88/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9164 - loss: 0.1949 - val_accuracy: 0.8073 - val_loss: 0.6224\n",
      "Epoch 89/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9124 - loss: 0.1969 - val_accuracy: 0.8073 - val_loss: 0.6337\n",
      "Epoch 90/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9040 - loss: 0.1960 - val_accuracy: 0.7890 - val_loss: 0.6380\n",
      "Epoch 91/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8859 - loss: 0.2182 - val_accuracy: 0.8073 - val_loss: 0.6283\n",
      "Epoch 92/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8983 - loss: 0.2146 - val_accuracy: 0.8165 - val_loss: 0.6489\n",
      "Epoch 93/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8967 - loss: 0.2047 - val_accuracy: 0.7890 - val_loss: 0.6468\n",
      "Epoch 94/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8776 - loss: 0.2123 - val_accuracy: 0.7982 - val_loss: 0.6583\n",
      "Epoch 95/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9023 - loss: 0.2003 - val_accuracy: 0.7982 - val_loss: 0.6648\n",
      "Epoch 96/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9222 - loss: 0.1779 - val_accuracy: 0.8073 - val_loss: 0.6581\n",
      "Epoch 97/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8917 - loss: 0.1976 - val_accuracy: 0.8073 - val_loss: 0.6696\n",
      "Epoch 98/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9022 - loss: 0.1835 - val_accuracy: 0.7798 - val_loss: 0.6782\n",
      "Epoch 99/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8929 - loss: 0.2228 - val_accuracy: 0.8073 - val_loss: 0.6781\n",
      "Epoch 100/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9144 - loss: 0.1829 - val_accuracy: 0.7890 - val_loss: 0.6850\n",
      "Epoch 101/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8951 - loss: 0.1931 - val_accuracy: 0.8073 - val_loss: 0.6901\n",
      "Epoch 102/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9171 - loss: 0.1766 - val_accuracy: 0.7890 - val_loss: 0.6905\n",
      "Epoch 103/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9213 - loss: 0.1618 - val_accuracy: 0.8073 - val_loss: 0.7163\n",
      "Epoch 104/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9086 - loss: 0.1829 - val_accuracy: 0.8257 - val_loss: 0.6996\n",
      "Epoch 105/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9023 - loss: 0.1852 - val_accuracy: 0.8165 - val_loss: 0.6968\n",
      "Epoch 106/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9261 - loss: 0.1572 - val_accuracy: 0.8073 - val_loss: 0.7136\n",
      "Epoch 107/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9284 - loss: 0.1661 - val_accuracy: 0.7890 - val_loss: 0.7247\n",
      "Epoch 108/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9099 - loss: 0.1887 - val_accuracy: 0.8073 - val_loss: 0.7202\n",
      "Epoch 109/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9012 - loss: 0.1857 - val_accuracy: 0.8073 - val_loss: 0.7256\n",
      "Epoch 110/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9256 - loss: 0.1729 - val_accuracy: 0.8165 - val_loss: 0.7263\n",
      "Epoch 111/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9174 - loss: 0.1635 - val_accuracy: 0.8349 - val_loss: 0.7431\n",
      "Epoch 112/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9194 - loss: 0.1741 - val_accuracy: 0.8073 - val_loss: 0.7376\n",
      "Epoch 113/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9240 - loss: 0.1603 - val_accuracy: 0.8165 - val_loss: 0.7529\n",
      "Epoch 114/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9238 - loss: 0.1608 - val_accuracy: 0.8165 - val_loss: 0.7456\n",
      "Epoch 115/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8861 - loss: 0.1878 - val_accuracy: 0.7890 - val_loss: 0.7662\n",
      "Epoch 116/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9380 - loss: 0.1645 - val_accuracy: 0.8165 - val_loss: 0.7690\n",
      "Epoch 117/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9264 - loss: 0.1579 - val_accuracy: 0.8257 - val_loss: 0.7611\n",
      "Epoch 118/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9158 - loss: 0.1798 - val_accuracy: 0.7890 - val_loss: 0.7658\n",
      "Epoch 119/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9260 - loss: 0.1628 - val_accuracy: 0.7982 - val_loss: 0.7745\n",
      "Epoch 120/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9049 - loss: 0.1714 - val_accuracy: 0.7982 - val_loss: 0.7910\n",
      "Epoch 121/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9321 - loss: 0.1597 - val_accuracy: 0.8073 - val_loss: 0.7935\n",
      "Epoch 122/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9288 - loss: 0.1501 - val_accuracy: 0.7982 - val_loss: 0.7955\n",
      "Epoch 123/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9202 - loss: 0.1477 - val_accuracy: 0.8073 - val_loss: 0.7954\n",
      "Epoch 124/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9133 - loss: 0.1745 - val_accuracy: 0.7982 - val_loss: 0.8083\n",
      "Epoch 125/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9309 - loss: 0.1500 - val_accuracy: 0.8073 - val_loss: 0.8209\n",
      "Epoch 126/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9080 - loss: 0.1563 - val_accuracy: 0.7982 - val_loss: 0.8331\n",
      "Epoch 127/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9171 - loss: 0.1585 - val_accuracy: 0.7890 - val_loss: 0.8409\n",
      "Epoch 128/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9227 - loss: 0.1549 - val_accuracy: 0.7982 - val_loss: 0.8392\n",
      "Epoch 129/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9204 - loss: 0.1549 - val_accuracy: 0.7982 - val_loss: 0.8649\n",
      "Epoch 130/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9139 - loss: 0.1914 - val_accuracy: 0.8165 - val_loss: 0.8539\n",
      "Epoch 131/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9078 - loss: 0.1632 - val_accuracy: 0.8073 - val_loss: 0.8653\n",
      "Epoch 132/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9227 - loss: 0.1681 - val_accuracy: 0.7890 - val_loss: 0.8574\n",
      "Epoch 133/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9290 - loss: 0.1588 - val_accuracy: 0.7982 - val_loss: 0.8493\n",
      "Epoch 134/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9315 - loss: 0.1351 - val_accuracy: 0.8257 - val_loss: 0.8665\n",
      "Epoch 135/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9328 - loss: 0.1501 - val_accuracy: 0.7982 - val_loss: 0.8770\n",
      "Epoch 136/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9149 - loss: 0.1485 - val_accuracy: 0.8073 - val_loss: 0.8881\n",
      "Epoch 137/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9078 - loss: 0.1773 - val_accuracy: 0.7982 - val_loss: 0.8832\n",
      "Epoch 138/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9310 - loss: 0.1264 - val_accuracy: 0.7982 - val_loss: 0.9011\n",
      "Epoch 139/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9223 - loss: 0.1469 - val_accuracy: 0.8257 - val_loss: 0.8958\n",
      "Epoch 140/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9346 - loss: 0.1423 - val_accuracy: 0.8257 - val_loss: 0.8893\n",
      "Epoch 141/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9410 - loss: 0.1279 - val_accuracy: 0.7890 - val_loss: 0.9171\n",
      "Epoch 142/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9372 - loss: 0.1360 - val_accuracy: 0.7982 - val_loss: 0.9172\n",
      "Epoch 143/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9479 - loss: 0.1258 - val_accuracy: 0.7982 - val_loss: 0.9329\n",
      "Epoch 144/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9392 - loss: 0.1342 - val_accuracy: 0.7890 - val_loss: 0.9330\n",
      "Epoch 145/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9099 - loss: 0.1360 - val_accuracy: 0.8165 - val_loss: 0.9390\n",
      "Epoch 146/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9296 - loss: 0.1332 - val_accuracy: 0.7890 - val_loss: 0.9591\n",
      "Epoch 147/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9112 - loss: 0.1630 - val_accuracy: 0.7706 - val_loss: 0.9464\n",
      "Epoch 148/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9080 - loss: 0.1514 - val_accuracy: 0.7890 - val_loss: 0.9887\n",
      "Epoch 149/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9342 - loss: 0.1396 - val_accuracy: 0.7706 - val_loss: 0.9805\n",
      "Epoch 150/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9217 - loss: 0.1514 - val_accuracy: 0.8257 - val_loss: 0.9473\n",
      "Model training finished.\n",
      "\n",
      "--- Analyzing Training and Validation Metrics Over Epochs ---\n",
      "\n",
      "Metrics after 150 epochs:\n",
      "  Training Loss: 0.1545, Training Compile_metrics: 0.9169\n",
      "  Validation Loss: 0.9473, Validation Compile_metrics: 0.8257\n",
      "\n",
      "Displaying plots of training and validation metrics over epochs...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Script execution finished.\n",
      "Review the plots to see how the training and validation metrics evolved throughout training.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt # For plotting the training history\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_PATH = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\encoded_data\\mri_only_pca_10.csv\"\n",
    "TEST_SIZE = 0.2 # Size of the validation set\n",
    "RANDOM_STATE = 42\n",
    "NUM_EPOCHS = 150 # Number of training iterations\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression' - adjusts loss/metrics\n",
    "\n",
    "# --- Function to Create the Model Architecture ---\n",
    "# A model architecture with enough capacity to learn the data patterns\n",
    "def create_model(input_shape):\n",
    "    \"\"\"\n",
    "    Function to create a neural network model architecture.\n",
    "    \"\"\"\n",
    "    print(f\"Creating a Dense network with input shape {input_shape}\")\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid' if TASK_TYPE == 'classification' else None) # Sigmoid for binary classification output\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# --- Helper Function to Load and Preprocess Data ---\n",
    "def load_and_preprocess(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None, None, None\n",
    "\n",
    "    X = data.iloc[:, :-1].values  # Features\n",
    "    y = data.iloc[:, -1].values  # Labels\n",
    "\n",
    "    # If classification and labels are not 0/1, convert them\n",
    "    if TASK_TYPE == 'classification' and np.unique(y).size == 2 and not all(np.isin(y, [0, 1])):\n",
    "         print(f\"Warning: Assuming binary classification, converting labels {np.unique(y)} to 0/1.\")\n",
    "         unique_labels = np.unique(y)\n",
    "         label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "         y = np.array([label_map[label] for label in y])\n",
    "\n",
    "    print(f\"Loaded and processed data from {file_path}. X Shape: {X.shape}, y Shape: {y.shape}\")\n",
    "    return X, y, X.shape[1]\n",
    "\n",
    "# --- Main Training Process ---\n",
    "\n",
    "print(\"--- Starting Model Training Process ---\")\n",
    "\n",
    "# Load and split data into training and validation sets\n",
    "X_data, y_data, input_shape = load_and_preprocess(DATA_PATH)\n",
    "if X_data is None:\n",
    "    exit()\n",
    "\n",
    "# Split data, using stratification for classification to maintain label distribution\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_data, y_data, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_data if TASK_TYPE == 'classification' else None\n",
    ")\n",
    "\n",
    "print(f\"Data split into: Training {X_train.shape}, Validation {X_val.shape}\")\n",
    "\n",
    "# Initialize the model\n",
    "model = create_model(input_shape)\n",
    "\n",
    "# Compile the model - setting up the optimizer, loss function, and metrics\n",
    "if TASK_TYPE == 'classification':\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='binary_crossentropy', # Loss for binary classification\n",
    "                  metrics=['accuracy']) # Metric to track accuracy\n",
    "    print(\"Compiled model for binary classification.\")\n",
    "else: # Regression\n",
    "     model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                   loss='mean_squared_error', # Loss for regression\n",
    "                   metrics=['mae']) # Metric to track Mean Absolute Error\n",
    "     print(\"Compiled model for regression.\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(f\"Training the model for {NUM_EPOCHS} epochs...\")\n",
    "# Train the model and store the training history\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_val, y_val), # Provide validation data\n",
    "                    verbose=1 # Show progress per epoch\n",
    "                   )\n",
    "print(\"Model training finished.\")\n",
    "\n",
    "# --- Analyze Training History ---\n",
    "\n",
    "print(\"\\n--- Analyzing Training and Validation Metrics Over Epochs ---\")\n",
    "\n",
    "# Get the training and validation metrics from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_metric_name = model.metrics_names[1] # Get the name of the primary metric (accuracy or mae)\n",
    "# --- FIX START ---\n",
    "# Get the correct metric name based on TASK_TYPE instead of relying on model.metrics_names[1]\n",
    "if TASK_TYPE == 'classification':\n",
    "    metric_name = 'accuracy'\n",
    "elif TASK_TYPE == 'regression':\n",
    "    metric_name = 'mae'\n",
    "else:\n",
    "    metric_name = 'unknown_metric' # Fallback, though should be accuracy or mae based on config\n",
    "\n",
    "# Access history using the correct metric name\n",
    "if metric_name in history.history:\n",
    "    train_metric = history.history[metric_name]\n",
    "    val_metric = history.history['val_' + metric_name]\n",
    "    metric_display_name = metric_name.capitalize()\n",
    "else:\n",
    "    print(f\"Warning: Metric '{metric_name}' not found in history. History keys: {history.history.keys()}\")\n",
    "    train_metric = [0] * len(epochs) # Placeholder if metric not found\n",
    "    val_metric = [0] * len(epochs)\n",
    "    metric_display_name = \"N/A\"\n",
    "\n",
    "# --- FIX END ---\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Print metrics at the end of training\n",
    "print(f\"\\nMetrics after {NUM_EPOCHS} epochs:\")\n",
    "print(f\"  Training Loss: {train_loss[-1]:.4f}, Training {train_metric_name.capitalize()}: {train_metric[-1]:.4f}\")\n",
    "print(f\"  Validation Loss: {val_loss[-1]:.4f}, Validation {train_metric_name.capitalize()}: {val_metric[-1]:.4f}\")\n",
    "\n",
    "\n",
    "# --- Plotting History to Visualize Learning Progress ---\n",
    "print(\"\\nDisplaying plots of training and validation metrics over epochs...\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, 'bo-', label='Training loss', alpha=0.6)\n",
    "plt.plot(epochs, val_loss, 'b-', label='Validation loss')\n",
    "plt.title('Training and validation loss over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Primary Metric (Accuracy or MAE)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_metric, 'bo-', label=f'Training {train_metric_name.capitalize()}', alpha=0.6)\n",
    "plt.plot(epochs, val_metric, 'b-', label=f'Validation {train_metric_name.capitalize()}')\n",
    "plt.title(f'Training and validation {train_metric_name.capitalize()} over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(train_metric_name.capitalize())\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScript execution finished.\")\n",
    "print(\"Review the plots to see how the training and validation metrics evolved throughout training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0fd8dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Training Base Model on MRI-only dataset ---\n",
      "Scaler reset.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'sub-105'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 136\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Now scale - Fit scaler *only* on the training subset\u001b[39;00m\n\u001b[0;32m    135\u001b[0m global_scaler \u001b[38;5;241m=\u001b[39m StandardScaler() \u001b[38;5;66;03m# Initialize the global scaler\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m X_train_mri \u001b[38;5;241m=\u001b[39m \u001b[43mglobal_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_mri_raw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Transform test data using the *fitted* scaler\u001b[39;00m\n\u001b[0;32m    139\u001b[0m X_test_mri \u001b[38;5;241m=\u001b[39m global_scaler\u001b[38;5;241m.\u001b[39mtransform(X_test_mri_raw)\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:930\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    929\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 930\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    938\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    837\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 839\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'sub-105'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os # Import os for path joining\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use os.path.join for better path handling across different OS\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\"\n",
    "MRI_FILE = \"mri_only_imputed.csv\"\n",
    "MRI_PET_FILE = \"mri_pet_imputed.csv\"\n",
    "\n",
    "MRI_PATH = os.path.join(BASE_DATA_DIR, MRI_FILE)\n",
    "MRI_PET_PATH = os.path.join(BASE_DATA_DIR, MRI_PET_FILE)\n",
    "\n",
    "BASE_MODEL_SAVE_PATH = \"base_model_mri.h5\"\n",
    "FINE_TUNED_MODEL_SAVE_PATH = \"fine_tuned_model_mri_pet.h5\"\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression' - adjusts loss/metrics/activation\n",
    "\n",
    "# Training Hyperparameters\n",
    "INITIAL_LEARNING_RATE = 0.001\n",
    "FINE_TUNE_LEARNING_RATE = 1e-5 # Start low for fine-tuning (experiment with 1e-6, 1e-7 if needed)\n",
    "BATCH_SIZE = 32\n",
    "MAX_INITIAL_EPOCHS = 100 # Set a max, Early Stopping will likely stop earlier\n",
    "MAX_FINE_TUNE_EPOCHS = 100 # Set a max for fine-tuning\n",
    "EARLY_STOPPING_PATIENCE = 15 # How many epochs to wait for validation improvement\n",
    "\n",
    "# Transfer Learning Configuration\n",
    "NUM_LAYERS_TO_UNFREEZE = 2 # Number of layers from the end to unfreeze (experiment: 1, 2, 3...)\n",
    "\n",
    "# --- Function to Create a Complex Model with Regularization ---\n",
    "# This model architecture is used for both base training and fine-tuning\n",
    "def create_complex_regularized_model(input_shape):\n",
    "    \"\"\"\n",
    "    Function to create a complex classical neural network model with regularization.\n",
    "    Designed for good performance and generalization.\n",
    "    \"\"\"\n",
    "    print(f\"Creating a complex, regularized Dense network with input shape {input_shape}\")\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.4), # Added Dropout layer (experiment with rate, e.2g., 0 to 0.5)\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        # Add more layers/neurons/dropout as needed, but balance complexity with data\n",
    "        Dense(1, activation='sigmoid' if TASK_TYPE == 'classification' else None) # Sigmoid for binary classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# --- Helper Function to Load and Preprocess Data with Consistent Scaling ---\n",
    "# Scaler will be fitted ONLY on the first dataset's training split\n",
    "global_scaler = None\n",
    "# Reset scaler function for clarity if running multiple times\n",
    "def reset_scaler():\n",
    "    global global_scaler\n",
    "    global_scaler = None\n",
    "    print(\"Scaler reset.\")\n",
    "\n",
    "\n",
    "def load_and_preprocess(file_path, fit_scaler=False):\n",
    "    \"\"\"\n",
    "    Loads data, handles labels, and scales features using a global scaler.\n",
    "    fit_scaler=True only for the initial training data source's training split.\n",
    "    \"\"\"\n",
    "    global global_scaler # Declare intent to use the global scaler\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None, None, None\n",
    "\n",
    "    X = data.iloc[:, :-1].values  # Features\n",
    "    y = data.iloc[:, -1].values  # Labels\n",
    "\n",
    "    # Scale features - Fit ONLY if fit_scaler is True, always transform\n",
    "    if fit_scaler:\n",
    "        if global_scaler is not None:\n",
    "             print(\"Warning: Scaler is already fitted. Ignoring fit_scaler=True for this call.\")\n",
    "        else:\n",
    "            global_scaler = StandardScaler()\n",
    "            X_scaled = global_scaler.fit_transform(X)\n",
    "            print(\"Fitted StandardScaler on this data.\")\n",
    "    else:\n",
    "        if global_scaler is None:\n",
    "             print(\"Error: Scaler not fitted yet! Fit scaler on the first dataset's training data first.\")\n",
    "             return None, None, None\n",
    "        X_scaled = global_scaler.transform(X)\n",
    "        print(\"Transformed data using fitted StandardScaler.\")\n",
    "\n",
    "\n",
    "    # If classification and labels are not 0/1, convert them\n",
    "    if TASK_TYPE == 'classification' and np.unique(y).size == 2 and not all(np.isin(y, [0, 1])):\n",
    "         print(f\"Warning: Assuming binary classification, converting labels {np.unique(y)} to 0/1.\")\n",
    "         unique_labels = np.unique(y)\n",
    "         label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "         y = np.array([label_map[label] for label in y])\n",
    "\n",
    "\n",
    "    print(f\"Loaded and processed data from {file_path}. Scaled X Shape: {X_scaled.shape}, y Shape: {y.shape}\")\n",
    "    return X_scaled, y, X_scaled.shape[1]\n",
    "\n",
    "\n",
    "# --- Step 1: Train the Base Model on the MRI-only dataset ---\n",
    "\n",
    "print(\"--- Step 1: Training Base Model on MRI-only dataset ---\")\n",
    "\n",
    "# Reset scaler to ensure a clean fit on the first dataset's training data\n",
    "reset_scaler()\n",
    "\n",
    "# Load and preprocess MRI data - Fit scaler on the training split\n",
    "# Need to split *before* calling load_and_preprocess with fit_scaler=True\n",
    "mri_data_full = pd.read_csv(MRI_PATH)\n",
    "X_mri_full = mri_data_full.iloc[:, :-1].values\n",
    "y_mri_full = mri_data_full.iloc[:, -1].values\n",
    "\n",
    "# Split *before* scaling\n",
    "X_train_mri_raw, X_test_mri_raw, y_train_mri, y_test_mri = train_test_split(\n",
    "    X_mri_full, y_mri_full, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_mri_full if TASK_TYPE == 'classification' else None\n",
    ")\n",
    "\n",
    "# Now scale - Fit scaler *only* on the training subset\n",
    "global_scaler = StandardScaler() # Initialize the global scaler\n",
    "X_train_mri = global_scaler.fit_transform(X_train_mri_raw)\n",
    "\n",
    "# Transform test data using the *fitted* scaler\n",
    "X_test_mri = global_scaler.transform(X_test_mri_raw)\n",
    "\n",
    "input_shape_mri = X_train_mri.shape[1]\n",
    "print(f\"Data shapes after split and scaling: Train={X_train_mri.shape}, Test={X_test_mri.shape}\")\n",
    "\n",
    "\n",
    "# Initialize the complex, regularized model\n",
    "model_mri = create_complex_regularized_model(input_shape_mri)\n",
    "\n",
    "# Define loss and metrics based on task type\n",
    "if TASK_TYPE == 'classification':\n",
    "    model_mri.compile(optimizer=Adam(learning_rate=INITIAL_LEARNING_RATE),\n",
    "                       loss='binary_crossentropy', # Use binary_crossentropy for binary classification\n",
    "                       metrics=['accuracy'])\n",
    "    print(\"Compiled base model for binary classification.\")\n",
    "else: # Regression\n",
    "     model_mri.compile(optimizer=Adam(learning_rate=INITIAL_LEARNING_RATE),\n",
    "                       loss='mean_squared_error',\n",
    "                       metrics=['mae']) # Use MAE for regression monitoring\n",
    "     print(\"Compiled base model for regression.\")\n",
    "\n",
    "\n",
    "model_mri.summary()\n",
    "\n",
    "# Add Early Stopping for preventing overfitting during initial training\n",
    "early_stopping_initial = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for initial training.\")\n",
    "\n",
    "print(f\"Fitting the base model on MRI-only dataset (max {MAX_INITIAL_EPOCHS} epochs)...\")\n",
    "history_mri = model_mri.fit(X_train_mri, y_train_mri,\n",
    "                            epochs=MAX_INITIAL_EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            validation_data=(X_test_mri, y_test_mri),\n",
    "                            callbacks=[early_stopping_initial], # Add Early Stopping callback\n",
    "                            verbose=1)\n",
    "print(\"Base model training finished.\")\n",
    "print(f\"Initial training stopped after {len(history_mri.history['loss'])} epochs.\")\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model_mri.save(BASE_MODEL_SAVE_PATH)\n",
    "print(f\"Base model saved as {BASE_MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Evaluate base model performance on its test set\n",
    "print(\"\\nEvaluating base model on MRI-only test set:\")\n",
    "base_model_eval = model_mri.evaluate(X_test_mri, y_test_mri, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"MRI-only Test Loss: {base_model_eval[0]:.4f}, Accuracy: {base_model_eval[1]:.4f}\")\n",
    "else:\n",
    "     print(f\"MRI-only Test Loss (MSE): {base_model_eval[0]:.4f}, Test MAE: {base_model_eval[1]:.4f}\")\n",
    "\n",
    "\n",
    "# --- Step 2: Fine-Tune the Model on the MRI-PET dataset ---\n",
    "\n",
    "print(\"\\n--- Step 2: Fine-Tuning Model on MRI-PET dataset ---\")\n",
    "\n",
    "# Load and preprocess MRI-PET data - Use the *already fitted* global_scaler\n",
    "mri_pet_data_full = pd.read_csv(MRI_PET_PATH)\n",
    "X_mri_pet_full = mri_pet_data_full.iloc[:, :-1].values\n",
    "y_mri_pet_full = mri_pet_data_full.iloc[:, -1].values\n",
    "\n",
    "# Split *before* scaling\n",
    "X_train_mri_pet_raw, X_test_mri_pet_raw, y_train_mri_pet, y_test_mri_pet = train_test_split(\n",
    "    X_mri_pet_full, y_mri_pet_full, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_mri_pet_full if TASK_TYPE == 'classification' else None\n",
    ")\n",
    "\n",
    "# Transform both train and test sets using the *already fitted* scaler\n",
    "if global_scaler is None:\n",
    "    print(\"Error: Scaler was not fitted in Step 1. Cannot proceed with fine-tuning.\")\n",
    "    exit()\n",
    "\n",
    "X_train_mri_pet = global_scaler.transform(X_train_mri_pet_raw)\n",
    "X_test_mri_pet = global_scaler.transform(X_test_mri_pet_raw)\n",
    "\n",
    "input_shape_mri_pet = X_train_mri_pet.shape[1]\n",
    "print(f\"MRI-PET Data shapes after split and scaling: Train={X_train_mri_pet.shape}, Test={X_test_mri_pet.shape}\")\n",
    "\n",
    "\n",
    "# Load the pre-trained model (from Step 1)\n",
    "try:\n",
    "    # Need to compile=False when loading if you plan to modify trainable status and re-compile\n",
    "    # Use custom_objects if your create_model function includes custom layers (like QNN layers)\n",
    "    base_model_mri = tf.keras.models.load_model(BASE_MODEL_SAVE_PATH, compile=False) # , custom_objects={...}\n",
    "    print(f\"Loaded base model from {BASE_MODEL_SAVE_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Base model file '{BASE_MODEL_SAVE_PATH}' not found. Run Step 1 first.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading base model: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Ensure the input shape matches the loaded model's expected input shape\n",
    "if input_shape_mri_pet != base_model_mri.input_shape[1]:\n",
    "    print(f\"Error: Input shapes of processed MRI-PET data ({input_shape_mri_pet}) and base model input ({base_model_mri.input_shape[1]}) differ.\")\n",
    "    print(\"Transfer learning requires the input feature dimension to be the same.\")\n",
    "    print(\"Check your PCA steps to ensure the final encoded dimensions are consistent.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Unfreeze layers for fine-tuning ---\n",
    "# Set layers to non-trainable except the last 'NUM_LAYERS_TO_UNFREEZE' layers\n",
    "print(f\"Freezing all layers except the last {NUM_LAYERS_TO_UNFREEZE} for fine-tuning...\")\n",
    "for layer in base_model_mri.layers:\n",
    "    # Set trainable false first for all layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Explicitly set the last N layers as trainable\n",
    "# Use len(base_model_mri.layers) for correct indexing\n",
    "num_model_layers = len(base_model_mri.layers)\n",
    "for i in range(1, NUM_LAYERS_TO_UNFREEZE + 1):\n",
    "    layer_index = num_model_layers - i\n",
    "    if layer_index >= 0:\n",
    "        base_model_mri.layers[layer_index].trainable = True\n",
    "        print(f\"  - Unfroze layer: {base_model_mri.layers[layer_index].name}\")\n",
    "    else:\n",
    "        print(f\"Warning: Cannot unfreeze {NUM_LAYERS_TO_UNFREEZE} layers, model only has {num_model_layers}.\")\n",
    "        break\n",
    "\n",
    "# --- End of Unfreeze layers ---\n",
    "\n",
    "\n",
    "# Compile the model for fine-tuning (important to re-compile after changing trainable status)\n",
    "# Use a much lower learning rate for fine-tuning\n",
    "if TASK_TYPE == 'classification':\n",
    "    base_model_mri.compile(optimizer=Adam(learning_rate=FINE_TUNE_LEARNING_RATE),\n",
    "                           loss='binary_crossentropy', # Use binary_crossentropy for binary classification\n",
    "                           metrics=['accuracy'])\n",
    "    print(f\"Compiled fine-tuning model for binary classification with LR {FINE_TUNE_LEARNING_RATE}.\")\n",
    "else: # Regression\n",
    "    base_model_mri.compile(optimizer=Adam(learning_rate=FINE_TUNE_LEARNING_RATE),\n",
    "                           loss='mean_squared_error',\n",
    "                           metrics=['mae'])\n",
    "    print(f\"Compiled fine-tuning model for regression with LR {FINE_TUNE_LEARNING_RATE}.\")\n",
    "\n",
    "\n",
    "base_model_mri.summary() # See which layers are trainable\n",
    "\n",
    "# Add Early Stopping for fine-tuning\n",
    "early_stopping_ft = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for fine-tuning.\")\n",
    "\n",
    "\n",
    "print(f\"Fitting the fine-tuning model on MRI-PET dataset (max {MAX_FINE_TUNE_EPOCHS} epochs)...\")\n",
    "history_mri_pet = base_model_mri.fit(X_train_mri_pet, y_train_mri_pet,\n",
    "                                    epochs=MAX_FINE_TUNE_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    validation_data=(X_test_mri_pet, y_test_mri_pet),\n",
    "                                    callbacks=[early_stopping_ft], # Add Early Stopping callback\n",
    "                                    verbose=1)\n",
    "print(\"Fine-tuning finished.\")\n",
    "print(f\"Fine-tuning training stopped after {len(history_mri_pet.history['loss'])} epochs.\")\n",
    "\n",
    "\n",
    "# Save the fine-tuned model (This is your final model using both datasets)\n",
    "base_model_mri.save(FINE_TUNED_MODEL_SAVE_PATH)\n",
    "print(f\"Fine-tuned model saved as {FINE_TUNED_MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Evaluate fine-tuned model performance on its test set (the primary goal)\n",
    "print(\"\\n--- Evaluating Fine-Tuned Model on MRI-PET Test Set ---\")\n",
    "fine_tuned_model_eval_pet = base_model_mri.evaluate(X_test_mri_pet, y_test_mri_pet, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"MRI-PET Test Loss: {fine_tuned_model_eval_pet[0]:.4f}, Accuracy: {fine_tuned_model_eval_pet[1]:.4f}\")\n",
    "else:\n",
    "     print(f\"MRI-PET Test Loss (MSE): {fine_tuned_model_eval_pet[0]:.4f}, Test MAE: {fine_tuned_model_eval_pet[1]:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nScript execution finished.\")\n",
    "print(f\"The final model saved at {FINE_TUNED_MODEL_SAVE_PATH} is the result of using both datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd05c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Training Base Model on MRI-only dataset ---\n",
      "Scaler reset.\n",
      "Successfully loaded raw data. X_mri_full shape: (542, 678), y_mri_full shape: (542,)\n",
      "Attempting to fit scaler...\n",
      "Scaler fitted successfully.\n",
      "Attempting to transform test data...\n",
      "Test data transformed successfully.\n",
      "Scaled data shapes after split: Train=(433, 678), Test=(109, 678)\n",
      "Creating a complex, regularized classical Dense network with input shape 678\n",
      "Compiled base model for binary classification.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">173,824</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_53 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m173,824\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_54 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_55 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_56 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_57 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">217,089</span> (848.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m217,089\u001b[0m (848.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">217,089</span> (848.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m217,089\u001b[0m (848.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Early Stopping with patience=15 monitoring 'val_loss' for initial training.\n",
      "Fitting the base model on MRI-only dataset (max 100 epochs)...\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7039 - loss: 0.5899 - val_accuracy: 0.8165 - val_loss: 0.4086\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8167 - loss: 0.4679 - val_accuracy: 0.8165 - val_loss: 0.4078\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8300 - loss: 0.4222 - val_accuracy: 0.8165 - val_loss: 0.4188\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7985 - loss: 0.4729 - val_accuracy: 0.8165 - val_loss: 0.4248\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8264 - loss: 0.4051 - val_accuracy: 0.8165 - val_loss: 0.4123\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8070 - loss: 0.4075 - val_accuracy: 0.8624 - val_loss: 0.4199\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8467 - loss: 0.3779 - val_accuracy: 0.7982 - val_loss: 0.4221\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8373 - loss: 0.3656 - val_accuracy: 0.8440 - val_loss: 0.4160\n",
      "Epoch 9/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7993 - loss: 0.3706 - val_accuracy: 0.8440 - val_loss: 0.4044\n",
      "Epoch 10/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8161 - loss: 0.3692 - val_accuracy: 0.7982 - val_loss: 0.4196\n",
      "Epoch 11/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8497 - loss: 0.3874 - val_accuracy: 0.8165 - val_loss: 0.4106\n",
      "Epoch 12/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8194 - loss: 0.3601 - val_accuracy: 0.8165 - val_loss: 0.4194\n",
      "Epoch 13/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8466 - loss: 0.3224 - val_accuracy: 0.7982 - val_loss: 0.4360\n",
      "Epoch 14/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8343 - loss: 0.3413 - val_accuracy: 0.8165 - val_loss: 0.4206\n",
      "Epoch 15/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7950 - loss: 0.3874 - val_accuracy: 0.7890 - val_loss: 0.4367\n",
      "Epoch 16/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8438 - loss: 0.3407 - val_accuracy: 0.7890 - val_loss: 0.4354\n",
      "Epoch 17/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8262 - loss: 0.3487 - val_accuracy: 0.7982 - val_loss: 0.4522\n",
      "Epoch 18/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8287 - loss: 0.3487 - val_accuracy: 0.8349 - val_loss: 0.4430\n",
      "Epoch 19/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8556 - loss: 0.3253 - val_accuracy: 0.7706 - val_loss: 0.4691\n",
      "Epoch 20/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8263 - loss: 0.3402 - val_accuracy: 0.7706 - val_loss: 0.4786\n",
      "Epoch 21/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8531 - loss: 0.3207 - val_accuracy: 0.8349 - val_loss: 0.4639\n",
      "Epoch 22/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8319 - loss: 0.3195 - val_accuracy: 0.7890 - val_loss: 0.4831\n",
      "Epoch 23/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8462 - loss: 0.3307 - val_accuracy: 0.7982 - val_loss: 0.4568\n",
      "Epoch 24/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8432 - loss: 0.3388 - val_accuracy: 0.7339 - val_loss: 0.5185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model training finished.\n",
      "Initial training stopped after 24 epochs.\n",
      "Base model saved as base_model_mri_normalized.h5\n",
      "\n",
      "Evaluating base model on MRI-only test set:\n",
      "MRI-only Test Loss: 0.4044, Accuracy: 0.8440\n",
      "\n",
      "--- Step 2: Fine-Tuning Model on MRI-PET dataset ---\n",
      "Loaded and split data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_pet_normalized.csv. Train shape: (33, 265), Test shape: (9, 265)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'sub-patient17'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 230\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Scaler was not fitted in Step 1. Cannot proceed with fine-tuning.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     exit()\n\u001b[1;32m--> 230\u001b[0m X_train_mri_pet \u001b[38;5;241m=\u001b[39m \u001b[43mglobal_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_mri_pet_raw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m X_test_mri_pet \u001b[38;5;241m=\u001b[39m global_scaler\u001b[38;5;241m.\u001b[39mtransform(X_test_mri_pet_raw)\n\u001b[0;32m    233\u001b[0m input_shape_mri_pet \u001b[38;5;241m=\u001b[39m X_train_mri_pet\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1059\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1061\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1062\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    837\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 839\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'sub-patient17'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os # Import os for path joining\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use os.path.join for better path handling across different OS\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\" # Updated directory\n",
    "MRI_FILE = \"mri_only_normalized.csv\"       # Updated file name\n",
    "MRI_PET_FILE = \"mri_pet_normalized.csv\"     # Updated file name\n",
    "\n",
    "MRI_PATH = os.path.join(BASE_DATA_DIR, MRI_FILE)\n",
    "MRI_PET_PATH = os.path.join(BASE_DATA_DIR, MRI_PET_FILE)\n",
    "\n",
    "BASE_MODEL_SAVE_PATH = \"base_model_mri_normalized.h5\" # Updated save name\n",
    "FINE_TUNED_MODEL_SAVE_PATH = \"fine_tuned_model_mri_pet_normalized.h5\" # Updated save name\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression' - adjusts loss/metrics/activation\n",
    "\n",
    "# Training Hyperparameters\n",
    "INITIAL_LEARNING_RATE = 0.001\n",
    "FINE_TUNE_LEARNING_RATE = 1e-5 # Start low for fine-tuning (experiment with 1e-6, 1e-7 if needed)\n",
    "BATCH_SIZE = 32\n",
    "MAX_INITIAL_EPOCHS = 100 # Set a max, Early Stopping will likely stop earlier\n",
    "MAX_FINE_TUNE_EPOCHS = 100 # Set a max for fine-tuning\n",
    "EARLY_STOPPING_PATIENCE = 15 # How many epochs to wait for validation improvement\n",
    "\n",
    "# Transfer Learning Configuration\n",
    "NUM_LAYERS_TO_UNFREEZE = 2 # Number of layers from the end to unfreeze (experiment: 1, 2, 3...)\n",
    "\n",
    "# --- Function to Create a Complex Classical Model with Regularization ---\n",
    "# This model architecture is used for both base training and fine-tuning\n",
    "def create_complex_regularized_classical_model(input_shape):\n",
    "    \"\"\"\n",
    "    Function to create a complex classical neural network model with regularization.\n",
    "    Designed for good performance and generalization using Dense layers.\n",
    "    \"\"\"\n",
    "    print(f\"Creating a complex, regularized classical Dense network with input shape {input_shape}\")\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.4), # Added Dropout layer (experiment with rate, e.2g., 0 to 0.5)\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        # Add more layers/neurons/dropout as needed, but balance complexity with data\n",
    "        Dense(1, activation='sigmoid' if TASK_TYPE == 'classification' else None) # Sigmoid for binary classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# --- Helper Function to Load and Preprocess Data with Consistent Scaling ---\n",
    "# Scaler will be fitted ONLY on the first dataset's training split\n",
    "global_scaler = None\n",
    "# Reset scaler function for clarity if running multiple times\n",
    "def reset_scaler():\n",
    "    global global_scaler\n",
    "    global_scaler = None\n",
    "    print(\"Scaler reset.\")\n",
    "\n",
    "def load_data_and_split(file_path, test_size, random_state, task_type):\n",
    "    \"\"\"Loads data, splits into train/test.\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Assuming the last column is the label and all others are features\n",
    "    # If the first column is an ID, you'll need to adjust iloc[:, :-1] to iloc[:, 1:-1]\n",
    "    # based on your file structure, as fixed in a previous response.\n",
    "    # Assuming ID column is NOT present in these \"normalized\" files, use [:, :-1]\n",
    "    # If ID is present, change iloc[:, :-1] to iloc[:, 1:-1]\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "\n",
    "    # If classification and labels are not 0/1, convert them\n",
    "    if task_type == 'classification' and np.unique(y).size == 2 and not all(np.isin(y, [0, 1])):\n",
    "         print(f\"Warning: Assuming binary classification, converting labels {np.unique(y)} to 0/1.\")\n",
    "         unique_labels = np.unique(y)\n",
    "         label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "         y = np.array([label_map[label] for label in y])\n",
    "\n",
    "    # Split data, using stratification for classification to maintain label distribution\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y if task_type == 'classification' else None\n",
    "    )\n",
    "\n",
    "    print(f\"Loaded and split data from {file_path}. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# --- Step 1: Train the Base Model on the MRI-only dataset ---\n",
    "\n",
    "print(\"--- Step 1: Training Base Model on MRI-only dataset ---\")\n",
    "\n",
    "# Reset scaler to ensure a clean fit on the first dataset's training data\n",
    "reset_scaler()\n",
    "\n",
    "# Load and split MRI data - Fit scaler on the training split\n",
    "# Need to split *before* calling load_and_preprocess with fit_scaler=True\n",
    "mri_data_full = pd.read_csv(MRI_PATH)\n",
    "\n",
    "# --- CRITICAL FIX VERIFICATION ---\n",
    "# Assuming the first column (index 0) is the subject ID (like 'sub-105')\n",
    "# and the last column is the numerical label.\n",
    "# Select features from the second column (index 1) up to, but NOT including, the last column.\n",
    "# If your file has a different structure, you MUST adjust this line accordingly.\n",
    "try:\n",
    "    X_mri_full = mri_data_full.iloc[:, 1:-1].values # Features exclude the first column (index 0)\n",
    "    y_mri_full = mri_data_full.iloc[:, -1].values  # Label is the last column\n",
    "    print(f\"Successfully loaded raw data. X_mri_full shape: {X_mri_full.shape}, y_mri_full shape: {y_mri_full.shape}\")\n",
    "    # Check if the feature matrix still contains non-numeric types after slicing\n",
    "    if not np.issubdtype(X_mri_full.dtype, np.number):\n",
    "         print(f\"Error: X_mri_full still contains non-numeric data after iloc[:, 1:-1]. Data type: {X_mri_full.dtype}\")\n",
    "         print(\"Please inspect your CSV file for non-numeric values in columns other than the first and last.\")\n",
    "         # You might need to inspect the first few rows/columns manually\n",
    "         # print(mri_data_full.head())\n",
    "         # print(mri_data_full.info())\n",
    "         exit() # Exit if non-numeric data is still present\n",
    "\n",
    "except IndexError:\n",
    "    print(\"Error: Indexing failed. Check if your file has enough columns (at least 3: ID, Feature(s), Label).\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during initial data loading: {e}\")\n",
    "    exit()\n",
    "# --- END CRITICAL FIX VERIFICATION ---\n",
    "\n",
    "\n",
    "# Split *before* scaling\n",
    "X_train_mri_raw, X_test_mri_raw, y_train_mri, y_test_mri = train_test_split(\n",
    "    X_mri_full, y_mri_full, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_mri_full if TASK_TYPE == 'classification' else None\n",
    ")\n",
    "\n",
    "# Now scale - Fit scaler *only* on the training subset\n",
    "global_scaler = StandardScaler() # Initialize the global scaler\n",
    "print(\"Attempting to fit scaler...\")\n",
    "# The error happens on the line below if X_train_mri_raw contains non-numeric data\n",
    "X_train_mri = global_scaler.fit_transform(X_train_mri_raw)\n",
    "print(\"Scaler fitted successfully.\")\n",
    "\n",
    "\n",
    "# Transform test data using the *fitted* scaler\n",
    "print(\"Attempting to transform test data...\")\n",
    "X_test_mri = global_scaler.transform(X_test_mri_raw)\n",
    "print(\"Test data transformed successfully.\")\n",
    "\n",
    "input_shape_mri = X_train_mri.shape[1]\n",
    "print(f\"Scaled data shapes after split: Train={X_train_mri.shape}, Test={X_test_mri.shape}\")\n",
    "\n",
    "\n",
    "# ... rest of Step 1 and all of Step 2 ...\n",
    "# (Ensure similar indexing logic is applied if you use load_and_preprocess helper elsewhere,\n",
    "# but the current script uses iloc directly in the main block for the initial load)\n",
    "\n",
    "# Initialize the complex, regularized classical model\n",
    "model_mri = create_complex_regularized_classical_model(input_shape_mri)\n",
    "\n",
    "# Define loss and metrics based on task type\n",
    "if TASK_TYPE == 'classification':\n",
    "    model_mri.compile(optimizer=Adam(learning_rate=INITIAL_LEARNING_RATE),\n",
    "                       loss='binary_crossentropy', # Use binary_crossentropy for binary classification\n",
    "                       metrics=['accuracy'])\n",
    "    print(\"Compiled base model for binary classification.\")\n",
    "else: # Regression\n",
    "     model_mri.compile(optimizer=Adam(learning_rate=INITIAL_LEARNING_RATE),\n",
    "                       loss='mean_squared_error',\n",
    "                       metrics=['mae']) # Use MAE for regression monitoring\n",
    "     print(\"Compiled base model for regression.\")\n",
    "\n",
    "\n",
    "model_mri.summary()\n",
    "\n",
    "# Add Early Stopping for preventing overfitting during initial training\n",
    "early_stopping_initial = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for initial training.\")\n",
    "\n",
    "print(f\"Fitting the base model on MRI-only dataset (max {MAX_INITIAL_EPOCHS} epochs)...\")\n",
    "history_mri = model_mri.fit(X_train_mri, y_train_mri,\n",
    "                            epochs=MAX_INITIAL_EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            validation_data=(X_test_mri, y_test_mri),\n",
    "                            callbacks=[early_stopping_initial], # Add Early Stopping callback\n",
    "                            verbose=1)\n",
    "print(\"Base model training finished.\")\n",
    "print(f\"Initial training stopped after {len(history_mri.history['loss'])} epochs.\")\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model_mri.save(BASE_MODEL_SAVE_PATH)\n",
    "print(f\"Base model saved as {BASE_MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Evaluate base model performance on its test set\n",
    "print(\"\\nEvaluating base model on MRI-only test set:\")\n",
    "base_model_eval = model_mri.evaluate(X_test_mri, y_test_mri, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"MRI-only Test Loss: {base_model_eval[0]:.4f}, Accuracy: {base_model_eval[1]:.4f}\")\n",
    "else:\n",
    "     print(f\"MRI-only Test Loss (MSE): {base_model_eval[0]:.4f}, Test MAE: {base_model_eval[1]:.4f}\")\n",
    "\n",
    "\n",
    "# --- Step 2: Fine-Tune the Model on the MRI-PET dataset ---\n",
    "\n",
    "print(\"\\n--- Step 2: Fine-Tuning Model on MRI-PET dataset ---\")\n",
    "\n",
    "# Load and split MRI-PET data\n",
    "X_train_mri_pet_raw, X_test_mri_pet_raw, y_train_mri_pet, y_test_mri_pet = load_data_and_split(\n",
    "    MRI_PET_PATH, TEST_SIZE, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri_pet_raw is None:\n",
    "    exit()\n",
    "\n",
    "# Transform both train and test sets using the *already fitted* scaler from Step 1\n",
    "if global_scaler is None:\n",
    "    print(\"Error: Scaler was not fitted in Step 1. Cannot proceed with fine-tuning.\")\n",
    "    exit()\n",
    "\n",
    "X_train_mri_pet = global_scaler.transform(X_train_mri_pet_raw)\n",
    "X_test_mri_pet = global_scaler.transform(X_test_mri_pet_raw)\n",
    "\n",
    "input_shape_mri_pet = X_train_mri_pet.shape[1]\n",
    "print(f\"Scaled MRI-PET Data shapes after split: Train={X_train_mri_pet.shape}, Test={X_test_mri_pet.shape}\")\n",
    "\n",
    "\n",
    "# Load the pre-trained model (from Step 1)\n",
    "try:\n",
    "    # Need to compile=False when loading if you plan to modify trainable status and re-compile\n",
    "    base_model_mri = tf.keras.models.load_model(BASE_MODEL_SAVE_PATH, compile=False)\n",
    "    print(f\"Loaded base model from {BASE_MODEL_SAVE_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Base model file '{BASE_MODEL_SAVE_PATH}' not found. Run Step 1 first.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading base model: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Ensure the input shape matches the loaded model's expected input shape\n",
    "# This is crucial when switching from PCA to full features - dimensions MUST match!\n",
    "if input_shape_mri_pet != base_model_mri.input_shape[1]:\n",
    "    print(f\"Error: Input shapes of processed MRI-PET data ({input_shape_mri_pet}) and base model input ({base_model_mri.input_shape[1]}) differ.\")\n",
    "    print(\"The number of features in mri_pet_normalized.csv must match the number of features in mri_only_normalized.csv\")\n",
    "    print(\"because the model architecture is fixed after Step 1.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Unfreeze layers for fine-tuning ---\n",
    "# Set layers to non-trainable except the last 'NUM_LAYERS_TO_UNFREEZE' layers\n",
    "print(f\"Freezing all layers except the last {NUM_LAYERS_TO_UNFREEZE} for fine-tuning...\")\n",
    "for layer in base_model_mri.layers:\n",
    "    # Set trainable false first for all layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Explicitly set the last N layers as trainable\n",
    "num_model_layers = len(base_model_mri.layers)\n",
    "for i in range(1, NUM_LAYERS_TO_UNFREEZE + 1):\n",
    "    layer_index = num_model_layers - i\n",
    "    if layer_index >= 0:\n",
    "        base_model_mri.layers[layer_index].trainable = True\n",
    "        print(f\"  - Unfroze layer: {base_model_mri.layers[layer_index].name}\")\n",
    "    else:\n",
    "        print(f\"Warning: Cannot unfreeze {NUM_LAYERS_TO_UNFREEZE} layers, model only has {num_model_layers}.\")\n",
    "        break\n",
    "\n",
    "# --- End of Unfreeze layers ---\n",
    "\n",
    "\n",
    "# Compile the model for fine-tuning (important to re-compile after changing trainable status)\n",
    "# Use a much lower learning rate for fine-tuning\n",
    "if TASK_TYPE == 'classification':\n",
    "    base_model_mri.compile(optimizer=Adam(learning_rate=FINE_TUNE_LEARNING_RATE),\n",
    "                           loss='binary_crossentropy', # Use binary_crossentropy for binary classification\n",
    "                           metrics=['accuracy'])\n",
    "    print(f\"Compiled fine-tuning model for binary classification with LR {FINE_TUNE_LEARNING_RATE}.\")\n",
    "else: # Regression\n",
    "    base_model_mri.compile(optimizer=Adam(learning_rate=FINE_TUNE_LEARNING_RATE),\n",
    "                           loss='mean_squared_error',\n",
    "                           metrics=['mae'])\n",
    "    print(f\"Compiled fine-tuning model for regression with LR {FINE_TUNE_LEARNING_RATE}.\")\n",
    "\n",
    "\n",
    "base_model_mri.summary() # See which layers are trainable\n",
    "\n",
    "# Add Early Stopping for fine-tuning\n",
    "early_stopping_ft = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for fine-tuning.\")\n",
    "\n",
    "\n",
    "print(f\"Fitting the fine-tuning model on MRI-PET dataset (max {MAX_FINE_TUNE_EPOCHS} epochs)...\")\n",
    "history_mri_pet = base_model_mri.fit(X_train_mri_pet, y_train_mri_pet,\n",
    "                                    epochs=MAX_FINE_TUNE_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    validation_data=(X_test_mri_pet, y_test_mri_pet),\n",
    "                                    callbacks=[early_stopping_ft], # Add Early Stopping callback\n",
    "                                    verbose=1)\n",
    "print(\"Fine-tuning finished.\")\n",
    "print(f\"Fine-tuning training stopped after {len(history_mri_pet.history['loss'])} epochs.\")\n",
    "\n",
    "\n",
    "# Save the fine-tuned model (This is your final model using both datasets)\n",
    "base_model_mri.save(FINE_TUNED_MODEL_SAVE_PATH)\n",
    "print(f\"Fine-tuned model saved as {FINE_TUNED_MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Evaluate fine-tuned model performance on its test set (the primary goal)\n",
    "print(\"\\n--- Evaluating Fine-Tuned Model on MRI-PET Test Set ---\")\n",
    "fine_tuned_model_eval_pet = base_model_mri.evaluate(X_test_mri_pet, y_test_mri_pet, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"MRI-PET Test Loss: {fine_tuned_model_eval_pet[0]:.4f}, Accuracy: {fine_tuned_model_eval_pet[1]:.4f}\")\n",
    "else:\n",
    "     print(f\"MRI-PET Test Loss (MSE): {fine_tuned_model_eval_pet[0]:.4f}, Test MAE: {fine_tuned_model_eval_pet[1]:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nScript execution finished.\")\n",
    "print(f\"The final model saved at {FINE_TUNED_MODEL_SAVE_PATH} is the result of using both datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22e645a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Training Base Model on MRI-only dataset ---\n",
      "Scaler reset.\n",
      "Loaded raw data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_only_normalized.csv. X shape: (542, 677), y shape: (542,)\n",
      "Split data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_only_normalized.csv. Train shape: (433, 677), Test shape: (109, 677)\n",
      "Attempting to fit scaler on MRI-only train data...\n",
      "Scaler fitted successfully.\n",
      "Attempting to transform MRI-only test data...\n",
      "MRI-only test data transformed successfully.\n",
      "Scaled data shapes after split: Train=(433, 677), Test=(109, 677)\n",
      "Creating a complex, regularized classical Dense network with input shape 677\n",
      "Compiled base model for binary classification.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">173,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_63 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m173,568\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_64 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_65 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_66 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_67 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,833</span> (847.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m216,833\u001b[0m (847.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,833</span> (847.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m216,833\u001b[0m (847.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Early Stopping with patience=15 monitoring 'val_loss' for initial training.\n",
      "Fitting the base model on MRI-only dataset (max 100 epochs)...\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.5856 - loss: 0.6448 - val_accuracy: 0.8165 - val_loss: 0.4236\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8267 - loss: 0.4288 - val_accuracy: 0.8165 - val_loss: 0.4068\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8052 - loss: 0.4493 - val_accuracy: 0.8165 - val_loss: 0.4194\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8109 - loss: 0.4641 - val_accuracy: 0.8257 - val_loss: 0.4285\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8339 - loss: 0.3940 - val_accuracy: 0.8349 - val_loss: 0.4117\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7823 - loss: 0.4782 - val_accuracy: 0.8165 - val_loss: 0.4209\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7996 - loss: 0.3864 - val_accuracy: 0.8349 - val_loss: 0.4079\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8105 - loss: 0.3815 - val_accuracy: 0.8165 - val_loss: 0.4330\n",
      "Epoch 9/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8090 - loss: 0.3929 - val_accuracy: 0.8349 - val_loss: 0.4162\n",
      "Epoch 10/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8100 - loss: 0.3550 - val_accuracy: 0.8257 - val_loss: 0.4226\n",
      "Epoch 11/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7919 - loss: 0.3823 - val_accuracy: 0.8440 - val_loss: 0.4158\n",
      "Epoch 12/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8491 - loss: 0.3257 - val_accuracy: 0.7982 - val_loss: 0.4336\n",
      "Epoch 13/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8176 - loss: 0.3875 - val_accuracy: 0.8165 - val_loss: 0.4295\n",
      "Epoch 14/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8303 - loss: 0.3753 - val_accuracy: 0.8349 - val_loss: 0.4377\n",
      "Epoch 15/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8316 - loss: 0.3304 - val_accuracy: 0.8440 - val_loss: 0.4424\n",
      "Epoch 16/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7951 - loss: 0.3466 - val_accuracy: 0.8440 - val_loss: 0.4436\n",
      "Epoch 17/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8423 - loss: 0.3167 - val_accuracy: 0.8532 - val_loss: 0.4345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model training finished.\n",
      "Initial training stopped after 17 epochs.\n",
      "Base model saved as base_model_mri_normalized.h5\n",
      "\n",
      "Evaluating base model on MRI-only test set:\n",
      "MRI-only Test Loss: 0.4068, Accuracy: 0.8165\n",
      "\n",
      "--- Step 2: Fine-Tuning Model on MRI-PET dataset ---\n",
      "Loaded raw data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_pet_normalized.csv. X shape: (42, 263), y shape: (42,)\n",
      "Split data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_pet_normalized.csv. Train shape: (33, 263), Test shape: (9, 263)\n",
      "Attempting to transform MRI-PET train data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 263 features, but StandardScaler is expecting 677 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 219\u001b[0m\n\u001b[0;32m    216\u001b[0m     exit()\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to transform MRI-PET train data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 219\u001b[0m X_train_mri_pet \u001b[38;5;241m=\u001b[39m \u001b[43mglobal_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_mri_pet_raw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# This should now receive only numbers\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMRI-PET train data transformed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to transform MRI-PET test data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1059\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1061\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1062\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\utils\\validation.py:2965\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 2965\u001b[0m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2832\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 263 features, but StandardScaler is expecting 677 features as input."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os # Import os for path joining\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use os.path.join for better path handling across different OS\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\" # Updated directory\n",
    "MRI_FILE = \"mri_only_normalized.csv\"       # Updated file name\n",
    "MRI_PET_FILE = \"mri_pet_normalized.csv\"     # Updated file name\n",
    "\n",
    "MRI_PATH = os.path.join(BASE_DATA_DIR, MRI_FILE)\n",
    "MRI_PET_PATH = os.path.join(BASE_DATA_DIR, MRI_PET_FILE)\n",
    "\n",
    "BASE_MODEL_SAVE_PATH = \"base_model_mri_normalized.h5\" # Updated save name\n",
    "FINE_TUNED_MODEL_SAVE_PATH = \"fine_tuned_model_mri_pet_normalized.h5\" # Updated save name\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression' - adjusts loss/metrics/activation\n",
    "\n",
    "# Training Hyperparameters\n",
    "INITIAL_LEARNING_RATE = 0.001\n",
    "FINE_TUNE_LEARNING_RATE = 1e-5 # Start low for fine-tuning (experiment with 1e-6, 1e-7 if needed)\n",
    "BATCH_SIZE = 32\n",
    "MAX_INITIAL_EPOCHS = 100 # Set a max, Early Stopping will likely stop earlier\n",
    "MAX_FINE_TUNE_EPOCHS = 100 # Set a max for fine-tuning\n",
    "EARLY_STOPPING_PATIENCE = 15 # How many epochs to wait for validation improvement\n",
    "\n",
    "# Transfer Learning Configuration\n",
    "NUM_LAYERS_TO_UNFREEZE = 2 # Number of layers from the end to unfreeze (experiment: 1, 2, 3...)\n",
    "\n",
    "# --- Function to Create a Complex Classical Model with Regularization ---\n",
    "# This model architecture is used for both base training and fine-tuning\n",
    "def create_complex_regularized_classical_model(input_shape):\n",
    "    \"\"\"\n",
    "    Function to create a complex classical neural network model with regularization.\n",
    "    Designed for good performance and generalization using Dense layers.\n",
    "    \"\"\"\n",
    "    print(f\"Creating a complex, regularized classical Dense network with input shape {input_shape}\")\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.4), # Added Dropout layer (experiment with rate, e.g., 0.2 to 0.5)\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        # Add more layers/neurons/dropout as needed, but balance complexity with data\n",
    "        Dense(1, activation='sigmoid' if TASK_TYPE == 'classification' else None) # Sigmoid for binary classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# --- Helper Function to Load Data and Split ---\n",
    "def load_data_and_split(file_path, test_size, random_state, task_type):\n",
    "    \"\"\"\n",
    "    Loads data from CSV, separates features and labels, and splits into train/test sets.\n",
    "    Assumes the first column is an ID and the last column is the label.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # --- CRITICAL FIX for Non-Numeric Columns ---\n",
    "    # Assuming the first column (index 0) is an ID column (like 'sub-patient17' or 'sub-105')\n",
    "    # and the last column is the numerical label.\n",
    "    # Select features from the second column (index 1) up to, but NOT including, the last column.\n",
    "    # If your files have a different structure, you MUST adjust these lines accordingly.\n",
    "    try:\n",
    "        X = data.iloc[:, 1:-1].values # Features exclude the first column (index 0)\n",
    "        y = data.iloc[:, -1].values  # Label is the last column\n",
    "        print(f\"Loaded raw data from {file_path}. X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "        # Optional check: Verify the feature matrix contains only numeric types\n",
    "        if not np.issubdtype(X.dtype, np.number):\n",
    "             print(f\"Error: Feature matrix from {file_path} still contains non-numeric data after slicing. Data type: {X.dtype}\")\n",
    "             print(\"Please inspect this CSV file for non-numeric values in columns other than the first and last.\")\n",
    "             # You might need to inspect the first few rows/columns manually\n",
    "             # print(data.head())\n",
    "             # print(data.info())\n",
    "             return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"Error: Indexing failed for {file_path}. Check if your file has enough columns (at least 3: ID, Feature(s), Label).\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during loading {file_path}: {e}\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    # --- END CRITICAL FIX ---\n",
    "\n",
    "    # If classification and labels are not 0/1, convert them\n",
    "    if task_type == 'classification' and np.unique(y).size == 2 and not all(np.isin(y, [0, 1])):\n",
    "         print(f\"Warning: Assuming binary classification, converting labels {np.unique(y)} to 0/1.\")\n",
    "         unique_labels = np.unique(y)\n",
    "         label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "         y = np.array([label_map[label] for label in y])\n",
    "\n",
    "    # Split data, using stratification for classification to maintain label distribution\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y if task_type == 'classification' else None\n",
    "    )\n",
    "\n",
    "    print(f\"Split data from {file_path}. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# --- Global Scaler ---\n",
    "# Scaler will be fitted ONLY on the first dataset's training split and reused.\n",
    "global_scaler = None\n",
    "def reset_scaler():\n",
    "    global global_scaler\n",
    "    global_scaler = None\n",
    "    print(\"Scaler reset.\")\n",
    "\n",
    "# --- Step 1: Train the Base Model on the MRI-only dataset ---\n",
    "\n",
    "print(\"--- Step 1: Training Base Model on MRI-only dataset ---\")\n",
    "\n",
    "# Reset scaler to ensure a clean fit on the first dataset's training data\n",
    "reset_scaler()\n",
    "\n",
    "# Load and split MRI data using the helper function (includes the iloc[:, 1:-1] fix)\n",
    "X_train_mri_raw, X_test_mri_raw, y_train_mri, y_test_mri = load_data_and_split(\n",
    "    MRI_PATH, TEST_SIZE, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri_raw is None: # Exit if loading/splitting failed in the helper\n",
    "    exit()\n",
    "\n",
    "# Scale the data - Fit scaler *only* on the training subset of the first dataset\n",
    "global_scaler = StandardScaler() # Initialize the global scaler\n",
    "print(\"Attempting to fit scaler on MRI-only train data...\")\n",
    "X_train_mri = global_scaler.fit_transform(X_train_mri_raw) # This should now receive only numbers\n",
    "print(\"Scaler fitted successfully.\")\n",
    "\n",
    "# Transform test data using the *fitted* scaler\n",
    "print(\"Attempting to transform MRI-only test data...\")\n",
    "X_test_mri = global_scaler.transform(X_test_mri_raw) # This should now receive only numbers\n",
    "print(\"MRI-only test data transformed successfully.\")\n",
    "\n",
    "input_shape_mri = X_train_mri.shape[1]\n",
    "print(f\"Scaled data shapes after split: Train={X_train_mri.shape}, Test={X_test_mri.shape}\")\n",
    "\n",
    "\n",
    "# Initialize the complex, regularized classical model\n",
    "model_mri = create_complex_regularized_classical_model(input_shape_mri)\n",
    "\n",
    "# Define loss and metrics based on task type\n",
    "if TASK_TYPE == 'classification':\n",
    "    model_mri.compile(optimizer=Adam(learning_rate=INITIAL_LEARNING_RATE),\n",
    "                       loss='binary_crossentropy', # Use binary_crossentropy for binary classification\n",
    "                       metrics=['accuracy'])\n",
    "    print(\"Compiled base model for binary classification.\")\n",
    "else: # Regression\n",
    "     model_mri.compile(optimizer=Adam(learning_rate=INITIAL_LEARNING_RATE),\n",
    "                       loss='mean_squared_error',\n",
    "                       metrics=['mae']) # Use MAE for regression monitoring\n",
    "     print(\"Compiled base model for regression.\")\n",
    "\n",
    "\n",
    "model_mri.summary()\n",
    "\n",
    "# Add Early Stopping for preventing overfitting during initial training\n",
    "early_stopping_initial = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for initial training.\")\n",
    "\n",
    "print(f\"Fitting the base model on MRI-only dataset (max {MAX_INITIAL_EPOCHS} epochs)...\")\n",
    "history_mri = model_mri.fit(X_train_mri, y_train_mri,\n",
    "                            epochs=MAX_INITIAL_EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            validation_data=(X_test_mri, y_test_mri),\n",
    "                            callbacks=[early_stopping_initial], # Add Early Stopping callback\n",
    "                            verbose=1)\n",
    "print(\"Base model training finished.\")\n",
    "print(f\"Initial training stopped after {len(history_mri.history['loss'])} epochs.\")\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model_mri.save(BASE_MODEL_SAVE_PATH)\n",
    "print(f\"Base model saved as {BASE_MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Evaluate base model performance on its test set\n",
    "print(\"\\nEvaluating base model on MRI-only test set:\")\n",
    "base_model_eval = model_mri.evaluate(X_test_mri, y_test_mri, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"MRI-only Test Loss: {base_model_eval[0]:.4f}, Accuracy: {base_model_eval[1]:.4f}\")\n",
    "else:\n",
    "     print(f\"MRI-only Test Loss (MSE): {base_model_eval[0]:.4f}, Test MAE: {base_model_eval[1]:.4f}\")\n",
    "\n",
    "\n",
    "# --- Step 2: Fine-Tune the Model on the MRI-PET dataset ---\n",
    "\n",
    "print(\"\\n--- Step 2: Fine-Tuning Model on MRI-PET dataset ---\")\n",
    "\n",
    "# Load and split MRI-PET data using the helper function (includes the iloc[:, 1:-1] fix)\n",
    "X_train_mri_pet_raw, X_test_mri_pet_raw, y_train_mri_pet, y_test_mri_pet = load_data_and_split(\n",
    "    MRI_PET_PATH, TEST_SIZE, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri_pet_raw is None: # Exit if loading/splitting failed in the helper\n",
    "    exit()\n",
    "\n",
    "# Transform both train and test sets using the *already fitted* scaler from Step 1\n",
    "if global_scaler is None:\n",
    "    print(\"Error: Scaler was not fitted in Step 1. Cannot proceed with fine-tuning.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Attempting to transform MRI-PET train data...\")\n",
    "X_train_mri_pet = global_scaler.transform(X_train_mri_pet_raw) # This should now receive only numbers\n",
    "print(\"MRI-PET train data transformed successfully.\")\n",
    "\n",
    "print(\"Attempting to transform MRI-PET test data...\")\n",
    "X_test_mri_pet = global_scaler.transform(X_test_mri_pet_raw) # This should now receive only numbers\n",
    "print(\"MRI-PET test data transformed successfully.\")\n",
    "\n",
    "input_shape_mri_pet = X_train_mri_pet.shape[1]\n",
    "print(f\"Scaled MRI-PET Data shapes after split: Train={X_train_mri_pet.shape}, Test={X_test_mri_pet.shape}\")\n",
    "\n",
    "\n",
    "# Load the pre-trained model (from Step 1)\n",
    "try:\n",
    "    # Need to compile=False when loading if you plan to modify trainable status and re-compile\n",
    "    base_model_mri = tf.keras.models.load_model(BASE_MODEL_SAVE_PATH, compile=False)\n",
    "    print(f\"Loaded base model from {BASE_MODEL_SAVE_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Base model file '{BASE_MODEL_SAVE_PATH}' not found. Run Step 1 first.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading base model: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Ensure the input shape matches the loaded model's expected input shape\n",
    "# This is crucial - number of features MUST match between the two datasets after slicing!\n",
    "if input_shape_mri_pet != base_model_mri.input_shape[1]:\n",
    "    print(f\"Error: Input shapes of processed MRI-PET data ({input_shape_mri_pet}) and base model input ({base_model_mri.input_shape[1]}) differ.\")\n",
    "    print(\"The number of features selected in mri_pet_normalized.csv must match the number of features selected in mri_only_normalized.csv\")\n",
    "    print(\"after excluding ID/Label columns. Check your CSV files' column counts.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Unfreeze layers for fine-tuning ---\n",
    "# Set layers to non-trainable except the last 'NUM_LAYERS_TO_UNFREEZE' layers\n",
    "print(f\"Freezing all layers except the last {NUM_LAYERS_TO_UNFREEZE} for fine-tuning...\")\n",
    "for layer in base_model_mri.layers:\n",
    "    # Set trainable false first for all layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Explicitly set the last N layers as trainable\n",
    "num_model_layers = len(base_model_mri.layers)\n",
    "for i in range(1, NUM_LAYERS_TO_UNFREEZE + 1):\n",
    "    layer_index = num_model_layers - i\n",
    "    if layer_index >= 0:\n",
    "        base_model_mri.layers[layer_index].trainable = True\n",
    "        print(f\"  - Unfroze layer: {base_model_mri.layers[layer_index].name}\")\n",
    "    else:\n",
    "        print(f\"Warning: Cannot unfreeze {NUM_LAYERS_TO_UNFREEZE} layers, model only has {num_model_layers}.\")\n",
    "        break\n",
    "\n",
    "# --- End of Unfreeze layers ---\n",
    "\n",
    "\n",
    "# Compile the model for fine-tuning (important to re-compile after changing trainable status)\n",
    "# Use a much lower learning rate for fine-tuning\n",
    "if TASK_TYPE == 'classification':\n",
    "    base_model_mri.compile(optimizer=Adam(learning_rate=FINE_TUNE_LEARNING_RATE),\n",
    "                           loss='binary_crossentropy', # Use binary_crossentropy for binary classification\n",
    "                           metrics=['accuracy'])\n",
    "    print(f\"Compiled fine-tuning model for binary classification with LR {FINE_TUNE_LEARNING_RATE}.\")\n",
    "else: # Regression\n",
    "    base_model_mri.compile(optimizer=Adam(learning_rate=FINE_TUNE_LEARNING_RATE),\n",
    "                           loss='mean_squared_error',\n",
    "                           metrics=['mae'])\n",
    "    print(f\"Compiled fine-tuning model for regression with LR {FINE_TUNE_LEARNING_RATE}.\")\n",
    "\n",
    "\n",
    "base_model_mri.summary() # See which layers are trainable\n",
    "\n",
    "# Add Early Stopping for fine-tuning\n",
    "early_stopping_ft = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for fine-tuning.\")\n",
    "\n",
    "\n",
    "print(f\"Fitting the fine-tuning model on MRI-PET dataset (max {MAX_FINE_TUNE_EPOCHS} epochs)...\")\n",
    "history_mri_pet = base_model_mri.fit(X_train_mri_pet, y_train_mri_pet,\n",
    "                                    epochs=MAX_FINE_TUNE_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    validation_data=(X_test_mri_pet, y_test_mri_pet),\n",
    "                                    callbacks=[early_stopping_ft], # Add Early Stopping callback\n",
    "                                    verbose=1)\n",
    "print(\"Fine-tuning finished.\")\n",
    "print(f\"Fine-tuning training stopped after {len(history_mri_pet.history['loss'])} epochs.\")\n",
    "\n",
    "\n",
    "# Save the fine-tuned model (This is your final model using both datasets)\n",
    "base_model_mri.save(FINE_TUNED_MODEL_SAVE_PATH)\n",
    "print(f\"Fine-tuned model saved as {FINE_TUNED_MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Evaluate fine-tuned model performance on its test set (the primary goal)\n",
    "print(\"\\n--- Evaluating Fine-Tuned Model on MRI-PET Test Set ---\")\n",
    "fine_tuned_model_eval_pet = base_model_mri.evaluate(X_test_mri_pet, y_test_mri_pet, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"MRI-PET Test Loss: {fine_tuned_model_eval_pet[0]:.4f}, Accuracy: {fine_tuned_model_eval_pet[1]:.4f}\")\n",
    "else:\n",
    "     print(f\"MRI-PET Test Loss (MSE): {fine_tuned_model_eval_pet[0]:.4f}, Test MAE: {fine_tuned_model_eval_pet[1]:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nScript execution finished.\")\n",
    "print(f\"The final model saved at {FINE_TUNED_MODEL_SAVE_PATH} is the result of using both datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3c738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Training Base Model on MRI-only dataset ---\n",
      "Loaded raw data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_only_normalized.csv. X shape: (542, 677), y shape: (542,)\n",
      "Split data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_only_normalized.csv. Train shape: (433, 677), Test shape: (109, 677)\n",
      "Data shapes after split: Train=(433, 677), Test=(109, 677)\n",
      "Creating a complex, regularized classical Dense network with input shape 677\n",
      "Compiled base model for binary classification.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">173,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m173,568\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,833</span> (847.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m216,833\u001b[0m (847.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,833</span> (847.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m216,833\u001b[0m (847.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Early Stopping with patience=15 monitoring 'val_loss' for initial training.\n",
      "Fitting the base model on MRI-only dataset (max 100 epochs)...\n",
      "Epoch 1/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.6944 - loss: 0.6283 - val_accuracy: 0.8165 - val_loss: 0.4041\n",
      "Epoch 2/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8472 - loss: 0.4069 - val_accuracy: 0.8165 - val_loss: 0.3980\n",
      "Epoch 3/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8267 - loss: 0.4520 - val_accuracy: 0.8165 - val_loss: 0.4186\n",
      "Epoch 4/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8154 - loss: 0.4468 - val_accuracy: 0.8165 - val_loss: 0.3956\n",
      "Epoch 5/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8135 - loss: 0.4282 - val_accuracy: 0.8165 - val_loss: 0.3963\n",
      "Epoch 6/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8121 - loss: 0.4412 - val_accuracy: 0.8165 - val_loss: 0.4030\n",
      "Epoch 7/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8206 - loss: 0.4028 - val_accuracy: 0.8165 - val_loss: 0.3965\n",
      "Epoch 8/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8142 - loss: 0.4467 - val_accuracy: 0.8165 - val_loss: 0.3963\n",
      "Epoch 9/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8163 - loss: 0.3975 - val_accuracy: 0.8165 - val_loss: 0.3937\n",
      "Epoch 10/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8287 - loss: 0.3758 - val_accuracy: 0.8165 - val_loss: 0.4037\n",
      "Epoch 11/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7888 - loss: 0.4278 - val_accuracy: 0.8165 - val_loss: 0.3929\n",
      "Epoch 12/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8248 - loss: 0.3924 - val_accuracy: 0.8165 - val_loss: 0.3923\n",
      "Epoch 13/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8007 - loss: 0.3839 - val_accuracy: 0.8165 - val_loss: 0.3911\n",
      "Epoch 14/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8061 - loss: 0.3879 - val_accuracy: 0.8165 - val_loss: 0.3972\n",
      "Epoch 15/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8127 - loss: 0.3799 - val_accuracy: 0.8165 - val_loss: 0.3937\n",
      "Epoch 16/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8099 - loss: 0.4029 - val_accuracy: 0.8165 - val_loss: 0.3945\n",
      "Epoch 17/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8142 - loss: 0.3977 - val_accuracy: 0.8165 - val_loss: 0.3987\n",
      "Epoch 18/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8167 - loss: 0.3659 - val_accuracy: 0.8165 - val_loss: 0.4092\n",
      "Epoch 19/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8070 - loss: 0.3961 - val_accuracy: 0.8165 - val_loss: 0.4019\n",
      "Epoch 20/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8180 - loss: 0.3781 - val_accuracy: 0.8440 - val_loss: 0.4192\n",
      "Epoch 21/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8274 - loss: 0.3585 - val_accuracy: 0.8349 - val_loss: 0.4056\n",
      "Epoch 22/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8383 - loss: 0.3768 - val_accuracy: 0.8440 - val_loss: 0.4066\n",
      "Epoch 23/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8502 - loss: 0.3340 - val_accuracy: 0.8532 - val_loss: 0.4097\n",
      "Epoch 24/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8353 - loss: 0.3628 - val_accuracy: 0.8440 - val_loss: 0.4120\n",
      "Epoch 25/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7920 - loss: 0.3967 - val_accuracy: 0.8257 - val_loss: 0.4172\n",
      "Epoch 26/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8285 - loss: 0.3641 - val_accuracy: 0.8165 - val_loss: 0.4220\n",
      "Epoch 27/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7991 - loss: 0.3611 - val_accuracy: 0.8440 - val_loss: 0.4318\n",
      "Epoch 28/100\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8338 - loss: 0.3718 - val_accuracy: 0.8440 - val_loss: 0.4214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model training finished.\n",
      "Initial training stopped after 28 epochs.\n",
      "Base model saved as base_model_mri_normalized_no_scaler.h5\n",
      "\n",
      "Evaluating base model on MRI-only test set:\n",
      "MRI-only Test Loss: 0.3911, Accuracy: 0.8165\n",
      "\n",
      "--- Step 2: Fine-Tuning Model on MRI-PET dataset ---\n",
      "Loaded raw data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_pet_normalized.csv. X shape: (42, 263), y shape: (42,)\n",
      "Split data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_pet_normalized.csv. Train shape: (33, 263), Test shape: (9, 263)\n",
      "MRI-PET Data shapes after split: Train=(33, 263), Test=(9, 263)\n",
      "Loaded base model from base_model_mri_normalized_no_scaler.h5\n",
      "\n",
      "======================================================================\n",
      "         !!!!! INPUT SHAPE MISMATCH ERROR !!!!!\n",
      "======================================================================\n",
      "Error: Input shapes of processed MRI-PET data (263) and base model input (677) differ.\n",
      "The number of features selected in 'mri_pet_normalized.csv' (263) must match the number of features selected in 'mri_only_normalized.csv' (677)\n",
      "after excluding ID/Label columns for transfer learning with this model architecture.\n",
      "\n",
      "**Reason:** The model architecture was fixed in Step 1 based on the first dataset's feature count.\n",
      "You cannot feed data with a different feature count to this fixed model.\n",
      "\n",
      "**To fix this:** You must either:\n",
      "1. Adjust your data preprocessing to make the feature counts identical in both files.\n",
      "2. Use a different modeling strategy, like training separate models and ensembling.\n",
      "======================================================================\n",
      "\n",
      "Freezing all layers except the last 2 for fine-tuning...\n",
      "  - Unfroze layer: dense_4\n",
      "  - Unfroze layer: dropout_3\n",
      "Compiled fine-tuning model for binary classification with LR 1e-05.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">173,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m173,568\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,833</span> (847.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m216,833\u001b[0m (847.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> (132.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33\u001b[0m (132.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,800</span> (846.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m216,800\u001b[0m (846.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Early Stopping with patience=15 monitoring 'val_loss' for fine-tuning.\n",
      "Fitting the fine-tuning model on MRI-PET dataset (max 100 epochs)...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 677, but received input with shape (None, 263)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 263), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 274\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUsing Early Stopping with patience=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEARLY_STOPPING_PATIENCE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m monitoring \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for fine-tuning.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting the fine-tuning model on MRI-PET dataset (max \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_FINE_TUNE_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 274\u001b[0m history_mri_pet \u001b[38;5;241m=\u001b[39m \u001b[43mbase_model_mri\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_mri_pet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_mri_pet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_FINE_TUNE_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test_mri_pet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_mri_pet\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_ft\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Add Early Stopping callback\u001b[39;49;00m\n\u001b[0;32m    279\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-tuning finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-tuning training stopped after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(history_mri_pet\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\venv4qml\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 677, but received input with shape (None, 263)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 263), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler # Removed StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os # Import os for path joining\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use os.path.join for better path handling across different OS\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\" # Updated directory\n",
    "MRI_FILE = \"mri_only_normalized.csv\"       # Updated file name\n",
    "MRI_PET_FILE = \"mri_pet_normalized.csv\"     # Updated file name\n",
    "\n",
    "MRI_PATH = os.path.join(BASE_DATA_DIR, MRI_FILE)\n",
    "MRI_PET_PATH = os.path.join(BASE_DATA_DIR, MRI_PET_FILE)\n",
    "\n",
    "BASE_MODEL_SAVE_PATH = \"base_model_mri_normalized_no_scaler.h5\" # Updated save name\n",
    "FINE_TUNED_MODEL_SAVE_PATH = \"fine_tuned_model_mri_pet_normalized_no_scaler.h5\" # Updated save name\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression' - adjusts loss/metrics/activation\n",
    "\n",
    "# Training Hyperparameters\n",
    "INITIAL_LEARNING_RATE = 0.001\n",
    "FINE_TUNE_LEARNING_RATE = 1e-5 # Start low for fine-tuning (experiment with 1e-6, 1e-7 if needed)\n",
    "BATCH_SIZE = 32\n",
    "MAX_INITIAL_EPOCHS = 100 # Set a max, Early Stopping will likely stop earlier\n",
    "MAX_FINE_TUNE_EPOCHS = 100 # Set a max for fine-tuning\n",
    "EARLY_STOPPING_PATIENCE = 15 # How many epochs to wait for validation improvement\n",
    "\n",
    "# Transfer Learning Configuration\n",
    "NUM_LAYERS_TO_UNFREEZE = 2 # Number of layers from the end to unfreeze (experiment: 1, 2, 3...)\n",
    "\n",
    "# --- Function to Create a Complex Classical Model with Regularization ---\n",
    "# This model architecture is used for both base training and fine-tuning\n",
    "def create_complex_regularized_classical_model(input_shape):\n",
    "    \"\"\"\n",
    "    Function to create a complex classical neural network model with regularization.\n",
    "    Designed for good performance and generalization using Dense layers.\n",
    "    \"\"\"\n",
    "    print(f\"Creating a complex, regularized classical Dense network with input shape {input_shape}\")\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.4), # Added Dropout layer (experiment with rate, e.g., 0.2 to 0.5)\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        # Add more layers/neurons/dropout as needed, but balance complexity with data\n",
    "        Dense(1, activation='sigmoid' if TASK_TYPE == 'classification' else None) # Sigmoid for binary classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# --- Helper Function to Load Data and Split ---\n",
    "def load_data_and_split(file_path, test_size, random_state, task_type):\n",
    "    \"\"\"\n",
    "    Loads data from CSV, separates features and labels, and splits into train/test sets.\n",
    "    Assumes the first column is an ID and the last column is the label.\n",
    "    Assumes data is already normalized and DOES NOT use StandardScaler.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # --- CRITICAL FIX for Non-Numeric Columns ---\n",
    "    # Assuming the first column (index 0) is an ID column\n",
    "    # and the last column is the numerical label.\n",
    "    # Select features from the second column (index 1) up to, but NOT including, the last column.\n",
    "    # If your files have a different structure, you MUST adjust these lines accordingly.\n",
    "    try:\n",
    "        X = data.iloc[:, 1:-1].values # Features exclude the first column (index 0)\n",
    "        y = data.iloc[:, -1].values  # Label is the last column\n",
    "        print(f\"Loaded raw data from {file_path}. X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "        # Optional check: Verify the feature matrix contains only numeric types\n",
    "        if not np.issubdtype(X.dtype, np.number):\n",
    "             print(f\"Error: Feature matrix from {file_path} still contains non-numeric data after slicing. Data type: {X.dtype}\")\n",
    "             print(\"Please inspect this CSV file for non-numeric values in columns other than the first and last.\")\n",
    "             # You might need to inspect the first few rows/columns manually\n",
    "             # print(data.head())\n",
    "             # print(data.info())\n",
    "             return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"Error: Indexing failed for {file_path}. Check if your file has enough columns (at least 3: ID, Feature(s), Label).\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during loading {file_path}: {e}\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    # --- END CRITICAL FIX ---\n",
    "\n",
    "    # If classification and labels are not 0/1, convert them\n",
    "    if task_type == 'classification' and np.unique(y).size == 2 and not all(np.isin(y, [0, 1])):\n",
    "         print(f\"Warning: Assuming binary classification, converting labels {np.unique(y)} to 0/1.\")\n",
    "         unique_labels = np.unique(y)\n",
    "         label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "         y = np.array([label_map[label] for label in y])\n",
    "\n",
    "    # Split data, using stratification for classification to maintain label distribution\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y if task_type == 'classification' else None\n",
    "    )\n",
    "\n",
    "    print(f\"Split data from {file_path}. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# --- Step 1: Train the Base Model on the MRI-only dataset ---\n",
    "\n",
    "print(\"--- Step 1: Training Base Model on MRI-only dataset ---\")\n",
    "\n",
    "# Load and split MRI data using the helper function (includes the iloc[:, 1:-1] fix, NO SCALING)\n",
    "X_train_mri, X_test_mri, y_train_mri, y_test_mri = load_data_and_split(\n",
    "    MRI_PATH, TEST_SIZE, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri is None: # Exit if loading/splitting failed in the helper\n",
    "    exit()\n",
    "\n",
    "input_shape_mri = X_train_mri.shape[1]\n",
    "print(f\"Data shapes after split: Train={X_train_mri.shape}, Test={X_test_mri.shape}\")\n",
    "\n",
    "\n",
    "# Initialize the complex, regularized classical model\n",
    "model_mri = create_complex_regularized_classical_model(input_shape_mri)\n",
    "\n",
    "# Define loss and metrics based on task type\n",
    "if TASK_TYPE == 'classification':\n",
    "    model_mri.compile(optimizer=Adam(learning_rate=INITIAL_LEARNING_RATE),\n",
    "                       loss='binary_crossentropy', # Use binary_crossentropy for binary classification\n",
    "                       metrics=['accuracy'])\n",
    "    print(\"Compiled base model for binary classification.\")\n",
    "else: # Regression\n",
    "     model_mri.compile(optimizer=Adam(learning_rate=INITIAL_LEARNING_RATE),\n",
    "                       loss='mean_squared_error',\n",
    "                       metrics=['mae']) # Use MAE for regression monitoring\n",
    "     print(\"Compiled base model for regression.\")\n",
    "\n",
    "\n",
    "model_mri.summary()\n",
    "\n",
    "# Add Early Stopping for preventing overfitting during initial training\n",
    "early_stopping_initial = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for initial training.\")\n",
    "\n",
    "print(f\"Fitting the base model on MRI-only dataset (max {MAX_INITIAL_EPOCHS} epochs)...\")\n",
    "history_mri = model_mri.fit(X_train_mri, y_train_mri,\n",
    "                            epochs=MAX_INITIAL_EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            validation_data=(X_test_mri, y_test_mri),\n",
    "                            callbacks=[early_stopping_initial], # Add Early Stopping callback\n",
    "                            verbose=1)\n",
    "print(\"Base model training finished.\")\n",
    "print(f\"Initial training stopped after {len(history_mri.history['loss'])} epochs.\")\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model_mri.save(BASE_MODEL_SAVE_PATH)\n",
    "print(f\"Base model saved as {BASE_MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Evaluate base model performance on its test set\n",
    "print(\"\\nEvaluating base model on MRI-only test set:\")\n",
    "base_model_eval = model_mri.evaluate(X_test_mri, y_test_mri, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"MRI-only Test Loss: {base_model_eval[0]:.4f}, Accuracy: {base_model_eval[1]:.4f}\")\n",
    "else:\n",
    "     print(f\"MRI-only Test Loss (MSE): {base_model_eval[0]:.4f}, Test MAE: {base_model_eval[1]:.4f}\")\n",
    "\n",
    "\n",
    "# --- Step 2: Fine-Tune the Model on the MRI-PET dataset ---\n",
    "\n",
    "print(\"\\n--- Step 2: Fine-Tuning Model on MRI-PET dataset ---\")\n",
    "\n",
    "# Load and split MRI-PET data using the helper function (includes the iloc[:, 1:-1] fix, NO SCALING)\n",
    "X_train_mri_pet, X_test_mri_pet, y_train_mri_pet, y_test_mri_pet = load_data_and_split(\n",
    "    MRI_PET_PATH, TEST_SIZE, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri_pet is None: # Exit if loading/splitting failed in the helper\n",
    "    exit()\n",
    "\n",
    "input_shape_mri_pet = X_train_mri_pet.shape[1]\n",
    "print(f\"MRI-PET Data shapes after split: Train={X_train_mri_pet.shape}, Test={X_test_mri_pet.shape}\")\n",
    "\n",
    "\n",
    "# Load the pre-trained model (from Step 1)\n",
    "try:\n",
    "    # Need to compile=False when loading if you plan to modify trainable status and re-compile\n",
    "    base_model_mri = tf.keras.models.load_model(BASE_MODEL_SAVE_PATH, compile=False)\n",
    "    print(f\"Loaded base model from {BASE_MODEL_SAVE_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Base model file '{BASE_MODEL_SAVE_PATH}' not found. Run Step 1 first.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading base model: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- CRITICAL ISSUE: INPUT SHAPE MISMATCH ---\n",
    "# This check will confirm the problem you are facing.\n",
    "# The model was built to accept input_shape_mri features (678).\n",
    "# It cannot directly accept input_shape_mri_pet features (264).\n",
    "if input_shape_mri_pet != base_model_mri.input_shape[1]:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"         !!!!! INPUT SHAPE MISMATCH ERROR !!!!!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Error: Input shapes of processed MRI-PET data ({input_shape_mri_pet}) and base model input ({base_model_mri.input_shape[1]}) differ.\")\n",
    "    print(f\"The number of features selected in '{MRI_PET_FILE}' ({input_shape_mri_pet}) must match the number of features selected in '{MRI_FILE}' ({base_model_mri.input_shape[1]})\")\n",
    "    print(\"after excluding ID/Label columns for transfer learning with this model architecture.\")\n",
    "    print(\"\\n**Reason:** The model architecture was fixed in Step 1 based on the first dataset's feature count.\")\n",
    "    print(\"You cannot feed data with a different feature count to this fixed model.\")\n",
    "    print(\"\\n**To fix this:** You must either:\")\n",
    "    print(\"1. Adjust your data preprocessing to make the feature counts identical in both files.\")\n",
    "    print(\"2. Use a different modeling strategy, like training separate models and ensembling.\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    exit()\n",
    "# --- END CRITICAL ISSUE ---\n",
    "\n",
    "\n",
    "# --- Unfreeze layers for fine-tuning ---\n",
    "# Set layers to non-trainable except the last 'NUM_LAYERS_TO_UNFREEZE' layers\n",
    "print(f\"Freezing all layers except the last {NUM_LAYERS_TO_UNFREEZE} for fine-tuning...\")\n",
    "for layer in base_model_mri.layers:\n",
    "    # Set trainable false first for all layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Explicitly set the last N layers as trainable\n",
    "num_model_layers = len(base_model_mri.layers)\n",
    "for i in range(1, NUM_LAYERS_TO_UNFREEZE + 1):\n",
    "    layer_index = num_model_layers - i\n",
    "    if layer_index >= 0:\n",
    "        base_model_mri.layers[layer_index].trainable = True\n",
    "        print(f\"  - Unfroze layer: {base_model_mri.layers[layer_index].name}\")\n",
    "    else:\n",
    "        print(f\"Warning: Cannot unfreeze {NUM_LAYERS_TO_UNFREEZE} layers, model only has {num_model_layers}.\")\n",
    "        break\n",
    "\n",
    "# --- End of Unfreeze layers ---\n",
    "\n",
    "\n",
    "# Compile the model for fine-tuning (important to re-compile after changing trainable status)\n",
    "# Use a much lower learning rate for fine-tuning\n",
    "if TASK_TYPE == 'classification':\n",
    "    base_model_mri.compile(optimizer=Adam(learning_rate=FINE_TUNE_LEARNING_RATE),\n",
    "                           loss='binary_crossentropy', # Use binary_crossentropy for binary classification\n",
    "                           metrics=['accuracy'])\n",
    "    print(f\"Compiled fine-tuning model for binary classification with LR {FINE_TUNE_LEARNING_RATE}.\")\n",
    "else: # Regression\n",
    "    base_model_mri.compile(optimizer=Adam(learning_rate=FINE_TUNE_LEARNING_RATE),\n",
    "                           loss='mean_squared_error',\n",
    "                           metrics=['mae'])\n",
    "    print(f\"Compiled fine-tuning model for regression with LR {FINE_TUNE_LEARNING_RATE}.\")\n",
    "\n",
    "\n",
    "base_model_mri.summary() # See which layers are trainable\n",
    "\n",
    "# Add Early Stopping for fine-tuning\n",
    "early_stopping_ft = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for fine-tuning.\")\n",
    "\n",
    "\n",
    "print(f\"Fitting the fine-tuning model on MRI-PET dataset (max {MAX_FINE_TUNE_EPOCHS} epochs)...\")\n",
    "history_mri_pet = base_model_mri.fit(X_train_mri_pet, y_train_mri_pet,\n",
    "                                    epochs=MAX_FINE_TUNE_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    validation_data=(X_test_mri_pet, y_test_mri_pet),\n",
    "                                    callbacks=[early_stopping_ft], # Add Early Stopping callback\n",
    "                                    verbose=1)\n",
    "print(\"Fine-tuning finished.\")\n",
    "print(f\"Fine-tuning training stopped after {len(history_mri_pet.history['loss'])} epochs.\")\n",
    "\n",
    "\n",
    "# Save the fine-tuned model (This is your final model using both datasets)\n",
    "base_model_mri.save(FINE_TUNED_MODEL_SAVE_PATH)\n",
    "print(f\"Fine-tuned model saved as {FINE_TUNED_MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Evaluate fine-tuned model performance on its test set (the primary goal)\n",
    "print(\"\\n--- Evaluating Fine-Tuned Model on MRI-PET Test Set ---\")\n",
    "fine_tuned_model_eval_pet = base_model_mri.evaluate(X_test_mri_pet, y_test_mri_pet, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"MRI-PET Test Loss: {fine_tuned_model_eval_pet[0]:.4f}, Accuracy: {fine_tuned_model_eval_pet[1]:.4f}\")\n",
    "else:\n",
    "     print(f\"MRI-PET Test Loss (MSE): {fine_tuned_model_eval_pet[0]:.4f}, Test MAE: {fine_tuned_model_eval_pet[1]:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nScript execution finished.\")\n",
    "print(f\"The final model saved at {FINE_TUNED_MODEL_SAVE_PATH} is the result of using both datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b9c7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Model 1 on MRI-only dataset (678 Features) ---\n",
      "Loaded raw data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_only_normalized.csv. X shape: (542, 677), y shape: (542,)\n",
      "Split data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_only_normalized.csv. Train shape: (433, 677), Test shape: (109, 677)\n",
      "Input shape for Model 1: 677\n",
      "Creating a complex, regularized classical Dense network with input shape 677\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">173,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m173,568\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,833</span> (847.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m216,833\u001b[0m (847.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,833</span> (847.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m216,833\u001b[0m (847.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Early Stopping with patience=15 monitoring 'val_loss' for Model 1 training.\n",
      "Fitting Model 1 on MRI-only dataset (max 200 epochs)...\n",
      "Epoch 1/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.7486 - loss: 0.5977 - val_accuracy: 0.8165 - val_loss: 0.3970\n",
      "Epoch 2/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.4531 - val_accuracy: 0.8165 - val_loss: 0.3958\n",
      "Epoch 3/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.4155 - val_accuracy: 0.8165 - val_loss: 0.4027\n",
      "Epoch 4/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.4275 - val_accuracy: 0.8165 - val_loss: 0.3977\n",
      "Epoch 5/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.4153 - val_accuracy: 0.8165 - val_loss: 0.3967\n",
      "Epoch 6/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.4432 - val_accuracy: 0.8165 - val_loss: 0.3984\n",
      "Epoch 7/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8049 - loss: 0.4233 - val_accuracy: 0.8165 - val_loss: 0.3990\n",
      "Epoch 8/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.4103 - val_accuracy: 0.8165 - val_loss: 0.3969\n",
      "Epoch 9/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.4052 - val_accuracy: 0.8165 - val_loss: 0.3887\n",
      "Epoch 10/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.4165 - val_accuracy: 0.8165 - val_loss: 0.3984\n",
      "Epoch 11/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.3983 - val_accuracy: 0.8165 - val_loss: 0.3905\n",
      "Epoch 12/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.3887 - val_accuracy: 0.8165 - val_loss: 0.3946\n",
      "Epoch 13/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.3821 - val_accuracy: 0.8165 - val_loss: 0.3916\n",
      "Epoch 14/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.3751 - val_accuracy: 0.8165 - val_loss: 0.4034\n",
      "Epoch 15/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8049 - loss: 0.3900 - val_accuracy: 0.8165 - val_loss: 0.3974\n",
      "Epoch 16/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.3683 - val_accuracy: 0.8165 - val_loss: 0.4088\n",
      "Epoch 17/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.3703 - val_accuracy: 0.8165 - val_loss: 0.4021\n",
      "Epoch 18/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8049 - loss: 0.3655 - val_accuracy: 0.8165 - val_loss: 0.4052\n",
      "Epoch 19/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.3603 - val_accuracy: 0.8165 - val_loss: 0.4037\n",
      "Epoch 20/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.3459 - val_accuracy: 0.8165 - val_loss: 0.4173\n",
      "Epoch 21/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.3438 - val_accuracy: 0.8165 - val_loss: 0.4099\n",
      "Epoch 22/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.3794 - val_accuracy: 0.8165 - val_loss: 0.4090\n",
      "Epoch 23/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8044 - loss: 0.3608 - val_accuracy: 0.8532 - val_loss: 0.4258\n",
      "Epoch 24/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8073 - loss: 0.3580 - val_accuracy: 0.8257 - val_loss: 0.4133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 training finished.\n",
      "Model 1 saved as model_mri_only_ensemble.h5\n",
      "\n",
      "Evaluating Model 1 on MRI-only test set:\n",
      "Model 1 (MRI-only) Test Loss: 0.3887, Accuracy: 0.8165\n",
      "\n",
      "--- Training Model 2 on MRI-PET dataset (263 Features) ---\n",
      "Loaded raw data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_pet_normalized.csv. X shape: (42, 263), y shape: (42,)\n",
      "Split data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_pet_normalized.csv. Train shape: (33, 263), Test shape: (9, 263)\n",
      "Input shape for Model 2: 263\n",
      "Creating a complex, regularized classical Dense network with input shape 263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">67,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m67,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,849</span> (433.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m110,849\u001b[0m (433.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">110,849</span> (433.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m110,849\u001b[0m (433.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Early Stopping with patience=15 monitoring 'val_loss' for Model 2 training.\n",
      "Fitting Model 2 on MRI-PET dataset (max 200 epochs)...\n",
      "Epoch 1/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 226ms/step - accuracy: 0.5101 - loss: 0.6958 - val_accuracy: 0.7778 - val_loss: 0.6805\n",
      "Epoch 2/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6430 - loss: 0.6871 - val_accuracy: 0.5556 - val_loss: 0.6765\n",
      "Epoch 3/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4182 - loss: 0.6841 - val_accuracy: 0.5556 - val_loss: 0.6699\n",
      "Epoch 4/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6326 - loss: 0.6687 - val_accuracy: 0.5556 - val_loss: 0.6603\n",
      "Epoch 5/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5713 - loss: 0.6532 - val_accuracy: 0.5556 - val_loss: 0.6480\n",
      "Epoch 6/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5713 - loss: 0.6504 - val_accuracy: 0.5556 - val_loss: 0.6350\n",
      "Epoch 7/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6020 - loss: 0.6353 - val_accuracy: 0.5556 - val_loss: 0.6224\n",
      "Epoch 8/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5713 - loss: 0.6162 - val_accuracy: 0.5556 - val_loss: 0.6118\n",
      "Epoch 9/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.6346 - val_accuracy: 0.5556 - val_loss: 0.6036\n",
      "Epoch 10/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.5713 - loss: 0.6264 - val_accuracy: 0.5556 - val_loss: 0.5962\n",
      "Epoch 11/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5713 - loss: 0.6402 - val_accuracy: 0.5556 - val_loss: 0.5893\n",
      "Epoch 12/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5713 - loss: 0.6033 - val_accuracy: 0.5556 - val_loss: 0.5828\n",
      "Epoch 13/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5713 - loss: 0.6005 - val_accuracy: 0.5556 - val_loss: 0.5783\n",
      "Epoch 14/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.5842 - val_accuracy: 0.5556 - val_loss: 0.5767\n",
      "Epoch 15/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5713 - loss: 0.5952 - val_accuracy: 0.5556 - val_loss: 0.5781\n",
      "Epoch 16/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5713 - loss: 0.5755 - val_accuracy: 0.5556 - val_loss: 0.5822\n",
      "Epoch 17/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.5981 - val_accuracy: 0.5556 - val_loss: 0.5880\n",
      "Epoch 18/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5713 - loss: 0.5720 - val_accuracy: 0.5556 - val_loss: 0.5950\n",
      "Epoch 19/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.5994 - val_accuracy: 0.5556 - val_loss: 0.6009\n",
      "Epoch 20/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.6286 - val_accuracy: 0.5556 - val_loss: 0.6047\n",
      "Epoch 21/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5713 - loss: 0.6086 - val_accuracy: 0.5556 - val_loss: 0.6066\n",
      "Epoch 22/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.6276 - val_accuracy: 0.5556 - val_loss: 0.6079\n",
      "Epoch 23/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5713 - loss: 0.6080 - val_accuracy: 0.5556 - val_loss: 0.6081\n",
      "Epoch 24/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.6479 - val_accuracy: 0.5556 - val_loss: 0.6055\n",
      "Epoch 25/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.5893 - val_accuracy: 0.5556 - val_loss: 0.5991\n",
      "Epoch 26/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.6137 - val_accuracy: 0.5556 - val_loss: 0.5923\n",
      "Epoch 27/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5713 - loss: 0.6100 - val_accuracy: 0.5556 - val_loss: 0.5852\n",
      "Epoch 28/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5713 - loss: 0.5397 - val_accuracy: 0.5556 - val_loss: 0.5773\n",
      "Epoch 29/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5407 - loss: 0.5574 - val_accuracy: 0.5556 - val_loss: 0.5709\n",
      "Epoch 30/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5407 - loss: 0.5570 - val_accuracy: 0.5556 - val_loss: 0.5665\n",
      "Epoch 31/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5713 - loss: 0.5924 - val_accuracy: 0.5556 - val_loss: 0.5621\n",
      "Epoch 32/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5713 - loss: 0.5811 - val_accuracy: 0.6667 - val_loss: 0.5567\n",
      "Epoch 33/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6020 - loss: 0.5624 - val_accuracy: 0.7778 - val_loss: 0.5501\n",
      "Epoch 34/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6326 - loss: 0.5311 - val_accuracy: 0.6667 - val_loss: 0.5447\n",
      "Epoch 35/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5713 - loss: 0.6028 - val_accuracy: 0.6667 - val_loss: 0.5353\n",
      "Epoch 36/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.6326 - loss: 0.4960 - val_accuracy: 0.6667 - val_loss: 0.5303\n",
      "Epoch 37/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6326 - loss: 0.5107 - val_accuracy: 0.6667 - val_loss: 0.5322\n",
      "Epoch 38/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5101 - loss: 0.5056 - val_accuracy: 0.6667 - val_loss: 0.5354\n",
      "Epoch 39/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5713 - loss: 0.5117 - val_accuracy: 0.6667 - val_loss: 0.5399\n",
      "Epoch 40/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6326 - loss: 0.4941 - val_accuracy: 0.6667 - val_loss: 0.5460\n",
      "Epoch 41/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6020 - loss: 0.5145 - val_accuracy: 0.6667 - val_loss: 0.5491\n",
      "Epoch 42/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6632 - loss: 0.4982 - val_accuracy: 0.6667 - val_loss: 0.5517\n",
      "Epoch 43/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6938 - loss: 0.5181 - val_accuracy: 0.6667 - val_loss: 0.5487\n",
      "Epoch 44/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5713 - loss: 0.5367 - val_accuracy: 0.6667 - val_loss: 0.5398\n",
      "Epoch 45/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.6020 - loss: 0.5040 - val_accuracy: 0.6667 - val_loss: 0.5276\n",
      "Epoch 46/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6632 - loss: 0.5339 - val_accuracy: 0.6667 - val_loss: 0.5151\n",
      "Epoch 47/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7244 - loss: 0.4620 - val_accuracy: 0.6667 - val_loss: 0.5063\n",
      "Epoch 48/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8163 - loss: 0.4755 - val_accuracy: 0.6667 - val_loss: 0.4980\n",
      "Epoch 49/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7857 - loss: 0.4951 - val_accuracy: 0.6667 - val_loss: 0.4860\n",
      "Epoch 50/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6326 - loss: 0.5093 - val_accuracy: 0.6667 - val_loss: 0.4778\n",
      "Epoch 51/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6938 - loss: 0.4819 - val_accuracy: 0.6667 - val_loss: 0.4725\n",
      "Epoch 52/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6020 - loss: 0.4806 - val_accuracy: 0.6667 - val_loss: 0.4702\n",
      "Epoch 53/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7244 - loss: 0.4680 - val_accuracy: 0.6667 - val_loss: 0.4690\n",
      "Epoch 54/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8163 - loss: 0.4343 - val_accuracy: 0.6667 - val_loss: 0.4671\n",
      "Epoch 55/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7551 - loss: 0.4782 - val_accuracy: 0.7778 - val_loss: 0.4611\n",
      "Epoch 56/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7551 - loss: 0.4356 - val_accuracy: 0.7778 - val_loss: 0.4592\n",
      "Epoch 57/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7244 - loss: 0.4914 - val_accuracy: 0.7778 - val_loss: 0.4601\n",
      "Epoch 58/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7551 - loss: 0.4352 - val_accuracy: 0.7778 - val_loss: 0.4617\n",
      "Epoch 59/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7857 - loss: 0.4516 - val_accuracy: 0.6667 - val_loss: 0.4636\n",
      "Epoch 60/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6632 - loss: 0.4335 - val_accuracy: 0.6667 - val_loss: 0.4669\n",
      "Epoch 61/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7551 - loss: 0.4710 - val_accuracy: 0.6667 - val_loss: 0.4701\n",
      "Epoch 62/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7551 - loss: 0.4677 - val_accuracy: 0.6667 - val_loss: 0.4710\n",
      "Epoch 63/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7857 - loss: 0.4490 - val_accuracy: 0.6667 - val_loss: 0.4709\n",
      "Epoch 64/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7244 - loss: 0.4998 - val_accuracy: 0.6667 - val_loss: 0.4655\n",
      "Epoch 65/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6938 - loss: 0.4664 - val_accuracy: 0.7778 - val_loss: 0.4623\n",
      "Epoch 66/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8163 - loss: 0.4364 - val_accuracy: 0.7778 - val_loss: 0.4606\n",
      "Epoch 67/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7857 - loss: 0.4025 - val_accuracy: 0.7778 - val_loss: 0.4619\n",
      "Epoch 68/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7244 - loss: 0.4482 - val_accuracy: 0.6667 - val_loss: 0.4665\n",
      "Epoch 69/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7857 - loss: 0.4784 - val_accuracy: 0.6667 - val_loss: 0.4680\n",
      "Epoch 70/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7857 - loss: 0.4385 - val_accuracy: 0.6667 - val_loss: 0.4695\n",
      "Epoch 71/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7551 - loss: 0.4604 - val_accuracy: 0.7778 - val_loss: 0.4659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 training finished.\n",
      "Model 2 saved as model_mri_pet_ensemble.h5\n",
      "\n",
      "Evaluating Model 2 on MRI-PET test set:\n",
      "Model 2 (MRI-PET) Test Loss: 0.4592, Accuracy: 0.7778\n",
      "\n",
      "--- Ensembling Predictions ---\n",
      "Models trained independently on datasets with different feature counts.\n",
      "\n",
      "To ensemble these models for a final prediction on a new sample, you would need:\n",
      "1. The 678-feature representation of the new sample (processed like MRI-only data).\n",
      "2. The 263-feature representation of the new sample (processed like MRI-PET data).\n",
      "3. Feed the 678-features to Model 1 (loaded from model_mri_only_ensemble.h5).\n",
      "4. Feed the 263-features to Model 2 (loaded from model_mri_pet_ensemble.h5).\n",
      "5. Combine their predicted probabilities (e.g., average) to get the final ensemble prediction.\n",
      "\n",
      "Example: Getting predictions on respective test sets:\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "Predictions from Model 1 (MRI-only) on its test set (shape: (109, 1))\n",
      "Predictions from Model 2 (MRI-PET) on its test set (shape: (9, 1))\n",
      "\n",
      "Script execution finished.\n",
      "Two models trained and saved for ensembling.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler # Not using StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os # Import os for path joining\n",
    "from sklearn.metrics import accuracy_score # To evaluate the combined prediction\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use os.path.join for better path handling across different OS\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\"\n",
    "MRI_FILE = \"mri_only_normalized.csv\"\n",
    "MRI_PET_FILE = \"mri_pet_normalized.csv\"\n",
    "\n",
    "MRI_PATH = os.path.join(BASE_DATA_DIR, MRI_FILE)\n",
    "MRI_PET_PATH = os.path.join(BASE_DATA_DIR, MRI_PET_FILE)\n",
    "\n",
    "MODEL_MRI_SAVE_PATH = \"model_mri_only_ensemble.h5\"\n",
    "MODEL_MRI_PET_SAVE_PATH = \"model_mri_pet_ensemble.h5\"\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42 # Use different random seeds for different models for diversity\n",
    "RANDOM_STATE_MODEL_1 = 42\n",
    "RANDOM_STATE_MODEL_2 = 123 # Different seed for the second model\n",
    "\n",
    "TASK_TYPE = 'classification' # Assuming classification for probability averaging\n",
    "\n",
    "# Training Hyperparameters (can tune these for each model)\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "MAX_EPOCHS = 200 # Set a max, Early Stopping will likely stop earlier\n",
    "EARLY_STOPPING_PATIENCE = 15 # How many epochs to wait for validation improvement\n",
    "\n",
    "# --- Function to Create Complex Classical Model Architecture with Regularization ---\n",
    "# We'll use this base architecture for both models, but they will have different input layers\n",
    "def create_complex_regularized_classical_model(input_shape):\n",
    "    \"\"\"\n",
    "    Function to create a complex classical neural network model with regularization.\n",
    "    Designed for good performance and generalization using Dense layers.\n",
    "    Input shape is determined by the dataset used for training.\n",
    "    \"\"\"\n",
    "    print(f\"Creating a complex, regularized classical Dense network with input shape {input_shape}\")\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)), # Input shape is set here\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.4), # Added Dropout layer (experiment with rate, e.g., 0.2 to 0.5)\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        # Add more layers/neurons/dropout as needed, but balance complexity with data\n",
    "        Dense(1, activation='sigmoid' if TASK_TYPE == 'classification' else None) # Sigmoid for binary classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# --- Helper Function to Load Data and Split ---\n",
    "def load_data_and_split(file_path, test_size, random_state, task_type):\n",
    "    \"\"\"\n",
    "    Loads data from CSV, separates features and labels, and splits into train/test sets.\n",
    "    Assumes the first column is an ID and the last column is the label.\n",
    "    Assumes data is already normalized and DOES NOT use StandardScaler.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # --- CRITICAL FIX for Non-Numeric Columns ---\n",
    "    # Assuming the first column (index 0) is an ID column\n",
    "    # and the last column is the numerical label.\n",
    "    # Select features from the second column (index 1) up to, but NOT including, the last column.\n",
    "    # If your files have a different structure, you MUST adjust these lines accordingly.\n",
    "    try:\n",
    "        X = data.iloc[:, 1:-1].values # Features exclude the first column (index 0)\n",
    "        y = data.iloc[:, -1].values  # Label is the last column\n",
    "        print(f\"Loaded raw data from {file_path}. X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "        # Optional check: Verify the feature matrix contains only numeric types\n",
    "        if not np.issubdtype(X.dtype, np.number):\n",
    "             print(f\"Error: Feature matrix from {file_path} still contains non-numeric data after slicing. Data type: {X.dtype}\")\n",
    "             print(\"Please inspect this CSV file for non-numeric values in columns other than the first and last.\")\n",
    "             return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"Error: Indexing failed for {file_path}. Check if your file has enough columns (at least 3: ID, Feature(s), Label).\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during loading {file_path}: {e}\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    # --- END CRITICAL FIX ---\n",
    "\n",
    "    # If classification and labels are not 0/1, convert them\n",
    "    if task_type == 'classification' and np.unique(y).size == 2 and not all(np.isin(y, [0, 1])):\n",
    "         print(f\"Warning: Assuming binary classification, converting labels {np.unique(y)} to 0/1.\")\n",
    "         unique_labels = np.unique(y)\n",
    "         label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "         y = np.array([label_map[label] for label in y])\n",
    "\n",
    "    # Split data, using stratification for classification to maintain label distribution\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y if task_type == 'classification' else None\n",
    "    )\n",
    "\n",
    "    print(f\"Split data from {file_path}. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# --- Train Model 1 (on MRI-only data) ---\n",
    "\n",
    "print(\"--- Training Model 1 on MRI-only dataset ({} Features) ---\".format(678)) # Hardcoded based on previous error\n",
    "\n",
    "# Load and split MRI-only data\n",
    "X_train_mri, X_test_mri, y_train_mri, y_test_mri = load_data_and_split(\n",
    "    MRI_PATH, TEST_SIZE, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri is None:\n",
    "    exit()\n",
    "\n",
    "input_shape_mri = X_train_mri.shape[1]\n",
    "print(f\"Input shape for Model 1: {input_shape_mri}\")\n",
    "\n",
    "# Initialize Model 1\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_1) # Set seed for reproducibility of Model 1 training\n",
    "model_mri = create_complex_regularized_classical_model(input_shape_mri)\n",
    "model_mri.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='binary_crossentropy', # Assuming classification\n",
    "                  metrics=['accuracy'])\n",
    "model_mri.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 1\n",
    "early_stopping_mri = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 1 training.\")\n",
    "\n",
    "\n",
    "print(f\"Fitting Model 1 on MRI-only dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "history_mri = model_mri.fit(X_train_mri, y_train_mri,\n",
    "                            epochs=MAX_EPOCHS,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            validation_data=(X_test_mri, y_test_mri),\n",
    "                            callbacks=[early_stopping_mri],\n",
    "                            verbose=1)\n",
    "print(\"Model 1 training finished.\")\n",
    "\n",
    "# Save Model 1\n",
    "model_mri.save(MODEL_MRI_SAVE_PATH)\n",
    "print(f\"Model 1 saved as {MODEL_MRI_SAVE_PATH}\")\n",
    "\n",
    "# Evaluate Model 1 on its test set\n",
    "print(\"\\nEvaluating Model 1 on MRI-only test set:\")\n",
    "eval_results_mri = model_mri.evaluate(X_test_mri, y_test_mri, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 1 (MRI-only) Test Loss: {eval_results_mri[0]:.4f}, Accuracy: {eval_results_mri[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 1 (MRI-only) Test Loss (MSE): {eval_results_mri[0]:.4f}, Test MAE: {eval_results_mri[1]:.4f}\")\n",
    "\n",
    "\n",
    "# --- Train Model 2 (on MRI-PET data) ---\n",
    "\n",
    "print(\"\\n--- Training Model 2 on MRI-PET dataset ({} Features) ---\".format(263)) # Hardcoded based on previous error\n",
    "\n",
    "# Load and split MRI-PET data\n",
    "X_train_mri_pet, X_test_mri_pet, y_train_mri_pet, y_test_mri_pet = load_data_and_split(\n",
    "    MRI_PET_PATH, TEST_SIZE, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri_pet is None:\n",
    "    exit()\n",
    "\n",
    "input_shape_mri_pet = X_train_mri_pet.shape[1]\n",
    "print(f\"Input shape for Model 2: {input_shape_mri_pet}\")\n",
    "\n",
    "# Initialize Model 2 (Note: This model will have a different input layer size)\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_2) # Set a different seed for Model 2 training\n",
    "model_mri_pet = create_complex_regularized_classical_model(input_shape_mri_pet) # Uses the specific input shape\n",
    "model_mri_pet.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                      loss='binary_crossentropy', # Assuming classification\n",
    "                      metrics=['accuracy'])\n",
    "model_mri_pet.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 2\n",
    "early_stopping_mri_pet = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 2 training.\")\n",
    "\n",
    "print(f\"Fitting Model 2 on MRI-PET dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "history_mri_pet = model_mri_pet.fit(X_train_mri_pet, y_train_mri_pet,\n",
    "                                    epochs=MAX_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    validation_data=(X_test_mri_pet, y_test_mri_pet),\n",
    "                                    callbacks=[early_stopping_mri_pet],\n",
    "                                    verbose=1)\n",
    "print(\"Model 2 training finished.\")\n",
    "\n",
    "# Save Model 2\n",
    "model_mri_pet.save(MODEL_MRI_PET_SAVE_PATH)\n",
    "print(f\"Model 2 saved as {MODEL_MRI_PET_SAVE_PATH}\")\n",
    "\n",
    "# Evaluate Model 2 on its test set\n",
    "print(\"\\nEvaluating Model 2 on MRI-PET test set:\")\n",
    "eval_results_mri_pet = model_mri_pet.evaluate(X_test_mri_pet, y_test_mri_pet, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 2 (MRI-PET) Test Loss: {eval_results_mri_pet[0]:.4f}, Accuracy: {eval_results_mri_pet[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 2 (MRI-PET) Test Loss (MSE): {eval_results_mri_pet[0]:.4f}, Test MAE: {eval_results_mri_pet[1]:.4f}\")\n",
    "\n",
    "\n",
    "# --- Ensembling Predictions ---\n",
    "print(\"\\n--- Ensembling Predictions ---\")\n",
    "print(\"Models trained independently on datasets with different feature counts.\")\n",
    "\n",
    "print(\"\\nTo ensemble these models for a final prediction on a new sample, you would need:\")\n",
    "print(\"1. The 678-feature representation of the new sample (processed like MRI-only data).\")\n",
    "print(\"2. The 263-feature representation of the new sample (processed like MRI-PET data).\")\n",
    "print(\"3. Feed the 678-features to Model 1 (loaded from {}).\".format(MODEL_MRI_SAVE_PATH))\n",
    "print(\"4. Feed the 263-features to Model 2 (loaded from {}).\".format(MODEL_MRI_PET_SAVE_PATH))\n",
    "print(\"5. Combine their predicted probabilities (e.g., average) to get the final ensemble prediction.\")\n",
    "\n",
    "# Example of getting predictions on their respective test sets:\n",
    "print(\"\\nExample: Getting predictions on respective test sets:\")\n",
    "mri_only_test_predictions = model_mri.predict(X_test_mri)\n",
    "mri_pet_test_predictions = model_mri_pet.predict(X_test_mri_pet)\n",
    "\n",
    "print(f\"Predictions from Model 1 (MRI-only) on its test set (shape: {mri_only_test_predictions.shape})\")\n",
    "print(f\"Predictions from Model 2 (MRI-PET) on its test set (shape: {mri_pet_test_predictions.shape})\")\n",
    "\n",
    "# Note: You cannot directly average mri_only_test_predictions and mri_pet_test_predictions\n",
    "# because they are from different test sets (different subjects/samples).\n",
    "\n",
    "print(\"\\nScript execution finished.\")\n",
    "print(\"Two models trained and saved for ensembling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222667be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Model 1 (QNN) on MRI-only dataset (677 Features) ---\n",
      "Loaded raw data from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\\mri_only_normalized.csv. X shape: (542, 677), y shape: (542,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 187\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training Model 1 (QNN) on MRI-only dataset (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Features) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m677\u001b[39m))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Load and split MRI-only data\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m X_train_mri, X_test_mri, y_train_mri, y_test_mri \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_and_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMRI_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEST_SIZE_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRANDOM_STATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTASK_TYPE\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_train_mri \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     exit()\n",
      "Cell \u001b[1;32mIn[1], line 174\u001b[0m, in \u001b[0;36mload_data_and_split\u001b[1;34m(file_path, test_size, random_state, task_type)\u001b[0m\n\u001b[0;32m    171\u001b[0m      y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([label_map[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m y])\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Split data, using stratification for classification to maintain label distribution\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(\n\u001b[0;32m    175\u001b[0m     X, y, test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39mrandom_state, stratify\u001b[38;5;241m=\u001b[39my \u001b[38;5;28;01mif\u001b[39;00m task_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplit data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_test, y_train, y_test\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from sklearn.preprocessing import StandardScaler # Not using StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os # Import os for path joining\n",
    "from sklearn.metrics import accuracy_score # To evaluate the combined prediction\n",
    "\n",
    "# --- Import Quantum Libraries ---\n",
    "# You'll need to install a quantum library like PennyLane, Cirq, or Qiskit\n",
    "# and their TensorFlow integration if available.\n",
    "# Example using PennyLane:\n",
    "# import pennylane as qml\n",
    "# from pennylane.keras import KerasLayer # Use this if PennyLane provides a Keras layer\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\results\"\n",
    "MRI_FILE = \"mri_only_normalized.csv\"\n",
    "MRI_PET_FILE = \"mri_pet_normalized.csv\"\n",
    "\n",
    "MRI_PATH = os.path.join(BASE_DATA_DIR, MRI_FILE)\n",
    "MRI_PET_PATH = os.path.join(BASE_DATA_DIR, MRI_PET_FILE)\n",
    "\n",
    "MODEL_MRI_SAVE_PATH_QNN = \"model_mri_only_ensemble_qnn.h5\" # Updated save name\n",
    "MODEL_MRI_PET_SAVE_PATH_QNN = \"model_mri_pet_ensemble_qnn.h5\" # Updated save name\n",
    "\n",
    "TEST_SIZE_1 = 0.02\n",
    "TEST_SIZE_2 = 0.05\n",
    "RANDOM_STATE = 42\n",
    "RANDOM_STATE_MODEL_1 = 42\n",
    "RANDOM_STATE_MODEL_2 = 123\n",
    "\n",
    "TASK_TYPE = 'classification' # Assuming classification\n",
    "\n",
    "# Training Hyperparameters (can tune these for each model)\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 64 # Batch size can impact quantum memory/simulation time\n",
    "MAX_EPOCHS = 200\n",
    "EARLY_STOPPING_PATIENCE = 40\n",
    "\n",
    "# --- Quantum Configuration (Define based on your QNN design and hardware) ---\n",
    "N_QUBITS_MODEL_1 = 20 # <<< You MUST determine appropriate qubits for 677 features\n",
    "N_QUBITS_MODEL_2 = 20 # <<< You MUST determine appropriate qubits for 263 features\n",
    "# Note: These qubit counts are likely TOO SMALL for direct encoding of 677/263 features\n",
    "# with standard methods. You will need a specific encoding strategy.\n",
    "\n",
    "# Example: Choose a quantum device (simulator or hardware)\n",
    "# DEVICE_SHORT_NAME = \"default.qubit.tf\" # PennyLane simulator integrated with TF\n",
    "# DEV_MODEL_1 = qml.device(DEVICE_SHORT_NAME, wires=N_QUBITS_MODEL_1)\n",
    "# DEV_MODEL_2 = qml.device(DEVICE_SHORT_NAME, wires=N_QUBITS_MODEL_2)\n",
    "\n",
    "\n",
    "# --- Function to Create a Quantum Neural Network Model ---\n",
    "# IMPORTANT: Replace the placeholder quantum circuit logic with your actual QNN\n",
    "def create_qnn_model(input_shape, n_qubits, device):\n",
    "    \"\"\"\n",
    "    Function to create a Keras Sequential model with a Quantum Neural Network layer.\n",
    "    Replace the quantum_circuit and KerasLayer definition with your actual QNN.\n",
    "    \"\"\"\n",
    "    print(f\"Creating a QNN model with input shape {input_shape} using {n_qubits} qubits.\")\n",
    "\n",
    "    # --- Placeholder Quantum Circuit (REPLACE THIS) ---\n",
    "    # You need to define your quantum circuit that takes 'inputs' (classical data)\n",
    "    # and potentially 'weights' (trainable parameters) and returns classical results.\n",
    "    # This is a simplified example.\n",
    "\n",
    "    # @qml.qnode(device, interface=\"tensorflow\")\n",
    "    # def quantum_circuit(inputs, weights):\n",
    "    #     # Example: Angle Encoding followed by trainable layers\n",
    "    #     # Note: AngleEmbedding needs input_shape <= n_qubits for simple versions\n",
    "    #     # For high dimensions, you need a different encoding or repeat encoding layers.\n",
    "    #     # qml.AngleEmbedding(inputs, wires=range(input_shape)) # May not work for input_shape > n_qubits\n",
    "    #     # Example: Simplified encoding by repeating features or mapping subsets\n",
    "    #     # For input_shape > n_qubits, you might need to encode subsets or use a different method.\n",
    "    #     # Here's a dummy encoding assuming input_shape can be mapped to angles somehow:\n",
    "    #     for i in range(min(input_shape, n_qubits)):\n",
    "    #          qml.RY(inputs[i], wires=i) # Dummy encoding\n",
    "\n",
    "    #     # Example: Trainable quantum layers (e.g., Strongly Entangling Layers)\n",
    "    #     # num_layers = 6 # Example number of trainable layers\n",
    "    #     # qml.StronglyEntanglingLayers(weights, wires=range(n_qubits), # Example trainable layers\n",
    "    #     #                             depth=num_layers)\n",
    "\n",
    "    #     # Example: Measurement\n",
    "    #     # return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)] # Example measurements\n",
    "\n",
    "\n",
    "    # # Define weight shapes for trainable quantum layers\n",
    "    # # weight_shapes = {\"weights\": (num_layers, n_qubits, 3)} # Example shape for StronglyEntanglingLayers\n",
    "\n",
    "\n",
    "    # --- Construct Keras Model with QNN Layer (REPLACE/ADJUST THIS) ---\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_shape,)))\n",
    "\n",
    "    # Optional: Add classical layers before the QNN layer\n",
    "    # model.add(Dense(input_shape // 2, activation='relu')) # Example pre-processing classical layer\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # --- Your Quantum Layer Integration (REPLACE THIS) ---\n",
    "    # Example using PennyLane's KerasLayer:\n",
    "    # qnn_layer = KerasLayer(quantum_circuit, weight_shapes, output_dim=n_qubits) # output_dim = number of measurements\n",
    "    # model.add(qnn_layer)\n",
    "    # print(f\"Added KerasLayer with QNN circuit. Output dimension: {n_qubits}\")\n",
    "\n",
    "    # --- Placeholder Classical Layers (REMOVE THIS ONCE YOU ADD YOUR QNN) ---\n",
    "    # This is a dummy classical network that matches the structure but is NOT quantum.\n",
    "    print(\"WARNING: Using a classical Dense network placeholder for QNN. Replace with your QNN!\")\n",
    "    model.add(Dense(64, activation='relu')) # Dummy layer\n",
    "    model.add(Dropout(0.3)) # Dummy layer\n",
    "    model.add(Dense(n_qubits, activation='relu')) # Dummy layer outputting something related to n_qubits\n",
    "    # --- End of Placeholder ---\n",
    "\n",
    "\n",
    "    # Optional: Add classical layers after the QNN layer to map QNN output to final prediction\n",
    "    # Assuming QNN output_dim matches n_qubits measurements\n",
    "    model.add(Dense(32, activation='relu')) # Example post-processing classical layer\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid' if TASK_TYPE == 'classification' else None)) # Final output layer\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Helper Function to Load Data and Split ---\n",
    "# Assumes data is normalized and DOES NOT use StandardScaler.\n",
    "# Assumes the first column is an ID and the last column is the label.\n",
    "def load_data_and_split(file_path, test_size, random_state, task_type):\n",
    "    \"\"\"\n",
    "    Loads data from CSV, separates features and labels, and splits into train/test sets.\n",
    "    Assumes the first column is an ID and the last column is the label.\n",
    "    Assumes data is already normalized and DOES NOT use StandardScaler.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # --- CRITICAL FIX for Non-Numeric Columns ---\n",
    "    # Assuming the first column (index 0) is an ID column\n",
    "    # and the last column is the numerical label.\n",
    "    # Select features from the second column (index 1) up to, but NOT including, the last column.\n",
    "    # If your files have a different structure, you MUST adjust these lines accordingly.\n",
    "    try:\n",
    "        X = data.iloc[:, 1:-1].values # Features exclude the first column (index 0)\n",
    "        y = data.iloc[:, -1].values  # Label is the last column\n",
    "        print(f\"Loaded raw data from {file_path}. X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "        # Optional check: Verify the feature matrix contains only numeric types\n",
    "        if not np.issubdtype(X.dtype, np.number):\n",
    "             print(f\"Error: Feature matrix from {file_path} still contains non-numeric data after slicing. Data type: {X.dtype}\")\n",
    "             print(\"Please inspect this CSV file for non-numeric values in columns other than the first and last.\")\n",
    "             return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"Error: Indexing failed for {file_path}. Check if your file has enough columns (at least 3: ID, Feature(s), Label).\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during loading {file_path}: {e}\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    # --- END CRITICAL FIX ---\n",
    "\n",
    "    # If classification and labels are not 0/1, convert them\n",
    "    if task_type == 'classification' and np.unique(y).size == 2 and not all(np.isin(y, [0, 1])):\n",
    "         print(f\"Warning: Assuming binary classification, converting labels {np.unique(y)} to 0/1.\")\n",
    "         unique_labels = np.unique(y)\n",
    "         label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "         y = np.array([label_map[label] for label in y])\n",
    "\n",
    "    # Split data, using stratification for classification to maintain label distribution\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y if task_type == 'classification' else None\n",
    "    )\n",
    "\n",
    "    print(f\"Split data from {file_path}. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# --- Train Model 1 (QNN on MRI-only data) ---\n",
    "\n",
    "print(\"--- Training Model 1 (QNN) on MRI-only dataset ({} Features) ---\".format(677))\n",
    "\n",
    "# Load and split MRI-only data\n",
    "X_train_mri, X_test_mri, y_train_mri, y_test_mri = load_data_and_split(\n",
    "    MRI_PATH, TEST_SIZE_1, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri is None:\n",
    "    exit()\n",
    "\n",
    "input_shape_mri = X_train_mri.shape[1]\n",
    "print(f\"Input shape for Model 1 (QNN): {input_shape_mri}\") # Should be 677\n",
    "\n",
    "# Initialize Model 1 (QNN)\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_1) # Set seed for reproducibility\n",
    "# IMPORTANT: Pass the correct device and qubit count for Model 1\n",
    "# model_mri_qnn = create_qnn_model(input_shape_mri, N_QUBITS_MODEL_1, DEV_MODEL_1)\n",
    "# --- Placeholder Call (REMOVE THIS) ---\n",
    "model_mri_qnn = create_qnn_model(input_shape_mri, N_QUBITS_MODEL_1, None) # Dummy call for placeholder\n",
    "# --- End Placeholder ---\n",
    "\n",
    "\n",
    "model_mri_qnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                      loss='binary_crossentropy', # Assuming classification\n",
    "                      metrics=['accuracy'])\n",
    "model_mri_qnn.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 1 (QNN)\n",
    "early_stopping_mri_qnn = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 1 (QNN) training.\")\n",
    "\n",
    "\n",
    "print(f\"Fitting Model 1 (QNN) on MRI-only dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "# --- Training Step (May be slow/fail depending on QNN and backend) ---\n",
    "history_mri_qnn = model_mri_qnn.fit(X_train_mri, y_train_mri,\n",
    "                                    epochs=MAX_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    validation_data=(X_test_mri, y_test_mri),\n",
    "                                    callbacks=[early_stopping_mri_qnn],\n",
    "                                    verbose=1)\n",
    "print(\"Model 1 (QNN) training finished.\")\n",
    "\n",
    "# Save Model 1 (QNN)\n",
    "model_mri_qnn.save(MODEL_MRI_SAVE_PATH_QNN)\n",
    "print(f\"Model 1 (QNN) saved as {MODEL_MRI_SAVE_PATH_QNN}\")\n",
    "\n",
    "# Evaluate Model 1 (QNN) on its test set\n",
    "print(\"\\nEvaluating Model 1 (QNN) on MRI-only test set:\")\n",
    "eval_results_mri_qnn = model_mri_qnn.evaluate(X_test_mri, y_test_mri, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 1 (QNN, MRI-only) Test Loss: {eval_results_mri_qnn[0]:.4f}, Accuracy: {eval_results_mri_qnn[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 1 (QNN, MRI-only) Test Loss (MSE): {eval_results_mri_qnn[0]:.4f}, Test MAE: {eval_results_mri_qnn[1]:.4f}\")\n",
    "\n",
    "\n",
    "# --- Train Model 2 (QNN on MRI-PET data) ---\n",
    "\n",
    "print(\"\\n--- Training Model 2 (QNN) on MRI-PET dataset ({} Features) ---\".format(263))\n",
    "\n",
    "# Load and split MRI-PET data\n",
    "X_train_mri_pet, X_test_mri_pet, y_train_mri_pet, y_test_mri_pet = load_data_and_split(\n",
    "    MRI_PET_PATH, TEST_SIZE_2, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri_pet is None:\n",
    "    exit()\n",
    "\n",
    "input_shape_mri_pet = X_train_mri_pet.shape[1]\n",
    "print(f\"Input shape for Model 2 (QNN): {input_shape_mri_pet}\") # Should be 263\n",
    "\n",
    "# Initialize Model 2 (QNN)\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_2) # Set a different seed\n",
    "# IMPORTANT: Pass the correct device and qubit count for Model 2\n",
    "# model_mri_pet_qnn = create_qnn_model(input_shape_mri_pet, N_QUBITS_MODEL_2, DEV_MODEL_2)\n",
    "# --- Placeholder Call (REMOVE THIS) ---\n",
    "model_mri_pet_qnn = create_qnn_model(input_shape_mri_pet, N_QUBITS_MODEL_2, None) # Dummy call for placeholder\n",
    "# --- End Placeholder ---\n",
    "\n",
    "model_mri_pet_qnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                          loss='binary_crossentropy', # Assuming classification\n",
    "                          metrics=['accuracy'])\n",
    "model_mri_pet_qnn.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 2 (QNN)\n",
    "early_stopping_mri_pet_qnn = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 2 (QNN) training.\")\n",
    "\n",
    "print(f\"Fitting Model 2 (QNN) on MRI-PET dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "# --- Training Step (May be slow/fail depending on QNN and backend) ---\n",
    "history_mri_pet_qnn = model_mri_pet_qnn.fit(X_train_mri_pet, y_train_mri_pet,\n",
    "                                           epochs=MAX_EPOCHS,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           validation_data=(X_test_mri_pet, y_test_mri_pet),\n",
    "                                           callbacks=[early_stopping_mri_pet_qnn],\n",
    "                                           verbose=1)\n",
    "print(\"Model 2 (QNN) training finished.\")\n",
    "\n",
    "# Save Model 2 (QNN)\n",
    "model_mri_pet_qnn.save(MODEL_MRI_PET_SAVE_PATH_QNN)\n",
    "print(f\"Model 2 (QNN) saved as {MODEL_MRI_PET_SAVE_PATH_QNN}\")\n",
    "\n",
    "# Evaluate Model 2 (QNN) on its test set\n",
    "print(\"\\nEvaluating Model 2 (QNN) on MRI-PET test set:\")\n",
    "eval_results_mri_pet_qnn = model_mri_pet_qnn.evaluate(X_test_mri_pet, y_test_mri_pet, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 2 (QNN, MRI-PET) Test Loss: {eval_results_mri_pet_qnn[0]:.4f}, Accuracy: {eval_results_mri_pet_qnn[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 2 (QNN, MRI-PET) Test Loss (MSE): {eval_results_mri_pet_qnn[0]:.4f}, Test MAE: {eval_results_mri_pet_qnn[1]:.4f}\")\n",
    "\n",
    "\n",
    "# --- Ensembling Predictions ---\n",
    "print(\"\\n--- Ensembling QNN Predictions ---\")\n",
    "print(\"QNN models trained independently on datasets with different feature counts.\")\n",
    "\n",
    "print(\"\\nTo ensemble these QNN models for a final prediction on a new sample, you would need:\")\n",
    "print(\"1. The 677-feature representation of the new sample (processed like MRI-only data).\")\n",
    "print(\"2. The 263-feature representation of the new sample (processed like MRI-PET data).\")\n",
    "print(\"3. Feed the 677-features to Model 1 QNN (loaded from {}).\".format(MODEL_MRI_SAVE_PATH_QNN))\n",
    "print(\"4. Feed the 263-features to Model 2 QNN (loaded from {}).\".format(MODEL_MRI_PET_SAVE_PATH_QNN))\n",
    "print(\"5. Combine their predicted probabilities (e.g., average) to get the final ensemble prediction.\")\n",
    "\n",
    "# Example of getting predictions on their respective test sets:\n",
    "print(\"\\nExample: Getting predictions on respective test sets:\")\n",
    "# --- Prediction Steps (May be slow depending on QNN and backend) ---\n",
    "mri_only_test_predictions_qnn = model_mri_qnn.predict(X_test_mri)\n",
    "mri_pet_test_predictions_qnn = model_mri_pet_qnn.predict(X_test_mri_pet)\n",
    "\n",
    "print(f\"Predictions from Model 1 (QNN, MRI-only) on its test set (shape: {mri_only_test_predictions_qnn.shape})\")\n",
    "print(f\"Predictions from Model 2 (QNN, MRI-PET) on its test set (shape: {mri_pet_test_predictions_qnn.shape})\")\n",
    "\n",
    "# Note: You cannot directly average these predictions for a single ensemble score\n",
    "# because they are from different test sets (disjoint samples).\n",
    "\n",
    "print(\"\\nScript execution finished.\")\n",
    "print(\"Two QNN models trained and saved for ensembling based on their respective datasets.\") \n",
    "\n",
    "# Add this import at the beginning of your script\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Plotting History for Model 1 (QNN on MRI-only) ---\n",
    "print(\"\\n--- Plotting Training History for Model 1 (QNN on MRI-only) ---\")\n",
    "\n",
    "# Get the training and validation metrics from the history object\n",
    "history = history_mri_qnn # Use the history from the first model\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Determine the metric name ('accuracy' or 'mae')\n",
    "metric_name = 'accuracy' if TASK_TYPE == 'classification' else 'mae'\n",
    "train_metric = history.history[metric_name]\n",
    "val_metric = history.history[f'val_{metric_name}'] # Use f-string for val metric\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, 'bo-', label='Training loss', alpha=0.6)\n",
    "plt.plot(epochs, val_loss, 'b-', label='Validation loss')\n",
    "plt.title('Model 1 (MRI-only QNN): Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Primary Metric (Accuracy or MAE)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_metric, 'bo-', label=f'Training {metric_name.capitalize()}', alpha=0.6)\n",
    "plt.plot(epochs, val_metric, 'b-', label=f'Validation {metric_name.capitalize()}')\n",
    "plt.title(f'Model 1 (MRI-only QNN): Training and validation {metric_name.capitalize()}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(metric_name.capitalize())\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show() # Keep commented out if you want to show both model plots at the end\n",
    "\n",
    "\n",
    "# --- Plotting History for Model 2 (QNN on MRI-PET) ---\n",
    "print(\"\\n--- Plotting Training History for Model 2 (QNN on MRI-PET) ---\")\n",
    "\n",
    "# Get the training and validation metrics from the history object\n",
    "history = history_mri_pet_qnn # Use the history from the second model\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Metric name is the same as configured\n",
    "train_metric = history.history[metric_name]\n",
    "val_metric = history.history[f'val_{metric_name}']\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, 'bo-', label='Training loss', alpha=0.6)\n",
    "plt.plot(epochs, val_loss, 'b-', label='Validation loss')\n",
    "plt.title('Model 2 (MRI-PET QNN): Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Primary Metric (Accuracy or MAE)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_metric, 'bo-', label=f'Training {metric_name.capitalize()}', alpha=0.6)\n",
    "plt.plot(epochs, val_metric, 'b-', label=f'Validation {metric_name.capitalize()}')\n",
    "plt.title(f'Model 2 (MRI-PET QNN): Training and validation {metric_name.capitalize()}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(metric_name.capitalize())\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show() # Keep commented out if you want to show both model plots at the end\n",
    "\n",
    "\n",
    "# --- Show all plots ---\n",
    "plt.show() # Uncomment this single line to display all generated plots at once\n",
    "\n",
    "print(\"\\nPlotting finished.\")\n",
    "\n",
    "# --- Text Summary of Training History ---\n",
    "print(\"\\n--- Text Summary of Training History ---\")\n",
    "\n",
    "# --- Summary for Model 1 (QNN on MRI-only) ---\n",
    "print(\"\\nSummary for Model 1 (QNN on MRI-only):\")\n",
    "history_1 = history_mri_qnn # Get the history object for Model 1\n",
    "epochs_trained_1 = len(history_1.history['loss'])\n",
    "metric_name = 'accuracy' if TASK_TYPE == 'classification' else 'mae' # Determine metric name\n",
    "\n",
    "if epochs_trained_1 > 0:\n",
    "    final_train_loss_1 = history_1.history['loss'][-1]\n",
    "    final_val_loss_1 = history_1.history['val_loss'][-1]\n",
    "    final_train_metric_1 = history_1.history[metric_name][-1]\n",
    "    final_val_metric_1 = history_1.history[f'val_{metric_name}'][-1]\n",
    "\n",
    "    print(f\"  Epochs Trained: {epochs_trained_1}\")\n",
    "    print(f\"  Final Training Loss: {final_train_loss_1:.4f}\")\n",
    "    print(f\"  Final Validation Loss: {final_val_loss_1:.4f}\")\n",
    "    print(f\"  Final Training {metric_name.capitalize()}: {final_train_metric_1:.4f}\")\n",
    "    print(f\"  Final Validation {metric_name.capitalize()}: {final_val_metric_1:.4f}\")\n",
    "\n",
    "    # If using restore_best_weights=True with Early Stopping,\n",
    "    # the actual *evaluated* test performance (printed later) is based on the best epoch weights,\n",
    "    # while these history values are for the *last* epoch run.\n",
    "    # The evaluated test set results are usually more representative of the best performance.\n",
    "    print(\"  (Note: Final metrics shown are for the last epoch run. Evaluate results below reflect best epoch if Early Stopping restored weights.)\")\n",
    "\n",
    "else:\n",
    "    print(\"  No epochs were trained for Model 1.\")\n",
    "\n",
    "\n",
    "# --- Summary for Model 2 (QNN on MRI-PET) ---\n",
    "print(\"\\nSummary for Model 2 (QNN on MRI-PET):\")\n",
    "history_2 = history_mri_pet_qnn # Get the history object for Model 2\n",
    "epochs_trained_2 = len(history_2.history['loss'])\n",
    "# metric_name is already determined above\n",
    "\n",
    "if epochs_trained_2 > 0:\n",
    "    final_train_loss_2 = history_2.history['loss'][-1]\n",
    "    final_val_loss_2 = history_2.history['val_loss'][-1]\n",
    "    final_train_metric_2 = history_2.history[metric_name][-1]\n",
    "    final_val_metric_2 = history_2.history[f'val_{metric_name}'][-1]\n",
    "\n",
    "    print(f\"  Epochs Trained: {epochs_trained_2}\")\n",
    "    print(f\"  Final Training Loss: {final_train_loss_2:.4f}\")\n",
    "    print(f\"  Final Validation Loss: {final_val_loss_2:.4f}\")\n",
    "    print(f\"  Final Training {metric_name.capitalize()}: {final_train_metric_2:.4f}\")\n",
    "    print(f\"  Final Validation {metric_name.capitalize()}: {final_val_metric_2:.4f}\")\n",
    "    print(\"  (Note: Final metrics shown are for the last epoch run. Evaluate results below reflect best epoch if Early Stopping restored weights.)\")\n",
    "\n",
    "else:\n",
    "    print(\"  No epochs were trained for Model 2.\")\n",
    "\n",
    "print(\"\\n--- End of Text Summary ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b583cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Model 1 (QNN) on mri_only_normalized.csv (677 Features) ---\n",
      "Error: Data file not found at C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\mri_only_normalized.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 322\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_train_mri \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     exit()\n\u001b[1;32m--> 322\u001b[0m input_shape_mri \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_mri\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape for Model 1 (QNN): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape_mri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# Initialize Model 1 (QNN)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler # Not using StandardScaler\n",
    "# Import upsampling library - you'll need to install imbalanced-learn: pip install imbalanced-learn\n",
    "from imblearn.over_sampling import RandomOverSampler # Or SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os # Import os for path joining\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "from sklearn.metrics import accuracy_score # To evaluate the combined prediction\n",
    "\n",
    "# --- Import Quantum Libraries ---\n",
    "# You'll need to install a quantum library like PennyLane, Cirq, or Qiskit\n",
    "# and their TensorFlow integration if available.\n",
    "# Example using PennyLane:\n",
    "# import pennylane as qml\n",
    "# from pennylane.keras import KerasLayer # Use this if PennyLane provides a Keras layer\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use os.path.join for better path handling across different OS\n",
    "# Updated BASE_DATA_DIR based on your provided paths\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\"\n",
    "\n",
    "MRI_FILE = \"mri_only_normalized.csv\"\n",
    "MRI_PET_FILE = \"mri_pet_normalized.csv\"\n",
    "\n",
    "MRI_DATA_PATH = os.path.join(BASE_DATA_DIR, MRI_FILE)\n",
    "MRI_PET_DATA_PATH = os.path.join(BASE_DATA_DIR, MRI_PET_FILE)\n",
    "\n",
    "\n",
    "# Model save paths match the locations you provided (optional, but good practice)\n",
    "MODEL_MRI_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_only_ensemble_qnn.h5\")\n",
    "MODEL_MRI_PET_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_pet_ensemble_qnn.h5\")\n",
    "\n",
    "\n",
    "TEST_SIZE_1 = 0.02 # Test size for MRI-only split\n",
    "TEST_SIZE_2 = 0.05 # Test size for MRI-PET split\n",
    "RANDOM_STATE = 42 # Random state for data splitting\n",
    "RANDOM_STATE_MODEL_1 = 42 # Random state for Model 1 initialization/training\n",
    "RANDOM_STATE_MODEL_2 = 123 # Different random state for Model 2\n",
    "\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression'\n",
    "\n",
    "# Training Hyperparameters (can tune these for each model)\n",
    "LEARNING_RATE = 0.0001 # Example learning rate\n",
    "BATCH_SIZE = 64 # Example batch size (impacts QNN memory/sim time)\n",
    "MAX_EPOCHS = 200 # Maximum number of epochs for each model\n",
    "EARLY_STOPPING_PATIENCE = 40 # Patience for early stopping\n",
    "\n",
    "# --- Quantum Configuration (Define based on your QNN design and hardware) ---\n",
    "# These are placeholders. You MUST define the number of qubits needed based on\n",
    "# your input dimensionality (677 and 263) and your chosen encoding strategy and hardware limitations.\n",
    "# N_QUBITS_MODEL_1 = 20 # <<< You MUST determine appropriate qubits for 677 features\n",
    "# N_QUBITS_MODEL_2 = 20 # <<< You MUST determine appropriate qubits for 263 features\n",
    "# Note: These placeholder qubit counts are likely TOO SMALL for direct encoding of 677/263 features\n",
    "# with standard methods. You will need a specific, possibly more complex, encoding strategy.\n",
    "\n",
    "# Example: Choose a quantum device (simulator or hardware)\n",
    "# DEVICE_SHORT_NAME = \"default.qubit.tf\" # Example: PennyLane simulator integrated with TF\n",
    "# DEV_MODEL_1 = qml.device(DEVICE_SHORT_NAME, wires=N_QUBITS_MODEL_1) # Define device for Model 1\n",
    "# DEV_MODEL_2 = qml.device(DEVICE_SHORT_NAME, wires=N_QUBITS_MODEL_2) # Define device for Model 2\n",
    "\n",
    "\n",
    "# --- Function to Create a Quantum Neural Network Model ---\n",
    "# IMPORTANT: Replace the placeholder quantum circuit logic with your actual QNN\n",
    "def create_qnn_model(input_shape, n_qubits, device):\n",
    "    \"\"\"\n",
    "    Function to create a Keras Sequential model with a Quantum Neural Network layer.\n",
    "    Contains placeholder classical layers to allow the script structure to run,\n",
    "    but you MUST replace these with your actual QNN implementation.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): The number of classical input features.\n",
    "        n_qubits (int): The number of qubits the QNN will use.\n",
    "        device: The quantum device (e.g., PennyLane device).\n",
    "    \"\"\"\n",
    "    print(f\"Creating a QNN model with input shape {input_shape} using {n_qubits} qubits.\")\n",
    "\n",
    "    # --- Placeholder Quantum Circuit Definition (REPLACE THIS ENTIRE SECTION) ---\n",
    "    # You need to define your quantum circuit that takes 'inputs' (classical data)\n",
    "    # and potentially 'weights' (trainable parameters) and returns classical results (measurements).\n",
    "    # This is a simplified example structure using PennyLane syntax as a placeholder.\n",
    "\n",
    "    # @qml.qnode(device, interface=\"tensorflow\") # Decorator to make it compatible with TensorFlow\n",
    "    # def quantum_circuit(inputs, weights):\n",
    "    #     # --- Quantum Encoding (You MUST implement your encoding strategy here) ---\n",
    "    #     # How to map 'inputs' (shape=[input_shape]) to quantum gates on 'n_qubits'.\n",
    "    #     # This is the most critical part for high dimensions (677 or 263).\n",
    "    #     # Example: Simple Angle Encoding (works if input_shape <= n_qubits)\n",
    "    #     # qml.AngleEmbedding(inputs, wires=range(input_shape))\n",
    "    #     # Example: Encoding subsets or repeating features if input_shape > n_qubits\n",
    "    #     # For input_shape = 677 or 263 and n_qubits = 20, you need a custom approach.\n",
    "    #     for i in range(min(input_shape, n_qubits)):\n",
    "    #         qml.RY(inputs[i], wires=i % n_qubits) # Dummy encoding example: Map features cyclically if more features than qubits\n",
    "\n",
    "    #     # --- Trainable Quantum Layers (You MUST define your parameterized quantum circuit here) ---\n",
    "    #     # These layers contain the trainable parameters 'weights'.\n",
    "    #     # Example: Strongly Entangling Layers\n",
    "    #     # num_layers = 6 # Example number of trainable layers\n",
    "    #     # qml.StronglyEntanglingLayers(weights, wires=range(n_qubits), depth=num_layers)\n",
    "\n",
    "    #     # --- Measurement (You MUST define your measurement strategy here) ---\n",
    "    #     # How to get classical output from the quantum state.\n",
    "    #     # Example: Expectation value of PauliZ on each qubit\n",
    "    #     # return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "\n",
    "    # # Define weight shapes for trainable quantum layers (Matches the 'weights' argument in quantum_circuit)\n",
    "    # # weight_shapes = {\"weights\": (num_layers, n_qubits, 3)} # Example shape for StronglyEntanglingLayers\n",
    "\n",
    "\n",
    "    # --- Construct Keras Model with QNN Layer (REPLACE/ADJUST THIS) ---\n",
    "    # This section builds the Keras model including your quantum part.\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_shape,)))\n",
    "\n",
    "    # Optional: Add classical layers BEFORE the QNN layer to reduce dimensions or preprocess\n",
    "    # model.add(Dense(input_shape // 4, activation='relu')) # Example pre-processing classical layer\n",
    "    # model.add(Dropout(0.2))\n",
    "    # Current_dim = input_shape // 4 # Update dimension if pre-processing\n",
    "\n",
    "    # --- Your Quantum Layer Integration (REPLACE THIS SECTION) ---\n",
    "    # Integrate your quantum circuit into the Keras model using a wrapper layer.\n",
    "    # Example using PennyLane's KerasLayer:\n",
    "    # Make sure the input dimension to KerasLayer matches the output dimension of the previous layer (input_shape or Current_dim).\n",
    "    # Make sure output_dim of KerasLayer matches the number of classical measurements from your quantum_circuit.\n",
    "    # qnn_layer = KerasLayer(quantum_circuit, weight_shapes, output_dim=n_qubits, name='quantum_layer') # Example\n",
    "    # model.add(qnn_layer)\n",
    "    # print(f\"Added KerasLayer with QNN circuit. Output dimension: {n_qubits}\") # Assumes output_dim=n_qubits\n",
    "\n",
    "\n",
    "    # --- Placeholder Classical Layers (REMOVE THIS ENTIRE SECTION ONCE YOU ADD YOUR QNN) ---\n",
    "    # These layers are here ONLY to allow the script structure to run if you haven't\n",
    "    # implemented the QNN part yet. They are NOT a quantum model.\n",
    "    print(\"WARNING: Using classical Dense network placeholders for QNN. REPLACE with your QNN!\")\n",
    "    # This placeholder tries to mimic a path through dense/dropout\n",
    "    model.add(Dense(64, activation='relu', name='placeholder_dense_1'))\n",
    "    model.add(Dropout(0.3, name='placeholder_dropout_1'))\n",
    "    model.add(Dense(n_qubits, activation='relu', name='placeholder_dense_2')) # Output shape roughly like n_qubits measurements\n",
    "    # --- End of Placeholder Classical Layers ---\n",
    "\n",
    "\n",
    "    # Optional: Add classical layers AFTER the QNN layer to map QNN output to final prediction\n",
    "    # The input dimension to these layers would be the output_dim of your KerasLayer (e.g., n_qubits if measuring all qubits).\n",
    "    # model.add(Dense(32, activation='relu', name='post_qnn_dense_1')) # Example post-processing classical layer\n",
    "    # model.add(Dropout(0.2, name='post_qnn_dropout_1'))\n",
    "\n",
    "    # --- Final Output Layer ---\n",
    "    # This layer maps the output of the last quantum or classical layer to the final prediction (1 output neuron for binary classification).\n",
    "    model.add(Dense(1, activation='sigmoid' if TASK_TYPE == 'classification' else None, name='output_layer'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Helper Function to Load Data and Split ---\n",
    "# Assumes data is normalized and DOES NOT use StandardScaler.\n",
    "# Assumes the first column is an ID and the last column is the label.\n",
    "def load_data_and_split(file_path, test_size, random_state, task_type):\n",
    "    \"\"\"\n",
    "    Loads data from CSV, separates features and labels, and splits into train/test sets.\n",
    "    Assumes the first column is an ID and the last column is the label.\n",
    "    Assumes data is already normalized and DOES NOT use StandardScaler.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # --- CRITICAL FIX for Non-Numeric Columns ---\n",
    "    # Assuming the first column (index 0) is an ID column\n",
    "    # and the last column is the numerical label.\n",
    "    # Select features from the second column (index 1) up to, but NOT including, the last column.\n",
    "    # If your files have a different structure, you MUST adjust these lines accordingly.\n",
    "    try:\n",
    "        # Check if the file is empty or has only header/ID/Label\n",
    "        if data.shape[1] < 3:\n",
    "             print(f\"Error: File {file_path} does not have enough columns (expected at least ID, 1+ features, Label).\")\n",
    "             return None, None, None, None\n",
    "\n",
    "        X = data.iloc[:, 1:-1].values # Features exclude the first column (index 0)\n",
    "        y = data.iloc[:, -1].values  # Label is the last column\n",
    "        print(f\"Loaded raw data from {file_path}. X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "        # Optional check: Verify the feature matrix contains only numeric types\n",
    "        if not np.issubdtype(X.dtype, np.number):\n",
    "             print(f\"Error: Feature matrix from {file_path} still contains non-numeric data after slicing. Data type: {X.dtype}\")\n",
    "             print(\"Please inspect this CSV file for non-numeric values in columns other than the first and last.\")\n",
    "             return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"Error: Indexing failed for {file_path}. Check if your file has enough columns (at least 3: ID, Feature(s), Label).\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during loading {file_path}: {e}\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    # --- END CRITICAL FIX ---\n",
    "\n",
    "    # If classification and labels are not 0/1, convert them\n",
    "    if task_type == 'classification' and np.unique(y).size == 2 and not all(np.isin(y, [0, 1])):\n",
    "         print(f\"Warning: Assuming binary classification, converting labels {np.unique(y)} to 0/1.\")\n",
    "         unique_labels = np.unique(y)\n",
    "         label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "         y = np.array([label_map[label] for label in y])\n",
    "\n",
    "    # Split data, using stratification for classification to maintain label distribution\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y if task_type == 'classification' else None\n",
    "    )\n",
    "\n",
    "    print(f\"Split data from {file_path}. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# --- Function to Apply Upsampling ---\n",
    "def apply_upsampling(X_train, y_train, sampling_strategy='auto'):\n",
    "    \"\"\"\n",
    "    Applies RandomOverSampler upsampling to the training data.\n",
    "    This should ONLY be applied AFTER the train/test split.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training features.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        sampling_strategy (str or dict): Sampling strategy for RandomOverSampler. 'auto' balances classes.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): Upsampled training features and labels.\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying RandomOverSampler upsampling to the training data (strategy: {sampling_strategy})...\")\n",
    "    print(f\"Original training shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Original class distribution: {np.bincount(y_train)}\")\n",
    "\n",
    "    # Initialize the upsampler\n",
    "    ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Fit and resample the training data\n",
    "    X_train_upsampled, y_train_upsampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(f\"Upsampled training shape: X={X_train_upsampled.shape}, y={y_train_upsampled.shape}\")\n",
    "    print(f\"Upsampled class distribution: {np.bincount(y_train_upsampled)}\")\n",
    "    print(\"Upsampling finished.\")\n",
    "\n",
    "    return X_train_upsampled, y_train_upsampled\n",
    "\n",
    "\n",
    "# --- Text Summary of Training History ---\n",
    "def print_training_summary(history, model_name, task_type):\n",
    "    \"\"\"Prints a text summary of the training history.\"\"\"\n",
    "    print(f\"\\nSummary for {model_name}:\")\n",
    "    epochs_trained = len(history.history['loss'])\n",
    "    metric_name = 'accuracy' if task_type == 'classification' else 'mae'\n",
    "\n",
    "    if epochs_trained > 0:\n",
    "        final_train_loss = history.history['loss'][-1]\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        final_train_metric = history.history[metric_name][-1]\n",
    "        final_val_metric = history.history[f'val_{metric_name}'][-1]\n",
    "\n",
    "        print(f\"  Epochs Trained: {epochs_trained}\")\n",
    "        print(f\"  Final Training Loss: {final_train_loss:.4f}\")\n",
    "        print(f\"  Final Validation Loss: {final_val_loss:.4f}\")\n",
    "        print(f\"  Final Training {metric_name.capitalize()}: {final_train_metric:.4f}\")\n",
    "        print(f\"  Final Validation {metric_name.capitalize()}: {final_val_metric:.4f}\")\n",
    "        print(\"  (Note: Final metrics shown are for the last epoch run. Evaluate results below reflect best epoch if Early Stopping restored weights.)\")\n",
    "    else:\n",
    "        print(f\"  No epochs were trained for {model_name}.\")\n",
    "    print(f\"--- End of {model_name} Summary ---\")\n",
    "\n",
    "\n",
    "# --- Plotting History ---\n",
    "def plot_training_history(history, model_name, task_type):\n",
    "    \"\"\"Plots training and validation loss and metrics.\"\"\"\n",
    "    print(f\"\\n--- Plotting Training History for {model_name} ---\")\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    metric_name = 'accuracy' if task_type == 'classification' else 'mae'\n",
    "    train_metric = history.history[metric_name]\n",
    "    val_metric = history.history[f'val_{metric_name}']\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, 'bo-', label='Training loss', alpha=0.6)\n",
    "    plt.plot(epochs, val_loss, 'b-', label='Validation loss')\n",
    "    plt.title(f'{model_name}: Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Primary Metric (Accuracy or MAE)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_metric, 'bo-', label=f'Training {metric_name.capitalize()}', alpha=0.6)\n",
    "    plt.plot(epochs, val_metric, 'b-', label=f'Validation {metric_name.capitalize()}')\n",
    "    plt.title(f'{model_name}: Training and validation {metric_name.capitalize()}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name.capitalize())\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show() # Keep commented out if plotting multiple models\n",
    "\n",
    "\n",
    "# --- Train Model 1 (QNN on MRI-only data - NO UPSAMPLING) ---\n",
    "\n",
    "print(\"--- Training Model 1 (QNN) on {} ({} Features) ---\".format(os.path.basename(MRI_DATA_PATH), 677))\n",
    "\n",
    "# Load and split MRI-only data\n",
    "X_train_mri, X_test_mri, y_train_mri, y_test_mri = load_data_and_split(\n",
    "    MRI_DATA_PATH, TEST_SIZE_1, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri is None:\n",
    "    exit()\n",
    "\n",
    "input_shape_mri = X_train_mri.shape[1]\n",
    "print(f\"Input shape for Model 1 (QNN): {input_shape_mri}\")\n",
    "\n",
    "# Initialize Model 1 (QNN)\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_1) # Set seed for reproducibility\n",
    "# IMPORTANT: Pass the correct device and qubit count for your QNN implementation\n",
    "# model_mri_qnn = create_qnn_model(input_shape_mri, N_QUBITS_MODEL_1, DEV_MODEL_1)\n",
    "# --- Placeholder Call (REMOVE THIS AND USE THE LINE ABOVE) ---\n",
    "# Determine placeholder qubits if N_QUBITS_MODEL_1 is not set\n",
    "n_qubits_m1 = 20 # Default for placeholder if config isn't uncommented\n",
    "# Check if N_QUBITS_MODEL_1 is defined and use it if so\n",
    "try:\n",
    "    if N_QUBITS_MODEL_1 is not None:\n",
    "        n_qubits_m1 = N_QUBITS_MODEL_1\n",
    "except NameError:\n",
    "    pass # Use default if N_QUBITS_MODEL_1 is not defined\n",
    "\n",
    "model_mri_qnn = create_qnn_model(input_shape_mri, n_qubits_m1, None) # Dummy call for placeholder\n",
    "# --- End Placeholder ---\n",
    "\n",
    "model_mri_qnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                      loss='binary_crossentropy' if TASK_TYPE == 'classification' else 'mean_squared_error',\n",
    "                      metrics=['accuracy'] if TASK_TYPE == 'classification' else ['mae'])\n",
    "model_mri_qnn.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 1 (QNN)\n",
    "early_stopping_mri_qnn = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 1 (QNN) training.\")\n",
    "\n",
    "\n",
    "print(f\"Fitting Model 1 (QNN) on MRI-only dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "# --- Training Step (May be slow/fail depending on QNN and backend) ---\n",
    "history_mri_qnn = model_mri_qnn.fit(X_train_mri, y_train_mri,\n",
    "                                    epochs=MAX_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    validation_data=(X_test_mri, y_test_mri),\n",
    "                                    callbacks=[early_stopping_mri_qnn],\n",
    "                                    verbose=1)\n",
    "print(\"Model 1 (QNN) training finished.\")\n",
    "\n",
    "# Save Model 1 (QNN) - Recommend saving in .keras format\n",
    "model_mri_qnn.save(MODEL_MRI_SAVE_PATH_QNN)\n",
    "print(f\"Model 1 (QNN) saved as {MODEL_MRI_SAVE_PATH_QNN}\")\n",
    "\n",
    "# Evaluate Model 1 (QNN) on its test set\n",
    "print(\"\\nEvaluating Model 1 (QNN) on MRI-only test set:\")\n",
    "eval_results_mri_qnn = model_mri_qnn.evaluate(X_test_mri, y_test_mri, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 1 (QNN, MRI-only) Test Loss: {eval_results_mri_qnn[0]:.4f}, Accuracy: {eval_results_mri_qnn[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 1 (QNN, MRI-only) Test Loss (MSE): {eval_results_mri_qnn[0]:.4f}, Test MAE: {eval_results_mri_qnn[1]:.4f}\")\n",
    "\n",
    "# Print and Plot Summary for Model 1\n",
    "print_training_summary(history_mri_qnn, \"Model 1 (QNN on MRI-only)\", TASK_TYPE)\n",
    "plot_training_history(history_mri_qnn, \"Model 1 (QNN on MRI-only)\", TASK_TYPE)\n",
    "\n",
    "\n",
    "# --- Train Model 2 (QNN on MRI-PET data - WITH UPSAMPLING) ---\n",
    "\n",
    "print(\"\\n--- Training Model 2 (QNN) on {} ({} Features) - WITH UPSAMPLING ---\".format(os.path.basename(MRI_PET_DATA_PATH), 263))\n",
    "\n",
    "# Load and split MRI-PET data\n",
    "X_train_mri_pet_raw, X_test_mri_pet_raw, y_train_mri_pet_raw, y_test_mri_pet_raw = load_data_and_split(\n",
    "    MRI_PET_DATA_PATH, TEST_SIZE_2, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri_pet_raw is None:\n",
    "    exit()\n",
    "\n",
    "# --- Apply Upsampling to MRI-PET Training Data (AFTER SPLIT) ---\n",
    "# This is the correct place to apply upsampling.\n",
    "X_train_mri_pet_upsampled, y_train_mri_pet_upsampled = apply_upsampling(\n",
    "    X_train_mri_pet_raw, y_train_mri_pet_raw, sampling_strategy='auto' # Use 'auto' to balance classes, or specify a dict\n",
    ")\n",
    "# Note: We train on the upsampled data but evaluate on the original test data\n",
    "\n",
    "input_shape_mri_pet = X_train_mri_pet_raw.shape[1] # Input shape is based on original features, not upsampled count\n",
    "print(f\"Input shape for Model 2 (QNN): {input_shape_mri_pet}\") # Should be 263\n",
    "\n",
    "# Initialize Model 2 (QNN) - Input shape based on original features\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_2) # Set a different seed\n",
    "# IMPORTANT: Pass the correct device and qubit count for your QNN implementation\n",
    "# model_mri_pet_qnn = create_qnn_model(input_shape_mri_pet, N_QUBITS_MODEL_2, DEV_MODEL_2)\n",
    "# --- Placeholder Call (REMOVE THIS AND USE THE LINE ABOVE) ---\n",
    "# Determine placeholder qubits if N_QUBITS_MODEL_2 is not set\n",
    "n_qubits_m2 = 20 # Default for placeholder if config isn't uncommented\n",
    "try:\n",
    "    if N_QUBITS_MODEL_2 is not None:\n",
    "        n_qubits_m2 = N_QUBITS_MODEL_2\n",
    "except NameError:\n",
    "    pass # Use default if N_QUBITS_MODEL_2 is not defined\n",
    "model_mri_pet_qnn = create_qnn_model(input_shape_mri_pet, n_qubits_m2, None) # Dummy call for placeholder\n",
    "# --- End Placeholder ---\n",
    "\n",
    "model_mri_pet_qnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                          loss='binary_crossentropy' if TASK_TYPE == 'classification' else 'mean_squared_error',\n",
    "                          metrics=['accuracy'] if TASK_TYPE == 'classification' else ['mae'])\n",
    "model_mri_pet_qnn.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 2 (QNN)\n",
    "early_stopping_mri_pet_qnn = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 2 (QNN) training.\")\n",
    "\n",
    "print(f\"Fitting Model 2 (QNN) on upsampled MRI-PET dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "# --- Training Step (May be slow/fail depending on QNN and backend) ---\n",
    "# Train using the upsampled data\n",
    "history_mri_pet_qnn = model_mri_pet_qnn.fit(X_train_mri_pet_upsampled, y_train_mri_pet_upsampled,\n",
    "                                           epochs=MAX_EPOCHS,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           validation_data=(X_test_mri_pet_raw, y_test_mri_pet_raw), # VALIDATE ON ORIGINAL TEST DATA\n",
    "                                           callbacks=[early_stopping_mri_pet_qnn],\n",
    "                                           verbose=1)\n",
    "print(\"Model 2 (QNN) training finished.\")\n",
    "\n",
    "# Save Model 2 (QNN) - Recommend saving in .keras format\n",
    "model_mri_pet_qnn.save(MODEL_MRI_PET_SAVE_PATH_QNN)\n",
    "print(f\"Model 2 (QNN) saved as {MODEL_MRI_PET_SAVE_PATH_QNN}\")\n",
    "\n",
    "# Evaluate Model 2 (QNN) on its original test set\n",
    "print(\"\\nEvaluating Model 2 (QNN) on original MRI-PET test set:\")\n",
    "eval_results_mri_pet_qnn = model_mri_pet_qnn.evaluate(X_test_mri_pet_raw, y_test_mri_pet_raw, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 2 (QNN, MRI-PET) Test Loss: {eval_results_mri_pet_qnn[0]:.4f}, Accuracy: {eval_results_mri_pet_qnn[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 2 (QNN, MRI-PET) Test Loss (MSE): {eval_results_mri_pet_qnn[0]:.4f}, Test MAE: {eval_results_mri_pet_qnn[1]:.4f}\")\n",
    "\n",
    "# Print and Plot Summary for Model 2\n",
    "print_training_summary(history_mri_pet_qnn, \"Model 2 (QNN on MRI-PET)\", TASK_TYPE)\n",
    "plot_training_history(history_mri_pet_qnn, \"Model 2 (QNN on MRI-PET)\", TASK_TYPE)\n",
    "\n",
    "\n",
    "# --- Ensembling Predictions ---\n",
    "print(\"\\n--- Ensembling QNN Predictions ---\")\n",
    "print(\"QNN models trained independently on datasets with different feature counts.\")\n",
    "\n",
    "print(\"\\nTo ensemble these QNN models for a final prediction on a new sample, you would need:\")\n",
    "print(\"1. The {} feature representation of the new sample (processed like MRI-only data).\".format(input_shape_mri))\n",
    "print(\"2. The {} feature representation of the new sample (processed like MRI-PET data).\".format(input_shape_mri_pet))\n",
    "print(\"3. Load Model 1 QNN (from {}) and feed it the {} features.\".format(MODEL_MRI_SAVE_PATH_QNN, input_shape_mri))\n",
    "print(\"4. Load Model 2 QNN (from {}) and feed it the {} features.\".format(MODEL_MRI_PET_SAVE_PATH_QNN, input_shape_mri_pet))\n",
    "print(\"5. Combine their predicted probabilities (e.g., average) to get the final ensemble prediction.\")\n",
    "\n",
    "# Example of getting predictions on their respective test sets:\n",
    "print(\"\\nExample: Getting predictions on respective test sets:\")\n",
    "# --- Prediction Steps (May be slow depending on QNN and backend) ---\n",
    "# Load models here if they weren't saved/loaded earlier in this script execution lifecycle\n",
    "# model_mri_qnn_loaded = tf.keras.models.load_model(MODEL_MRI_SAVE_PATH_QNN, custom_objects={...}) # Load if needed\n",
    "# model_mri_pet_qnn_loaded = tf.keras.models.load_model(MODEL_MRI_PET_SAVE_PATH_QNN, custom_objects={...}) # Load if needed\n",
    "\n",
    "mri_only_test_predictions_qnn = model_mri_qnn.predict(X_test_mri) # Predict using the model trained earlier\n",
    "mri_pet_test_predictions_qnn = model_mri_pet_qnn.predict(X_test_mri_pet_raw) # Predict using the model trained earlier (use raw test data)\n",
    "\n",
    "print(f\"Predictions from Model 1 (QNN, MRI-only) on its test set (shape: {mri_only_test_predictions_qnn.shape})\")\n",
    "print(f\"Predictions from Model 2 (QNN, MRI-PET) on its ORIGINAL test set (shape: {mri_pet_test_predictions_qnn.shape})\")\n",
    "\n",
    "# Note: You cannot directly average these predictions for a single ensemble score\n",
    "# because they are from different test sets (disjoint samples).\n",
    "\n",
    "# --- Show all plots ---\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScript execution finished.\")\n",
    "print(\"Two QNN models trained and saved for ensembling based on their respective datasets, with upsampling on the MRI-PET training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Model 1 (QNN) on mri_only_normalized.csv (677 Features) ---\n",
      "Error: Data file not found at C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\mri_only_normalized.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 355\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_train_mri \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m     exit()\n\u001b[1;32m--> 355\u001b[0m input_shape_mri \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_mri\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape for Model 1 (QNN): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape_mri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# Initialize Model 1 (QNN)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler # Not using StandardScaler\n",
    "# Import upsampling library - you'll need to install imbalanced-learn: pip install imbalanced-learn\n",
    "from imblearn.over_sampling import RandomOverSampler # Or SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os # Import os for path joining\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "from sklearn.metrics import accuracy_score # To evaluate the combined prediction\n",
    "\n",
    "# --- Import Quantum Libraries ---\n",
    "# You'll need to install a quantum library like PennyLane, Cirq, or Qiskit\n",
    "# and their TensorFlow integration if available.\n",
    "# Example using PennyLane:\n",
    "# import pennylane as qml\n",
    "# from pennylane.keras import KerasLayer # Use this if PennyLane provides a Keras layer\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use os.path.join for better path handling across different OS\n",
    "# Updated BASE_DATA_DIR based on your provided paths\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\"\n",
    "\n",
    "MRI_FILE = \"mri_only_normalized.csv\"\n",
    "MRI_PET_FILE = \"mri_pet_normalized.csv\"\n",
    "\n",
    "MRI_DATA_PATH = os.path.join(BASE_DATA_DIR, MRI_FILE)\n",
    "MRI_PET_DATA_PATH = os.path.join(BASE_DATA_DIR, MRI_PET_FILE)\n",
    "\n",
    "\n",
    "# Model save paths match the locations you provided (optional, but good practice)\n",
    "MODEL_MRI_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_only_ensemble_qnn.h5\")\n",
    "MODEL_MRI_PET_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_pet_ensemble_qnn.h5\")\n",
    "\n",
    "\n",
    "TEST_SIZE_1 = 0.02 # Test size for MRI-only split\n",
    "TEST_SIZE_2 = 0.05 # Test size for MRI-PET split\n",
    "RANDOM_STATE = 42 # Random state for data splitting\n",
    "RANDOM_STATE_MODEL_1 = 42 # Random state for Model 1 initialization/training\n",
    "RANDOM_STATE_MODEL_2 = 123 # Different random state for Model 2\n",
    "\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression'\n",
    "\n",
    "# Training Hyperparameters (can tune these for each model)\n",
    "LEARNING_RATE = 0.0001 # Example learning rate\n",
    "BATCH_SIZE = 64 # Example batch size (impacts QNN memory/sim time)\n",
    "MAX_EPOCHS = 200 # Maximum number of epochs for each model\n",
    "EARLY_STOPPING_PATIENCE = 40 # Patience for early stopping\n",
    "\n",
    "# --- Quantum Configuration (Define based on your QNN design and hardware) ---\n",
    "# These are placeholders. You MUST define the number of qubits needed based on\n",
    "# your input dimensionality (677 and 263) and your chosen encoding strategy and hardware limitations.\n",
    "# N_QUBITS_MODEL_1 = 20 # <<< You MUST determine appropriate qubits for 677 features\n",
    "# N_QUBITS_MODEL_2 = 20 # <<< You MUST determine appropriate qubits for 263 features\n",
    "# Note: These placeholder qubit counts are likely TOO SMALL for direct encoding of 677/263 features\n",
    "# with standard methods. You will need a specific, possibly more complex, encoding strategy.\n",
    "\n",
    "# Example: Choose a quantum device (simulator or hardware)\n",
    "# DEVICE_SHORT_NAME = \"default.qubit.tf\" # Example: PennyLane simulator integrated with TF\n",
    "# DEV_MODEL_1 = qml.device(DEVICE_SHORT_NAME, wires=N_QUBITS_MODEL_1) # Define device for Model 1\n",
    "# DEV_MODEL_2 = qml.device(DEVICE_SHORT_NAME, wires=N_QUBITS_MODEL_2) # Define device for Model 2\n",
    "\n",
    "\n",
    "# --- Function to Create a Quantum Neural Network Model ---\n",
    "# IMPORTANT: Replace the placeholder quantum circuit logic with your actual QNN\n",
    "def create_qnn_model(input_shape, n_qubits, device):\n",
    "    \"\"\"\n",
    "    Function to create a Keras Sequential model with a Quantum Neural Network layer.\n",
    "    Contains placeholder classical layers to allow the script structure to run,\n",
    "    but you MUST replace these with your actual QNN implementation.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): The number of classical input features.\n",
    "        n_qubits (int): The number of qubits the QNN will use.\n",
    "        device: The quantum device (e.g., PennyLane device).\n",
    "    \"\"\"\n",
    "    print(f\"Creating a QNN model with input shape {input_shape} using {n_qubits} qubits.\")\n",
    "\n",
    "    # --- Placeholder Quantum Circuit Definition (REPLACE THIS ENTIRE SECTION) ---\n",
    "    # You need to define your quantum circuit that takes 'inputs' (classical data)\n",
    "    # and potentially 'weights' (trainable parameters) and returns classical results (measurements).\n",
    "    # This is a simplified example structure using PennyLane syntax as a placeholder.\n",
    "\n",
    "    # @qml.qnode(device, interface=\"tensorflow\") # Decorator to make it compatible with TensorFlow\n",
    "    # def quantum_circuit(inputs, weights):\n",
    "    #     # --- Quantum Encoding (You MUST implement your encoding strategy here) ---\n",
    "    #     # How to map 'inputs' (shape=[input_shape]) to quantum gates on 'n_qubits'.\n",
    "    #     # This is the most critical part for high dimensions (677 or 263).\n",
    "    #     # Example: Simple Angle Encoding (works if input_shape <= n_qubits)\n",
    "    #     # qml.AngleEmbedding(inputs, wires=range(input_shape))\n",
    "    #     # Example: Encoding subsets or repeating features if input_shape > n_qubits\n",
    "    #     # For input_shape = 677 or 263 and n_qubits = 20, you need a custom approach.\n",
    "    #     for i in range(min(input_shape, n_qubits)):\n",
    "    #         qml.RY(inputs[i], wires=i % n_qubits) # Dummy encoding example: Map features cyclically if more features than qubits\n",
    "\n",
    "    #     # --- Trainable Quantum Layers (You MUST define your parameterized quantum circuit here) ---\n",
    "    #     # These layers contain the trainable parameters 'weights'.\n",
    "    #     # Example: Strongly Entangling Layers\n",
    "    #     # num_layers = 6 # Example number of trainable layers\n",
    "    #     # qml.StronglyEntanglingLayers(weights, wires=range(n_qubits), depth=num_layers)\n",
    "\n",
    "    #     # --- Measurement (You MUST define your measurement strategy here) ---\n",
    "    #     # How to get classical output from the quantum state.\n",
    "    #     # Example: Expectation value of PauliZ on each qubit\n",
    "    #     # return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "\n",
    "    # # Define weight shapes for trainable quantum layers (Matches the 'weights' argument in quantum_circuit)\n",
    "    # # weight_shapes = {\"weights\": (num_layers, n_qubits, 3)} # Example shape for StronglyEntanglingLayers\n",
    "\n",
    "\n",
    "    # --- Construct Keras Model with QNN Layer (REPLACE/ADJUST THIS) ---\n",
    "    # This section builds the Keras model including your quantum part.\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_shape,)))\n",
    "\n",
    "    # Optional: Add classical layers BEFORE the QNN layer to reduce dimensions or preprocess\n",
    "    # model.add(Dense(input_shape // 4, activation='relu')) # Example pre-processing classical layer\n",
    "    # model.add(Dropout(0.2))\n",
    "    # Current_dim = input_shape // 4 # Update dimension if pre-processing\n",
    "\n",
    "    # --- Your Quantum Layer Integration (REPLACE THIS SECTION) ---\n",
    "    # Integrate your quantum circuit into the Keras model using a wrapper layer.\n",
    "    # Example using PennyLane's KerasLayer:\n",
    "    # Make sure the input dimension to KerasLayer matches the output dimension of the previous layer (input_shape or Current_dim).\n",
    "    # Make sure output_dim of KerasLayer matches the number of classical measurements from your quantum_circuit.\n",
    "    # qnn_layer = KerasLayer(quantum_circuit, weight_shapes, output_dim=n_qubits, name='quantum_layer') # Example\n",
    "    # model.add(qnn_layer)\n",
    "    # print(f\"Added KerasLayer with QNN circuit. Output dimension: {n_qubits}\") # Assumes output_dim=n_qubits\n",
    "\n",
    "\n",
    "    # --- Placeholder Classical Layers (REMOVE THIS ENTIRE SECTION ONCE YOU ADD YOUR QNN) ---\n",
    "    # These layers are here ONLY to allow the script structure to run if you haven't\n",
    "    # implemented the QNN part yet. They are NOT a quantum model.\n",
    "    print(\"WARNING: Using classical Dense network placeholders for QNN. REPLACE with your QNN!\")\n",
    "    # This placeholder tries to mimic a path through dense/dropout\n",
    "    model.add(Dense(64, activation='relu', name='placeholder_dense_1'))\n",
    "    model.add(Dropout(0.3, name='placeholder_dropout_1'))\n",
    "    model.add(Dense(n_qubits, activation='relu', name='placeholder_dense_2')) # Output shape roughly like n_qubits measurements\n",
    "    # --- End of Placeholder Classical Layers ---\n",
    "\n",
    "\n",
    "    # Optional: Add classical layers AFTER the QNN layer to map QNN output to final prediction\n",
    "    # The input dimension to these layers would be the output_dim of your KerasLayer (e.g., n_qubits if measuring all qubits).\n",
    "    # model.add(Dense(32, activation='relu', name='post_qnn_dense_1')) # Example post-processing classical layer\n",
    "    # model.add(Dropout(0.2, name='post_qnn_dropout_1'))\n",
    "\n",
    "    # --- Final Output Layer ---\n",
    "    # This layer maps the output of the last quantum or classical layer to the final prediction (1 output neuron for binary classification).\n",
    "    model.add(Dense(1, activation='sigmoid' if TASK_TYPE == 'classification' else None, name='output_layer'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Helper Function to Load Data and Split ---\n",
    "# Assumes data is normalized and DOES NOT use StandardScaler.\n",
    "# Assumes the first column is an ID and the last column is the label.\n",
    "def load_data_and_split(file_path, test_size, random_state, task_type):\n",
    "    \"\"\"\n",
    "    Loads data from CSV, separates features and labels, and splits into train/test sets.\n",
    "    Assumes the first column is an ID and the last column is the label.\n",
    "    Assumes data is already normalized and DOES NOT use StandardScaler.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # --- CRITICAL FIX for Non-Numeric Columns ---\n",
    "    # Assuming the first column (index 0) is an ID column\n",
    "    # and the last column is the numerical label.\n",
    "    # Select features from the second column (index 1) up to, but NOT including, the last column.\n",
    "    # If your files have a different structure, you MUST adjust these lines accordingly.\n",
    "    try:\n",
    "        # Check if the file is empty or has only header/ID/Label\n",
    "        if data.shape[1] < 2: # Needs at least features and a label\n",
    "             print(f\"Error: File {file_path} does not have enough columns (expected at least 1 feature, Label).\")\n",
    "             return None, None, None, None\n",
    "        # Adjust slicing if the first column is NOT always an ID to be skipped\n",
    "        # If no ID, use data.iloc[:, :-1].values for X\n",
    "        X = data.iloc[:, 1:-1].values # Features exclude the first column (index 0)\n",
    "        y = data.iloc[:, -1].values  # Label is the last column\n",
    "\n",
    "        # Double check if iloc[:, 1:-1] resulted in 0 features, meaning maybe only ID and Label were present?\n",
    "        if X.shape[1] == 0:\n",
    "             print(f\"Error: iloc[:, 1:-1] resulted in 0 features for {file_path}. Check your column structure. Expected > 0 features.\")\n",
    "             # As a fallback, if only 2 columns total (ID, Label), iloc[:, 1:-1] fails.\n",
    "             # If only 2 columns, X should be empty. If 3 columns (ID, Feature, Label), X is data[:, 1].\n",
    "             # Let's add a check based on original columns and expected features.\n",
    "             expected_features = data.shape[1] - 2 # Assuming ID and Label are removed\n",
    "             if expected_features > 0 and X.shape[1] == 0:\n",
    "                 print(f\"Warning: Assuming format ID,Feature1,...,FeatureN,Label. With {data.shape[1]} total columns, expected {expected_features} features.\")\n",
    "                 print(\"Current iloc[:, 1:-1] might be wrong if ID is not the first column or there are no features between ID and Label.\")\n",
    "                 # Suggest inspecting data.columns and data.head()\n",
    "\n",
    "             return None, None, None, None # Still return None as data loading failed logic check\n",
    "\n",
    "\n",
    "        print(f\"Loaded raw data from {file_path}. X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "\n",
    "        # Optional check: Verify the feature matrix contains only numeric types\n",
    "        if not np.issubdtype(X.dtype, np.number):\n",
    "             print(f\"Error: Feature matrix from {file_path} still contains non-numeric data after slicing. Data type: {X.dtype}\")\n",
    "             print(\"Please inspect this CSV file for non-numeric values in columns other than the first and last.\")\n",
    "             return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"Error: Indexing failed for {file_path}. Check if your file has enough columns (at least ID, Feature(s), Label). Minimum 2 columns total if no ID, 3 if ID is first.\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during loading {file_path}: {e}\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    # --- END CRITICAL FIX ---\n",
    "\n",
    "    # If classification and labels are not 0/1, convert them\n",
    "    if task_type == 'classification' and np.unique(y).size == 2 and not all(np.isin(y, [0, 1])):\n",
    "         print(f\"Warning: Assuming binary classification, converting labels {np.unique(y)} to 0/1.\")\n",
    "         unique_labels = np.unique(y)\n",
    "         label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "         y = np.array([label_map[label] for label in y])\n",
    "\n",
    "    # Split data, using stratification for classification to maintain label distribution\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y if task_type == 'classification' else None\n",
    "    )\n",
    "\n",
    "    print(f\"Split data from {file_path}. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# --- Function to Apply Upsampling ---\n",
    "def apply_upsampling(X_train, y_train, sampling_strategy='auto'):\n",
    "    \"\"\"\n",
    "    Applies RandomOverSampler upsampling to the training data.\n",
    "    This should ONLY be applied AFTER the train/test split.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training features.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        sampling_strategy (str or dict): Sampling strategy for RandomOverSampler. 'auto' balances classes.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): Upsampled training features and labels.\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying RandomOverSampler upsampling to the training data (strategy: {sampling_strategy})...\")\n",
    "    print(f\"Original training shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    if len(np.unique(y_train)) > 1:\n",
    "        print(f\"Original class distribution: {np.bincount(y_train)}\")\n",
    "    else:\n",
    "        print(\"Warning: Only one class found in training data. Upsampling is not applicable.\")\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "    # Initialize the upsampler\n",
    "    ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Fit and resample the training data\n",
    "    X_train_upsampled, y_train_upsampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(f\"Upsampled training shape: X={X_train_upsampled.shape}, y={y_train_upsampled.shape}\")\n",
    "    print(f\"Upsampled class distribution: {np.bincount(y_train_upsampled)}\")\n",
    "    print(\"Upsampling finished.\")\n",
    "\n",
    "    return X_train_upsampled, y_train_upsampled\n",
    "\n",
    "\n",
    "# --- Text Summary of Training History ---\n",
    "def print_training_summary(history, model_name, task_type):\n",
    "    \"\"\"Prints a text summary of the training history.\"\"\"\n",
    "    print(f\"\\nSummary for {model_name}:\")\n",
    "    epochs_trained = len(history.history.get('loss', [])) # Use .get for safety\n",
    "\n",
    "    if epochs_trained > 0:\n",
    "        final_train_loss = history.history['loss'][-1]\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        metric_name = 'accuracy' if task_type == 'classification' else 'mae'\n",
    "        final_train_metric = history.history.get(metric_name, [np.nan])[-1] # Use .get with fallback\n",
    "        final_val_metric = history.history.get(f'val_{metric_name}', [np.nan])[-1] # Use .get with fallback\n",
    "\n",
    "        print(f\"  Epochs Trained: {epochs_trained}\")\n",
    "        print(f\"  Final Training Loss: {final_train_loss:.4f}\")\n",
    "        print(f\"  Final Validation Loss: {final_val_loss:.4f}\")\n",
    "        print(f\"  Final Training {metric_name.capitalize()}: {final_train_metric:.4f}\")\n",
    "        print(f\"  Final Validation {metric_name.capitalize()}: {final_val_metric:.4f}\")\n",
    "        print(\"  (Note: Final metrics shown are for the last epoch run. Evaluate results below reflect best epoch if Early Stopping restored weights.)\")\n",
    "    else:\n",
    "        print(f\"  No epochs were trained for {model_name}. Training likely failed early.\")\n",
    "    print(f\"--- End of {model_name} Summary ---\")\n",
    "\n",
    "\n",
    "# --- Plotting History ---\n",
    "def plot_training_history(history, model_name, task_type):\n",
    "    \"\"\"Plots training and validation loss and metrics.\"\"\"\n",
    "    print(f\"\\n--- Plotting Training History for {model_name} ---\")\n",
    "    if not history.history.get('loss', []):\n",
    "        print(\"  No history data to plot.\")\n",
    "        return\n",
    "\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    metric_name = 'accuracy' if task_type == 'classification' else 'mae'\n",
    "    train_metric = history.history.get(metric_name, None) # Use .get, can be None if metric not tracked\n",
    "    val_metric = history.history.get(f'val_{metric_name}', None)\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, 'bo-', label='Training loss', alpha=0.6)\n",
    "    plt.plot(epochs, val_loss, 'b-', label='Validation loss')\n",
    "    plt.title(f'{model_name}: Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Primary Metric (Accuracy or MAE)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if train_metric is not None and val_metric is not None:\n",
    "        plt.plot(epochs, train_metric, 'bo-', label=f'Training {metric_name.capitalize()}', alpha=0.6)\n",
    "        plt.plot(epochs, val_metric, 'b-', label=f'Validation {metric_name.capitalize()}')\n",
    "        plt.title(f'{model_name}: Training and validation {metric_name.capitalize()}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric_name.capitalize())\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, f\"Metric '{metric_name}' not tracked during training.\",\n",
    "                 horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title(f'{model_name}: {metric_name.capitalize()} over epochs')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show() # Keep commented out if plotting multiple models\n",
    "\n",
    "\n",
    "# --- Train Model 1 (QNN on MRI-only data - NO UPSAMPLING) ---\n",
    "\n",
    "print(\"--- Training Model 1 (QNN) on {} ({} Features) ---\".format(os.path.basename(MRI_DATA_PATH), 677))\n",
    "\n",
    "# Load and split MRI-only data\n",
    "X_train_mri, X_test_mri, y_train_mri, y_test_mri = load_data_and_split(\n",
    "    MRI_DATA_PATH, TEST_SIZE_1, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri is None:\n",
    "    exit()\n",
    "\n",
    "input_shape_mri = X_train_mri.shape[1]\n",
    "print(f\"Input shape for Model 1 (QNN): {input_shape_mri}\")\n",
    "\n",
    "# Initialize Model 1 (QNN)\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_1) # Set seed for reproducibility\n",
    "# IMPORTANT: Pass the correct device and qubit count for your QNN implementation\n",
    "# model_mri_qnn = create_qnn_model(input_shape_mri, N_QUBITS_MODEL_1, DEV_MODEL_1)\n",
    "# --- Placeholder Call (REMOVE THIS AND USE THE LINE ABOVE) ---\n",
    "# Determine placeholder qubits if N_QUBITS_MODEL_1 is not set\n",
    "n_qubits_m1 = 20 # Default for placeholder if config isn't uncommented or is None\n",
    "try:\n",
    "    if 'N_QUBITS_MODEL_1' in locals() and N_QUBITS_MODEL_1 is not None:\n",
    "        n_qubits_m1 = N_QUBITS_MODEL_1\n",
    "except NameError:\n",
    "    pass # Use default if N_QUBITS_MODEL_1 is not defined\n",
    "\n",
    "model_mri_qnn = create_qnn_model(input_shape_mri, n_qubits_m1, None) # Dummy call for placeholder\n",
    "# --- End Placeholder ---\n",
    "\n",
    "\n",
    "model_mri_qnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                      loss='binary_crossentropy' if TASK_TYPE == 'classification' else 'mean_squared_error',\n",
    "                      metrics=['accuracy'] if TASK_TYPE == 'classification' else ['mae'])\n",
    "model_mri_qnn.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 1 (QNN)\n",
    "early_stopping_mri_qnn = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 1 (QNN) training.\")\n",
    "\n",
    "\n",
    "print(f\"Fitting Model 1 (QNN) on MRI-only dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "# --- Training Step (May be slow/fail depending on QNN and backend) ---\n",
    "history_mri_qnn = model_mri_qnn.fit(X_train_mri, y_train_mri,\n",
    "                                    epochs=MAX_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    validation_data=(X_test_mri, y_test_mri),\n",
    "                                    callbacks=[early_stopping_mri_qnn],\n",
    "                                    verbose=1)\n",
    "print(\"Model 1 (QNN) training finished.\")\n",
    "\n",
    "# Save Model 1 (QNN) - Recommend saving in .keras format\n",
    "model_mri_qnn.save(MODEL_MRI_SAVE_PATH_QNN)\n",
    "print(f\"Model 1 (QNN) saved as {MODEL_MRI_SAVE_PATH_QNN}\")\n",
    "\n",
    "# Evaluate Model 1 (QNN) on its test set\n",
    "print(\"\\nEvaluating Model 1 (QNN) on MRI-only test set:\")\n",
    "eval_results_mri_qnn = model_mri_qnn.evaluate(X_test_mri, y_test_mri, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 1 (QNN, MRI-only) Test Loss: {eval_results_mri_qnn[0]:.4f}, Accuracy: {eval_results_mri_qnn[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 1 (QNN, MRI-only) Test Loss (MSE): {eval_results_mri_qnn[0]:.4f}, Test MAE: {eval_results_mri_qnn[1]:.4f}\")\n",
    "\n",
    "# Print and Plot Summary for Model 1\n",
    "print_training_summary(history_mri_qnn, \"Model 1 (QNN on MRI-only)\", TASK_TYPE)\n",
    "plot_training_history(history_mri_qnn, \"Model 1 (QNN on MRI-only)\", TASK_TYPE)\n",
    "\n",
    "\n",
    "# --- Train Model 2 (QNN on MRI-PET data - WITH UPSAMPLING) ---\n",
    "\n",
    "print(\"\\n--- Training Model 2 (QNN) on {} ({} Features) - WITH UPSAMPLING ---\".format(os.path.basename(MRI_PET_DATA_PATH), 263))\n",
    "\n",
    "# Load and split MRI-PET data\n",
    "X_train_mri_pet_raw, X_test_mri_pet_raw, y_train_mri_pet_raw, y_test_mri_pet_raw = load_data_and_split(\n",
    "    MRI_PET_DATA_PATH, TEST_SIZE_2, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri_pet_raw is None:\n",
    "    exit()\n",
    "\n",
    "# --- Apply Upsampling to MRI-PET Training Data (AFTER SPLIT) ---\n",
    "# This is the correct place to apply upsampling.\n",
    "# Note: Upsampling is ONLY applied to the training subset.\n",
    "X_train_mri_pet_upsampled, y_train_mri_pet_upsampled = apply_upsampling(\n",
    "    X_train_mri_pet_raw, y_train_mri_pet_raw, sampling_strategy='auto' # Use 'auto' to balance classes, or specify a dict\n",
    ")\n",
    "# Note: We train on the upsampled data but validate/evaluate on the original test data\n",
    "\n",
    "input_shape_mri_pet = X_train_mri_pet_raw.shape[1] # Input shape is based on original features, not upsampled count\n",
    "print(f\"Input shape for Model 2 (QNN): {input_shape_mri_pet}\")\n",
    "\n",
    "# Initialize Model 2 (QNN) - Input shape based on original features\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_2) # Set a different seed\n",
    "# IMPORTANT: Pass the correct device and qubit count for your QNN implementation\n",
    "# model_mri_pet_qnn = create_qnn_model(input_shape_mri_pet, N_QUBITS_MODEL_2, DEV_MODEL_2)\n",
    "# --- Placeholder Call (REMOVE THIS AND USE THE LINE ABOVE) ---\n",
    "# Determine placeholder qubits if N_QUBITS_MODEL_2 is not set\n",
    "n_qubits_m2 = 20 # Default for placeholder if config isn't uncommented or is None\n",
    "try:\n",
    "    if 'N_QUBITS_MODEL_2' in locals() and N_QUBITS_MODEL_2 is not None:\n",
    "        n_qubits_m2 = N_QUBITS_MODEL_2\n",
    "except NameError:\n",
    "    pass # Use default if N_QUBITS_MODEL_2 is not defined\n",
    "model_mri_pet_qnn = create_qnn_model(input_shape_mri_pet, n_qubits_m2, None) # Dummy call for placeholder\n",
    "# --- End Placeholder ---\n",
    "\n",
    "\n",
    "model_mri_pet_qnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                          loss='binary_crossentropy' if TASK_TYPE == 'classification' else 'mean_squared_error',\n",
    "                          metrics=['accuracy'] if TASK_TYPE == 'classification' else ['mae'])\n",
    "model_mri_pet_qnn.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 2 (QNN)\n",
    "early_stopping_mri_pet_qnn = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 2 (QNN) training.\")\n",
    "\n",
    "print(f\"Fitting Model 2 (QNN) on upsampled MRI-PET dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "# --- Training Step (May be slow/fail depending on QNN and backend) ---\n",
    "# Train using the upsampled data\n",
    "history_mri_pet_qnn = model_mri_pet_qnn.fit(X_train_mri_pet_upsampled, y_train_mri_pet_upsampled,\n",
    "                                           epochs=MAX_EPOCHS,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           validation_data=(X_test_mri_pet_raw, y_test_mri_pet_raw), # VALIDATE ON ORIGINAL TEST DATA\n",
    "                                           callbacks=[early_stopping_mri_pet_qnn],\n",
    "                                           verbose=1)\n",
    "print(\"Model 2 (QNN) training finished.\")\n",
    "\n",
    "# Save Model 2 (QNN) - Recommend saving in .keras format\n",
    "model_mri_pet_qnn.save(MODEL_MRI_PET_SAVE_PATH_QNN)\n",
    "print(f\"Model 2 (QNN) saved as {MODEL_MRI_PET_SAVE_PATH_QNN}\")\n",
    "\n",
    "# Evaluate Model 2 (QNN) on its original test set\n",
    "print(\"\\nEvaluating Model 2 (QNN) on original MRI-PET test set:\")\n",
    "eval_results_mri_pet_qnn = model_mri_pet_qnn.evaluate(X_test_mri_pet_raw, y_test_mri_pet_raw, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 2 (QNN, MRI-PET) Test Loss: {eval_results_mri_pet_qnn[0]:.4f}, Accuracy: {eval_results_mri_pet_qnn[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 2 (QNN, MRI-PET) Test Loss (MSE): {eval_results_mri_pet_qnn[0]:.4f}, Test MAE: {eval_results_mri_pet_qnn[1]:.4f}\")\n",
    "\n",
    "# Print and Plot Summary for Model 2\n",
    "print_training_summary(history_mri_pet_qnn, \"Model 2 (QNN on MRI-PET)\", TASK_TYPE)\n",
    "plot_training_history(history_mri_pet_qnn, \"Model 2 (QNN on MRI-PET)\", TASK_TYPE)\n",
    "\n",
    "\n",
    "# --- Ensembling Predictions ---\n",
    "print(\"\\n--- Ensembling QNN Predictions ---\")\n",
    "print(\"QNN models trained independently on datasets with different feature counts.\")\n",
    "\n",
    "print(\"\\nTo ensemble these QNN models for a final prediction on a new sample, you would need:\")\n",
    "print(\"1. The {} feature representation of the new sample (processed like MRI-only data).\".format(input_shape_mri))\n",
    "print(\"2. The {} feature representation of the new sample (processed like MRI-PET data).\".format(input_shape_mri_pet))\n",
    "print(\"3. Load Model 1 QNN (from {}) and feed it the {} features.\".format(MODEL_MRI_SAVE_PATH_QNN, input_shape_mri))\n",
    "print(\"4. Load Model 2 QNN (from {}) and feed it the {} features.\".format(MODEL_MRI_PET_SAVE_PATH_QNN, input_shape_mri_pet))\n",
    "print(\"5. Combine their predicted probabilities (e.g., average) to get the final ensemble prediction.\")\n",
    "\n",
    "# Example of getting predictions on their respective test sets:\n",
    "print(\"\\nExample: Getting predictions on respective test sets:\")\n",
    "# --- Prediction Steps (May be slow depending on QNN and backend) ---\n",
    "# Load models here if they weren't saved/loaded earlier in this script execution lifecycle\n",
    "# model_mri_qnn_loaded = tf.keras.models.load_model(MODEL_MRI_SAVE_PATH_QNN, custom_objects={...}) # Load if needed\n",
    "# model_mri_pet_qnn_loaded = tf.keras.models.load_model(MODEL_MRI_PET_SAVE_PATH_QNN, custom_objects={...}) # Load if needed\n",
    "\n",
    "# Use the trained models directly from memory if script runs end-to-end\n",
    "mri_only_test_predictions_qnn = model_mri_qnn.predict(X_test_mri)\n",
    "mri_pet_test_predictions_qnn = model_mri_pet_qnn.predict(X_test_mri_pet_raw) # Predict using the model trained earlier (use original test data)\n",
    "\n",
    "print(f\"Predictions from Model 1 (QNN, MRI-only) on its test set (shape: {mri_only_test_predictions_qnn.shape})\")\n",
    "print(f\"Predictions from Model 2 (QNN, MRI-PET) on its ORIGINAL test set (shape: {mri_pet_test_predictions_qnn.shape})\")\n",
    "\n",
    "# Note: You cannot directly average these predictions for a single ensemble score\n",
    "# because they are from different test sets (disjoint samples).\n",
    "\n",
    "# --- Show all plots ---\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScript execution finished.\")\n",
    "print(\"Two QNN models trained and saved for ensembling based on their respective datasets, with upsampling on the MRI-PET training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bca93c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Ensemble Accuracy Calculation ---\n",
      "Successfully loaded model from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\model_mri_only_ensemble_qnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\model_mri_pet_ensemble_qnn.h5\n",
      "\n",
      "--- Loading Unified Ensemble Test Data (PLACEHOLDER) ---\n",
      "Replace this section with your code to load and prepare your unified test data.\n",
      "You need to produce three variables:\n",
      "X_ensemble_test_mri_format (NumPy array, shape: (N_test_samples, 677))\n",
      "X_ensemble_test_mri_pet_format (NumPy array, shape: (N_test_samples, 263))\n",
      "y_ensemble_test (NumPy array of true labels, shape: (N_test_samples,))\n",
      "Placeholder unified ensemble test data created for 20 samples.\n",
      "  X_ensemble_test_mri_format shape: (20, 677)\n",
      "  X_ensemble_test_mri_pet_format shape: (20, 263)\n",
      "  y_ensemble_test shape: (20,)\n",
      "\n",
      "Getting ensemble predictions for 20 samples...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Averaged predicted probabilities.\n",
      "\n",
      "--- Ensemble Accuracy ---\n",
      "Ensemble Test Accuracy: 0.6000\n",
      "-----------------------\n",
      "\n",
      "--- Ensemble Accuracy Calculation Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "# You need to import any custom layers (like KerasLayer from PennyLane)\n",
    "# that were used when saving the model if saving in HDF5 format (.h5).\n",
    "# If you saved in the native Keras format (.keras), custom_objects might not be needed.\n",
    "\n",
    "# --- Import Quantum Libraries and Custom Layers ---\n",
    "# from pennylane.keras import KerasLayer # Example if using PennyLane\n",
    "# from pennylane.templates.tensor import DenseLayer # Example if your QNN used this\n",
    "# from pennylane.templates.layers import StronglyEntanglingLayers # Example if your QNN used this\n",
    "\n",
    "# If your QNN circuit was defined within a function, you might need to import that too,\n",
    "# or ensure your KerasLayer definition is available in the current scope.\n",
    "# Example:\n",
    "# def quantum_circuit_placeholder(...):\n",
    "#    ... your quantum circuit logic ...\n",
    "#    return measurements\n",
    "\n",
    "# --- Define Custom Objects Dictionary (NEEDED IF USING .h5 SAVE WITH CUSTOM LAYERS) ---\n",
    "# custom_objects = {\n",
    "#     \"KerasLayer\": KerasLayer, # Map the class name string to the actual class\n",
    "#     # Add any other custom layers/functions used in your model definition/save\n",
    "#     # \"DenseLayer\": DenseLayer,\n",
    "#     # \"StronglyEntanglingLayers\": StronglyEntanglingLayers,\n",
    "#     # \"quantum_circuit_placeholder\": quantum_circuit_placeholder, # If needed\n",
    "# }\n",
    "\n",
    "# --- Configuration ---\n",
    "# Update BASE_DATA_DIR to where your models are saved\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\"\n",
    "MODEL_MRI_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_only_ensemble_qnn.h5\")\n",
    "MODEL_MRI_PET_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_pet_ensemble_qnn.h5\")\n",
    "\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression'\n",
    "\n",
    "# --- Function to Load Models ---\n",
    "def load_trained_model(model_path, custom_objects=None):\n",
    "    \"\"\"Loads a trained Keras model from a file.\"\"\"\n",
    "    try:\n",
    "        # If you saved as .keras, you usually don't need custom_objects\n",
    "        if model_path.lower().endswith('.keras'):\n",
    "             model = tf.keras.models.load_model(model_path) # custom_objects is often not needed here\n",
    "        else: # Assuming .h5 format requires custom_objects for custom layers\n",
    "             model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {model_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        print(\"If using .h5 with custom layers (like KerasLayer for QNN), ensure 'custom_objects' dictionary is correct.\")\n",
    "        print(\"Consider saving models in the native Keras format (.keras) in the future.\")\n",
    "        return None\n",
    "\n",
    "# --- Function to Perform Ensemble Prediction ---\n",
    "def get_ensemble_predictions(model_mri, model_mri_pet, X_data_mri_format, X_data_mri_pet_format, task_type='classification'):\n",
    "    \"\"\"\n",
    "    Gets predictions from two models and averages them.\n",
    "\n",
    "    Args:\n",
    "        model_mri: The trained model expecting MRI-only shaped features (677).\n",
    "        model_mri_pet: The trained model expecting MRI-PET shaped features (263).\n",
    "        X_data_mri_format: NumPy array of features for prediction, shaped (N_samples, 677).\n",
    "        X_data_mri_pet_format: NumPy array of features for prediction, shaped (N_samples, 263).\n",
    "        task_type: 'classification' or 'regression'.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of ensemble predictions.\n",
    "    \"\"\"\n",
    "    if model_mri is None or model_mri_pet is None:\n",
    "        print(\"Error: One or both models failed to load. Cannot perform ensemble prediction.\")\n",
    "        return None\n",
    "\n",
    "    if X_data_mri_format.shape[0] != X_data_mri_pet_format.shape[0]:\n",
    "         print(\"Error: The number of samples in the two feature arrays must be the same for ensembling.\")\n",
    "         return None\n",
    "\n",
    "    # Optional shape checks based on loaded models' input shapes (more robust)\n",
    "    expected_mri_shape = model_mri.input_shape[1] if model_mri.input_shape else 677 # Fallback if shape not readily available\n",
    "    expected_mri_pet_shape = model_mri_pet.input_shape[1] if model_mri_pet.input_shape else 263 # Fallback\n",
    "\n",
    "    if X_data_mri_format.shape[1] != expected_mri_shape:\n",
    "        print(f\"Error: MRI-only data features shape mismatch. Expected {expected_mri_shape}, got {X_data_mri_format.shape[1]}.\")\n",
    "        return None\n",
    "\n",
    "    if X_data_mri_pet_format.shape[1] != expected_mri_pet_shape:\n",
    "        print(f\"Error: MRI-PET data features shape mismatch. Expected {expected_mri_pet_shape}, got {X_data_mri_pet_format.shape[1]}.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    print(f\"\\nGetting ensemble predictions for {X_data_mri_format.shape[0]} samples...\")\n",
    "\n",
    "    # Get predictions from each model\n",
    "    # For classification, sigmoid output is already probabilities (0-1)\n",
    "    predictions_mri = model_mri.predict(X_data_mri_format)\n",
    "    predictions_mri_pet = model_mri_pet.predict(X_data_mri_pet_format)\n",
    "\n",
    "    # Combine predictions by averaging\n",
    "    if task_type == 'classification':\n",
    "        # Averaging probabilities for classification\n",
    "        ensemble_predictions = (predictions_mri + predictions_mri_pet) / 2.0\n",
    "        print(\"Averaged predicted probabilities.\")\n",
    "    else: # Regression\n",
    "        # Averaging predicted values for regression\n",
    "        ensemble_predictions = (predictions_mri + predictions_mri_pet) / 2.0\n",
    "        print(\"Averaged predicted values.\")\n",
    "\n",
    "    return ensemble_predictions\n",
    "\n",
    "# --- Main Execution for Ensemble Accuracy Calculation ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting Ensemble Accuracy Calculation ---\")\n",
    "\n",
    "    # Load the trained models\n",
    "    # Make sure custom_objects is defined if needed for .h5 files with custom layers\n",
    "    model_mri = load_trained_model(MODEL_MRI_SAVE_PATH_QNN) # , custom_objects=custom_objects\n",
    "    model_mri_pet = load_trained_model(MODEL_MRI_PET_SAVE_PATH_QNN) # , custom_objects=custom_objects\n",
    "\n",
    "    if model_mri is None or model_mri_pet is None:\n",
    "        print(\"Failed to load one or both models. Cannot calculate ensemble accuracy. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # --- PLACEHOLDER FOR UNIFIED ENSEMBLE TEST DATA ---\n",
    "    # You need to replace this section with your code to load and prepare\n",
    "    # the data for your ensemble test set.\n",
    "    # This test set must be DIFFERENT from the data used for training/validation\n",
    "    # of the individual models. Each sample in this test set must have:\n",
    "    # 1. Its 677-feature representation (X_ensemble_test_mri_format)\n",
    "    # 2. Its 263-feature representation (X_ensemble_test_mri_pet_format)\n",
    "    # 3. Its true label (y_ensemble_test)\n",
    "\n",
    "    print(\"\\n--- Loading Unified Ensemble Test Data (PLACEHOLDER) ---\")\n",
    "    print(\"Replace this section with your code to load and prepare your unified test data.\")\n",
    "    print(\"You need to produce three variables:\")\n",
    "    print(\"X_ensemble_test_mri_format (NumPy array, shape: (N_test_samples, 677))\")\n",
    "    print(\"X_ensemble_test_mri_pet_format (NumPy array, shape: (N_test_samples, 263))\")\n",
    "    print(\"y_ensemble_test (NumPy array of true labels, shape: (N_test_samples,))\")\n",
    "\n",
    "    # Example placeholder data (replace with your actual unified test data)\n",
    "    num_samples_ensemble_test = 20 # Example number of samples in your ensemble test set\n",
    "\n",
    "    # These need to be derived from your actual UNIFIED test data, not random\n",
    "    # Ensure these are DIFFERENT samples than those used to train Model 1 and Model 2.\n",
    "    X_ensemble_test_mri_format = np.random.rand(num_samples_ensemble_test, 677) # REPLACE\n",
    "    X_ensemble_test_mri_pet_format = np.random.rand(num_samples_ensemble_test, 263) # REPLACE\n",
    "    y_ensemble_test = np.random.randint(0, 2, num_samples_ensemble_test) # REPLACE (Ensure correct data type, e.g., int)\n",
    "\n",
    "    print(f\"Placeholder unified ensemble test data created for {num_samples_ensemble_test} samples.\")\n",
    "    print(f\"  X_ensemble_test_mri_format shape: {X_ensemble_test_mri_format.shape}\")\n",
    "    print(f\"  X_ensemble_test_mri_pet_format shape: {X_ensemble_test_mri_pet_format.shape}\")\n",
    "    print(f\"  y_ensemble_test shape: {y_ensemble_test.shape}\")\n",
    "    # --- END PLACEHOLDER ---\n",
    "\n",
    "\n",
    "    # Perform the ensemble prediction on the unified test data\n",
    "    ensemble_predictions_prob = get_ensemble_predictions(\n",
    "        model_mri,\n",
    "        model_mri_pet,\n",
    "        X_ensemble_test_mri_format, # Feed 677 features to Model 1\n",
    "        X_ensemble_test_mri_pet_format,  # Feed 263 features to Model 2\n",
    "        TASK_TYPE\n",
    "    )\n",
    "\n",
    "    if ensemble_predictions_prob is not None:\n",
    "        # Calculate ensemble accuracy (for classification)\n",
    "        if TASK_TYPE == 'classification':\n",
    "            # Convert probabilities to binary labels (e.g., threshold at 0.5)\n",
    "            ensemble_predicted_labels = (ensemble_predictions_prob > 0.5).astype(int)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            ensemble_accuracy = accuracy_score(y_ensemble_test, ensemble_predicted_labels)\n",
    "\n",
    "            print(f\"\\n--- Ensemble Accuracy ---\")\n",
    "            print(f\"Ensemble Test Accuracy: {ensemble_accuracy:.4f}\")\n",
    "            print(\"-----------------------\")\n",
    "\n",
    "        else: # Regression - you'd typically evaluate R2, MAE, MSE instead of accuracy\n",
    "             print(\"\\n--- Ensemble Regression Evaluation ---\")\n",
    "             print(\"For regression, you would calculate metrics like MAE, MSE, R2 here\")\n",
    "             # from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "             # mae = mean_absolute_error(y_ensemble_test, ensemble_predictions_prob)\n",
    "             # mse = mean_squared_error(y_ensemble_test, ensemble_predictions_prob)\n",
    "             # r2 = r2_score(y_ensemble_test, ensemble_predictions_prob)\n",
    "             # print(f\"Ensemble Test MAE: {mae:.4f}\")\n",
    "             # print(f\"Ensemble Test MSE: {mse:.4f}\")\n",
    "             # print(f\"Ensemble Test R2: {r2:.4f}\")\n",
    "             print(\"------------------------------------\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Ensemble Accuracy Calculation Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f616d895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Ensemble Prediction ---\n",
      "Successfully loaded model from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\model_mri_only_ensemble_qnn.h5\n",
      "Successfully loaded model from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\model_mri_pet_ensemble_qnn.h5\n",
      "\n",
      "--- Loading 50 Samples for Prediction (PLACEHOLDER) ---\n",
      "Replace this section with your code to load and preprocess your 50 samples.\n",
      "You need to produce two NumPy arrays, each with 50 rows:\n",
      "X_predict_mri_only_format (shape: (50, 677))\n",
      "X_predict_mri_pet_format (shape: (50, 263))\n",
      "Placeholder prediction data created for 50 samples.\n",
      "  X_predict_mri_only_format shape: (50, 677)\n",
      "  X_predict_mri_pet_format shape: (50, 263)\n",
      "\n",
      "Getting ensemble predictions for 50 samples...\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Averaged predicted probabilities.\n",
      "Ensemble prediction finished.\n",
      "\n",
      "Ensemble predictions generated for the 50 samples.\n",
      "Predictions (probabilities for classification):\n",
      "[[0.8115051 ]\n",
      " [0.8181024 ]\n",
      " [0.78855085]\n",
      " [0.78872085]\n",
      " [0.7955447 ]\n",
      " [0.87238   ]\n",
      " [0.8351581 ]\n",
      " [0.8372489 ]\n",
      " [0.8156339 ]\n",
      " [0.8049886 ]\n",
      " [0.8721973 ]\n",
      " [0.8239814 ]\n",
      " [0.79630363]\n",
      " [0.83113015]\n",
      " [0.81569767]\n",
      " [0.8245681 ]\n",
      " [0.80002534]\n",
      " [0.80974007]\n",
      " [0.8071028 ]\n",
      " [0.77507544]\n",
      " [0.82288945]\n",
      " [0.808218  ]\n",
      " [0.80354774]\n",
      " [0.8324473 ]\n",
      " [0.83300453]\n",
      " [0.7849752 ]\n",
      " [0.82058096]\n",
      " [0.77892447]\n",
      " [0.8450258 ]\n",
      " [0.8051753 ]\n",
      " [0.81735617]\n",
      " [0.8172627 ]\n",
      " [0.7880879 ]\n",
      " [0.83398837]\n",
      " [0.8317319 ]\n",
      " [0.8214922 ]\n",
      " [0.8533449 ]\n",
      " [0.8444177 ]\n",
      " [0.8106004 ]\n",
      " [0.80027854]\n",
      " [0.8061437 ]\n",
      " [0.8014634 ]\n",
      " [0.811486  ]\n",
      " [0.83226436]\n",
      " [0.85028315]\n",
      " [0.8135443 ]\n",
      " [0.80447316]\n",
      " [0.8247834 ]\n",
      " [0.8291424 ]\n",
      " [0.8286855 ]]\n",
      "\n",
      "Predicted binary labels:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "--- Ensemble Prediction Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "# You need to import any custom layers (like KerasLayer from PennyLane)\n",
    "# that were used when saving the model if saving in HDF5 format (.h5).\n",
    "# If you saved in the native Keras format (.keras), custom_objects might not be needed.\n",
    "\n",
    "# --- Import Quantum Libraries and Custom Layers ---\n",
    "# from pennylane.keras import KerasLayer # Example if using PennyLane\n",
    "# from pennylane.templates.tensor import DenseLayer # Example if your QNN used this\n",
    "# from pennylane.templates.layers import StronglyEntanglingLayers # Example if your QNN used this\n",
    "\n",
    "# If your QNN circuit was defined within a function, you might need to import that too,\n",
    "# or ensure your KerasLayer definition is available in the current scope.\n",
    "# Example:\n",
    "# def quantum_circuit_placeholder(...):\n",
    "#    ... your quantum circuit logic ...\n",
    "#    return measurements\n",
    "\n",
    "# --- Define Custom Objects Dictionary (NEEDED IF USING .h5 SAVE WITH CUSTOM LAYERS) ---\n",
    "# custom_objects = {\n",
    "#     \"KerasLayer\": KerasLayer, # Map the class name string to the actual class\n",
    "#     # Add any other custom layers/functions used in your model definition/save\n",
    "#     # \"DenseLayer\": DenseLayer,\n",
    "#     # \"StronglyEntanglingLayers\": StronglyEntanglingLayers,\n",
    "#     # \"quantum_circuit_placeholder\": quantum_circuit_placeholder, # If needed\n",
    "# }\n",
    "\n",
    "# --- Configuration ---\n",
    "# Update BASE_DATA_DIR to where your models are saved\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\" # Assuming models are here now\n",
    "MODEL_MRI_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_only_ensemble_qnn.h5\")\n",
    "MODEL_MRI_PET_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_pet_ensemble_qnn.h5\")\n",
    "\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression'\n",
    "\n",
    "# --- Function to Load Models ---\n",
    "def load_trained_model(model_path, custom_objects=None):\n",
    "    \"\"\"Loads a trained Keras model from a file.\"\"\"\n",
    "    try:\n",
    "        # If you saved as .keras, you usually don't need custom_objects\n",
    "        if model_path.lower().endswith('.keras'):\n",
    "             model = tf.keras.models.load_model(model_path) # custom_objects is often not needed here\n",
    "        else: # Assuming .h5 format requires custom_objects for custom layers\n",
    "             model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {model_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        print(\"If using .h5 with custom layers (like KerasLayer for QNN), ensure 'custom_objects' dictionary is correct.\")\n",
    "        print(\"Consider saving models in the native Keras format (.keras) in the future.\")\n",
    "        return None\n",
    "\n",
    "# --- Function to Perform Ensemble Prediction ---\n",
    "def get_ensemble_predictions(model_mri, model_mri_pet, X_data_mri_format, X_data_mri_pet_format, task_type='classification'):\n",
    "    \"\"\"\n",
    "    Gets predictions from two models and averages them.\n",
    "\n",
    "    Args:\n",
    "        model_mri: The trained model expecting MRI-only shaped features (677).\n",
    "        model_mri_pet: The trained model expecting MRI-PET shaped features (263).\n",
    "        X_data_mri_format: NumPy array of features for prediction, shaped (N_samples, 677).\n",
    "        X_data_mri_pet_format: NumPy array of features for prediction, shaped (N_samples, 263).\n",
    "        task_type: 'classification' or 'regression'.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of ensemble predictions.\n",
    "    \"\"\"\n",
    "    if model_mri is None or model_mri_pet is None:\n",
    "        print(\"Error: One or both models failed to load. Cannot perform ensemble prediction.\")\n",
    "        return None\n",
    "\n",
    "    if X_data_mri_format.shape[0] != X_data_mri_pet_format.shape[0]:\n",
    "         print(\"Error: The number of samples in the two feature arrays must be the same for ensembling.\")\n",
    "         return None\n",
    "\n",
    "    # Optional shape checks based on loaded models' input shapes (more robust)\n",
    "    expected_mri_shape = model_mri.input_shape[1] if model_mri.input_shape else 677 # Fallback if shape not readily available\n",
    "    expected_mri_pet_shape = model_mri_pet.input_shape[1] if model_mri_pet.input_shape else 263 # Fallback\n",
    "\n",
    "    if X_data_mri_format.shape[1] != expected_mri_shape:\n",
    "        print(f\"Error: MRI-only data features shape mismatch. Expected {expected_mri_shape}, got {X_data_mri_format.shape[1]}.\")\n",
    "        return None\n",
    "\n",
    "    if X_data_mri_pet_format.shape[1] != expected_mri_pet_shape:\n",
    "        print(f\"Error: MRI-PET data features shape mismatch. Expected {expected_mri_pet_shape}, got {X_data_mri_pet_format.shape[1]}.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    print(f\"\\nGetting ensemble predictions for {X_data_mri_format.shape[0]} samples...\")\n",
    "\n",
    "    # Get predictions from each model\n",
    "    # For classification, sigmoid output is already probabilities (0-1)\n",
    "    predictions_mri = model_mri.predict(X_data_mri_format)\n",
    "    predictions_mri_pet = model_mri_pet.predict(X_data_mri_pet_format)\n",
    "\n",
    "    # Combine predictions by averaging\n",
    "    if task_type == 'classification':\n",
    "        # Averaging probabilities for classification\n",
    "        ensemble_predictions = (predictions_mri + predictions_mri_pet) / 2.0\n",
    "        print(\"Averaged predicted probabilities.\")\n",
    "    else: # Regression\n",
    "        # Averaging predicted values for regression\n",
    "        ensemble_predictions = (predictions_mri + predictions_mri_pet) / 2.0\n",
    "        print(\"Averaged predicted values.\")\n",
    "\n",
    "    print(\"Ensemble prediction finished.\")\n",
    "    return ensemble_predictions\n",
    "\n",
    "# --- Main Execution for Ensemble Prediction ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting Ensemble Prediction ---\")\n",
    "\n",
    "    # Load the trained models\n",
    "    # Make sure custom_objects is defined if needed for .h5 files with custom layers\n",
    "    model_mri = load_trained_model(MODEL_MRI_SAVE_PATH_QNN) # , custom_objects=custom_objects\n",
    "    model_mri_pet = load_trained_model(MODEL_MRI_PET_SAVE_PATH_QNN) # , custom_objects=custom_objects\n",
    "\n",
    "    if model_mri is None or model_mri_pet is None:\n",
    "        print(\"Failed to load one or both models. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # --- PLACEHOLDER FOR YOUR 50 SAMPLES OF PREDICTION DATA ---\n",
    "    # You need to replace this section with your actual data loading and preprocessing\n",
    "    # for the 50 samples you want to predict on.\n",
    "    # The key is that X_predict_mri_only_format and X_predict_mri_pet_format\n",
    "    # MUST contain features for the EXACT SAME 50 SAMPLES.\n",
    "\n",
    "    print(\"\\n--- Loading 50 Samples for Prediction (PLACEHOLDER) ---\")\n",
    "    print(\"Replace this section with your code to load and preprocess your 50 samples.\")\n",
    "    print(\"You need to produce two NumPy arrays, each with 50 rows:\")\n",
    "    print(\"X_predict_mri_only_format (shape: (50, 677))\")\n",
    "    print(\"X_predict_mri_pet_format (shape: (50, 263))\")\n",
    "    # If you have true labels for these 50 samples (e.g., if this is a test set), also load them:\n",
    "    # y_predict_true = np.array([...]) # True labels for the 50 samples\n",
    "\n",
    "    # Example placeholder data (replace with your actual 50 samples)\n",
    "    num_samples_to_predict = 50\n",
    "\n",
    "    # These need to be derived from your actual 50 samples, not random\n",
    "    X_predict_mri_only_format = np.random.rand(num_samples_to_predict, 677) # REPLACE THIS LINE\n",
    "    X_predict_mri_pet_format = np.random.rand(num_samples_to_predict, 263) # REPLACE THIS LINE\n",
    "    # y_predict_true = np.random.randint(0, 2, num_samples_to_predict) # REPLACE/UNCOMMENT if you have true labels\n",
    "\n",
    "    print(f\"Placeholder prediction data created for {num_samples_to_predict} samples.\")\n",
    "    print(f\"  X_predict_mri_only_format shape: {X_predict_mri_only_format.shape}\")\n",
    "    print(f\"  X_predict_mri_pet_format shape: {X_predict_mri_pet_format.shape}\")\n",
    "    # if 'y_predict_true' in locals():\n",
    "    #     print(f\"  y_predict_true shape: {y_predict_true.shape}\")\n",
    "    # --- END PLACEHOLDER ---\n",
    "\n",
    "\n",
    "    # Perform the ensemble prediction for the 50 samples\n",
    "    ensemble_predictions_prob = get_ensemble_predictions(\n",
    "        model_mri,\n",
    "        model_mri_pet,\n",
    "        X_predict_mri_only_format, # Feed 677 features to Model 1\n",
    "        X_predict_mri_pet_format,  # Feed 263 features to Model 2\n",
    "        TASK_TYPE\n",
    "    )\n",
    "\n",
    "    if ensemble_predictions_prob is not None:\n",
    "        print(\"\\nEnsemble predictions generated for the 50 samples.\")\n",
    "        print(\"Predictions (probabilities for classification):\")\n",
    "        print(ensemble_predictions_prob)\n",
    "\n",
    "        if TASK_TYPE == 'classification':\n",
    "            # Convert probabilities to binary labels if needed (e.g., for interpreting results)\n",
    "            ensemble_predicted_labels = (ensemble_predictions_prob > 0.5).astype(int)\n",
    "            print(\"\\nPredicted binary labels:\")\n",
    "            print(ensemble_predicted_labels.flatten()) # Use flatten for cleaner printing\n",
    "\n",
    "            # If you loaded the true labels (y_predict_true) for these 50 samples,\n",
    "            # you can calculate the ensemble accuracy on these samples:\n",
    "            if 'y_predict_true' in locals():\n",
    "               ensemble_accuracy = accuracy_score(y_predict_true, ensemble_predicted_labels)\n",
    "               print(f\"\\nEnsemble Accuracy on these 50 samples: {ensemble_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Ensemble Prediction Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "852aeda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Ensemble Prediction and Accuracy Calculation ---\n",
      "Successfully loaded model from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\model_mri_only_ensemble_qnn.h5\n",
      "Successfully loaded model from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\model_mri_pet_ensemble_qnn.h5\n",
      "\n",
      "--- Loading 50 Samples for Prediction (PLACEHOLDER) ---\n",
      "Replace this section with your code to load and prepare your 50 samples.\n",
      "You need to produce two NumPy arrays for features, each with 50 rows:\n",
      "X_predict_mri_only_format (shape: (50, 677))\n",
      "X_predict_mri_pet_format (shape: (50, 263))\n",
      "You MUST also load the corresponding true labels into y_predict_true:\n",
      "y_predict_true (NumPy array of true labels, shape: (50,))\n",
      "Placeholder prediction data created for 50 samples.\n",
      "  X_predict_mri_only_format shape: (50, 677)\n",
      "  X_predict_mri_pet_format shape: (50, 263)\n",
      "  y_predict_true shape: (50,)\n",
      "\n",
      "Getting ensemble predictions for 50 samples...\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Averaged predicted probabilities.\n",
      "Ensemble prediction finished.\n",
      "\n",
      "Ensemble predictions generated for the 50 samples.\n",
      "Predictions (probabilities for classification):\n",
      "[[0.77133405]\n",
      " [0.79819596]\n",
      " [0.8142976 ]\n",
      " [0.8590892 ]\n",
      " [0.8323152 ]\n",
      " [0.8183125 ]\n",
      " [0.82784927]\n",
      " [0.8234992 ]\n",
      " [0.8233458 ]\n",
      " [0.84467506]\n",
      " [0.8451042 ]\n",
      " [0.8039889 ]\n",
      " [0.8230048 ]\n",
      " [0.83705443]\n",
      " [0.7816799 ]\n",
      " [0.8480388 ]\n",
      " [0.8302303 ]\n",
      " [0.80432546]\n",
      " [0.8125852 ]\n",
      " [0.7967198 ]\n",
      " [0.8327025 ]\n",
      " [0.7902785 ]\n",
      " [0.823001  ]\n",
      " [0.84056145]\n",
      " [0.8201916 ]\n",
      " [0.8499092 ]\n",
      " [0.7754059 ]\n",
      " [0.80649966]\n",
      " [0.8287916 ]\n",
      " [0.80722535]\n",
      " [0.840318  ]\n",
      " [0.8214152 ]\n",
      " [0.813529  ]\n",
      " [0.84224015]\n",
      " [0.812205  ]\n",
      " [0.85346305]\n",
      " [0.8061385 ]\n",
      " [0.82368684]\n",
      " [0.77621096]\n",
      " [0.8175887 ]\n",
      " [0.81706923]\n",
      " [0.84036624]\n",
      " [0.81139755]\n",
      " [0.8350048 ]\n",
      " [0.80288076]\n",
      " [0.78917384]\n",
      " [0.83053136]\n",
      " [0.81291074]\n",
      " [0.79409647]\n",
      " [0.82232183]]\n",
      "\n",
      "Predicted binary labels:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "--- Ensemble Accuracy on 50 Samples ---\n",
      "Ensemble Accuracy: 0.5400\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Ensemble Prediction and Accuracy Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "# You need to import any custom layers (like KerasLayer from PennyLane)\n",
    "# that were used when saving the model if saving in HDF5 format (.h5).\n",
    "# If you saved in the native Keras format (.keras), custom_objects might not be needed.\n",
    "\n",
    "# --- Import Quantum Libraries and Custom Layers ---\n",
    "# from pennylane.keras import KerasLayer # Example if using PennyLane\n",
    "# from pennylane.templates.tensor import DenseLayer # Example if your QNN used this\n",
    "# from pennylane.templates.layers import StronglyEntanglingLayers # Example if your QNN used this\n",
    "\n",
    "# If your QNN circuit was defined within a function, you might need to import that too,\n",
    "# or ensure your KerasLayer definition is available in the current scope.\n",
    "# Example:\n",
    "# def quantum_circuit_placeholder(...):\n",
    "#    ... your quantum circuit logic ...\n",
    "#    return measurements\n",
    "\n",
    "# --- Define Custom Objects Dictionary (NEEDED IF USING .h5 SAVE WITH CUSTOM LAYERS) ---\n",
    "# custom_objects = {\n",
    "#     \"KerasLayer\": KerasLayer, # Map the class name string to the actual class\n",
    "#     # Add any other custom layers/functions used in your model definition/save\n",
    "#     # \"DenseLayer\": DenseLayer,\n",
    "#     # \"StronglyEntanglingLayers\": StronglyEntanglingLayers,\n",
    "#     # \"quantum_circuit_placeholder\": quantum_circuit_placeholder, # If needed\n",
    "# }\n",
    "\n",
    "# --- Configuration ---\n",
    "# Update BASE_DATA_DIR to where your models are saved\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\" # Assuming models are here now\n",
    "MODEL_MRI_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_only_ensemble_qnn.h5\")\n",
    "MODEL_MRI_PET_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_pet_ensemble_qnn.h5\")\n",
    "\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression'\n",
    "# Accuracy calculation is typically for classification\n",
    "\n",
    "# --- Function to Load Models ---\n",
    "def load_trained_model(model_path, custom_objects=None):\n",
    "    \"\"\"Loads a trained Keras model from a file.\"\"\"\n",
    "    try:\n",
    "        # If you saved as .keras, you usually don't need custom_objects\n",
    "        if model_path.lower().endswith('.keras'):\n",
    "             model = tf.keras.models.load_model(model_path) # custom_objects is often not needed here\n",
    "        else: # Assuming .h5 format requires custom_objects for custom layers\n",
    "             model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {model_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        print(\"If using .h5 with custom layers (like KerasLayer for QNN), ensure 'custom_objects' dictionary is correct.\")\n",
    "        print(\"Consider saving models in the native Keras format (.keras) in the future.\")\n",
    "        return None\n",
    "\n",
    "# --- Function to Perform Ensemble Prediction ---\n",
    "def get_ensemble_predictions(model_mri, model_mri_pet, X_data_mri_format, X_data_mri_pet_format, task_type='classification'):\n",
    "    \"\"\"\n",
    "    Gets predictions from two models and averages them.\n",
    "\n",
    "    Args:\n",
    "        model_mri: The trained model expecting MRI-only shaped features (677).\n",
    "        model_mri_pet: The trained model expecting MRI-PET shaped features (263).\n",
    "        X_data_mri_format: NumPy array of features for prediction, shaped (N_samples, 677).\n",
    "        X_data_mri_pet_format: NumPy array of features for prediction, shaped (N_samples, 263).\n",
    "        task_type: 'classification' or 'regression'.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of ensemble predictions.\n",
    "    \"\"\"\n",
    "    if model_mri is None or model_mri_pet is None:\n",
    "        print(\"Error: One or both models failed to load. Cannot perform ensemble prediction.\")\n",
    "        return None\n",
    "\n",
    "    if X_data_mri_format.shape[0] != X_data_mri_pet_format.shape[0]:\n",
    "         print(\"Error: The number of samples in the two feature arrays must be the same for ensembling.\")\n",
    "         return None\n",
    "\n",
    "    # Optional shape checks based on loaded models' input shapes (more robust)\n",
    "    expected_mri_shape = model_mri.input_shape[1] if model_mri.input_shape else 677 # Fallback if shape not readily available\n",
    "    expected_mri_pet_shape = model_mri_pet.input_shape[1] if model_mri_pet.input_shape else 263 # Fallback\n",
    "\n",
    "    if X_data_mri_format.shape[1] != expected_mri_shape:\n",
    "        print(f\"Error: MRI-only data features shape mismatch. Expected {expected_mri_shape}, got {X_data_mri_format.shape[1]}.\")\n",
    "        return None\n",
    "\n",
    "    if X_data_mri_pet_format.shape[1] != expected_mri_pet_shape:\n",
    "        print(f\"Error: MRI-PET data features shape mismatch. Expected {expected_mri_pet_shape}, got {X_data_mri_pet_format.shape[1]}.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    print(f\"\\nGetting ensemble predictions for {X_data_mri_format.shape[0]} samples...\")\n",
    "\n",
    "    # Get predictions from each model\n",
    "    # For classification, sigmoid output is already probabilities (0-1)\n",
    "    predictions_mri = model_mri.predict(X_data_mri_format)\n",
    "    predictions_mri_pet = model_mri_pet.predict(X_data_mri_pet_format)\n",
    "\n",
    "    # Combine predictions by averaging\n",
    "    if task_type == 'classification':\n",
    "        # Averaging probabilities for classification\n",
    "        ensemble_predictions = (predictions_mri + predictions_mri_pet) / 2.0\n",
    "        print(\"Averaged predicted probabilities.\")\n",
    "    else: # Regression\n",
    "        # Averaging predicted values for regression\n",
    "        ensemble_predictions = (predictions_mri + predictions_mri_pet) / 2.0\n",
    "        print(\"Averaged predicted values.\")\n",
    "\n",
    "    print(\"Ensemble prediction finished.\")\n",
    "    return ensemble_predictions\n",
    "\n",
    "# --- Main Execution for Ensemble Prediction and Accuracy ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting Ensemble Prediction and Accuracy Calculation ---\")\n",
    "\n",
    "    # Load the trained models\n",
    "    # Make sure custom_objects is defined if needed for .h5 files with custom layers\n",
    "    model_mri = load_trained_model(MODEL_MRI_SAVE_PATH_QNN) # , custom_objects=custom_objects\n",
    "    model_mri_pet = load_trained_model(MODEL_MRI_PET_SAVE_PATH_QNN) # , custom_objects=custom_objects\n",
    "\n",
    "    if model_mri is None or model_mri_pet is None:\n",
    "        print(\"Failed to load one or both models. Cannot calculate ensemble accuracy. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # --- PLACEHOLDER FOR YOUR 50 SAMPLES OF PREDICTION DATA ---\n",
    "    # You need to replace this section with your actual data loading and preprocessing\n",
    "    # for the 50 samples you want to predict on.\n",
    "    # The key is that X_predict_mri_only_format and X_predict_mri_pet_format\n",
    "    # MUST contain features for the EXACT SAME 50 SAMPLES.\n",
    "    # You MUST also load the corresponding true labels into y_predict_true\n",
    "    # to calculate accuracy.\n",
    "\n",
    "    print(\"\\n--- Loading 50 Samples for Prediction (PLACEHOLDER) ---\")\n",
    "    print(\"Replace this section with your code to load and prepare your 50 samples.\")\n",
    "    print(\"You need to produce two NumPy arrays for features, each with 50 rows:\")\n",
    "    print(\"X_predict_mri_only_format (shape: (50, 677))\")\n",
    "    print(\"X_predict_mri_pet_format (shape: (50, 263))\")\n",
    "    print(\"You MUST also load the corresponding true labels into y_predict_true:\")\n",
    "    print(\"y_predict_true (NumPy array of true labels, shape: (50,))\")\n",
    "\n",
    "\n",
    "    # Example placeholder data (replace with your actual 50 samples)\n",
    "    num_samples_to_predict = 50\n",
    "\n",
    "    # These need to be derived from your actual 50 samples, not random\n",
    "    X_predict_mri_only_format = np.random.rand(num_samples_to_predict, 677) # REPLACE THIS LINE\n",
    "    X_predict_mri_pet_format = np.random.rand(num_samples_to_predict, 263) # REPLACE THIS LINE\n",
    "    y_predict_true = np.random.randint(0, 2, num_samples_to_predict) # REPLACE THIS LINE with your actual true labels\n",
    "\n",
    "    print(f\"Placeholder prediction data created for {num_samples_to_predict} samples.\")\n",
    "    print(f\"  X_predict_mri_only_format shape: {X_predict_mri_only_format.shape}\")\n",
    "    print(f\"  X_predict_mri_pet_format shape: {X_predict_mri_pet_format.shape}\")\n",
    "    print(f\"  y_predict_true shape: {y_predict_true.shape}\")\n",
    "\n",
    "    if X_predict_mri_only_format.shape[0] != y_predict_true.shape[0]:\n",
    "         print(\"Error: Number of samples in features and labels do not match. Cannot calculate accuracy.\")\n",
    "         exit()\n",
    "    # --- END PLACEHOLDER ---\n",
    "\n",
    "\n",
    "    # Perform the ensemble prediction for the 50 samples\n",
    "    ensemble_predictions_prob = get_ensemble_predictions(\n",
    "        model_mri,\n",
    "        model_mri_pet,\n",
    "        X_predict_mri_only_format, # Feed 677 features to Model 1\n",
    "        X_predict_mri_pet_format,  # Feed 263 features to Model 2\n",
    "        TASK_TYPE\n",
    "    )\n",
    "\n",
    "    if ensemble_predictions_prob is not None:\n",
    "        print(\"\\nEnsemble predictions generated for the 50 samples.\")\n",
    "        print(\"Predictions (probabilities for classification):\")\n",
    "        # Print predictions without truncation for 50 samples\n",
    "        np.set_printoptions(threshold=np.inf) # Allow printing full array\n",
    "        print(ensemble_predictions_prob)\n",
    "        np.set_printoptions() # Reset print options\n",
    "\n",
    "        if TASK_TYPE == 'classification':\n",
    "            # Convert probabilities to binary labels (e.g., threshold at 0.5)\n",
    "            ensemble_predicted_labels = (ensemble_predictions_prob > 0.5).astype(int)\n",
    "            print(\"\\nPredicted binary labels:\")\n",
    "            # Print labels without truncation for 50 samples\n",
    "            np.set_printoptions(threshold=np.inf) # Allow printing full array\n",
    "            print(ensemble_predicted_labels.flatten()) # Use flatten for cleaner printing\n",
    "            np.set_printoptions() # Reset print options\n",
    "\n",
    "\n",
    "            # Calculate the ensemble accuracy on these 50 samples\n",
    "            ensemble_accuracy = accuracy_score(y_predict_true, ensemble_predicted_labels.flatten()) # Use flatten for consistency\n",
    "\n",
    "            print(f\"\\n--- Ensemble Accuracy on {num_samples_to_predict} Samples ---\")\n",
    "            print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
    "            print(\"-------------------------------------------------\")\n",
    "\n",
    "        else: # Regression - you'd typically evaluate R2, MAE, MSE instead of accuracy\n",
    "             print(\"\\n--- Ensemble Regression Evaluation ---\")\n",
    "             print(\"For regression, you would calculate metrics like MAE, MSE, R2 here using y_predict_true and ensemble_predictions_prob\")\n",
    "             # from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "             # mae = mean_absolute_error(y_predict_true, ensemble_predictions_prob)\n",
    "             # mse = mean_squared_error(y_predict_true, ensemble_predictions_prob)\n",
    "             # r2 = r2_score(y_predict_true, ensemble_predictions_prob)\n",
    "             # print(f\"Ensemble Test MAE: {mae:.4f}\")\n",
    "             # print(f\"Ensemble Test MSE: {mse:.4f}\")\n",
    "             # print(f\"Ensemble Test R2: {r2:.4f}\")\n",
    "             print(\"------------------------------------\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Ensemble Prediction and Accuracy Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22d43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Model 1 (QNN) on mri_only_normalized.csv (677 Features) ---\n",
      "Error: Data file not found at C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\mri_only_normalized.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 350\u001b[0m\n\u001b[0;32m    347\u001b[0m     exit()\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# Ensure input shape matches expectation or use loaded shape\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m input_shape_mri \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_mri\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    351\u001b[0m expected_shape_mri \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m677\u001b[39m \u001b[38;5;66;03m# Based on previous runs\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_shape_mri \u001b[38;5;241m!=\u001b[39m expected_shape_mri:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler # Not using StandardScaler\n",
    "# Import upsampling library - you'll need to install imbalanced-learn: pip install imbalanced-learn\n",
    "from imblearn.over_sampling import RandomOverSampler # Or SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os # Import os for path joining\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "from sklearn.metrics import accuracy_score # To evaluate the combined prediction\n",
    "\n",
    "# --- Import Quantum Libraries ---\n",
    "# You'll need to install a quantum library like PennyLane, Cirq, or Qiskit\n",
    "# and their TensorFlow integration if available.\n",
    "# Example using PennyLane:\n",
    "# import pennylane as qml\n",
    "# from pennylane.keras import KerasLayer # Use this if PennyLane provides a Keras layer\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use os.path.join for better path handling across different OS\n",
    "# Updated BASE_DATA_DIR based on your provided paths\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\"\n",
    "\n",
    "MRI_FILE = \"mri_only_normalized.csv\"\n",
    "MRI_PET_FILE = \"mri_pet_normalized.csv\"\n",
    "\n",
    "MRI_DATA_PATH = os.path.join(BASE_DATA_DIR, MRI_FILE)\n",
    "MRI_PET_DATA_PATH = os.path.join(BASE_DATA_DIR, MRI_PET_FILE)\n",
    "\n",
    "\n",
    "# Model save paths match the locations you provided (optional, but good practice)\n",
    "MODEL_MRI_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_only_ensemble_qnn.h5\")\n",
    "MODEL_MRI_PET_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_pet_ensemble_qnn.h5\")\n",
    "\n",
    "\n",
    "TEST_SIZE_1 = 0.02 # Test size for MRI-only split\n",
    "TEST_SIZE_2 = 0.05 # Test size for MRI-PET split\n",
    "RANDOM_STATE = 42 # Random state for data splitting\n",
    "RANDOM_STATE_MODEL_1 = 42 # Random state for Model 1 initialization/training\n",
    "RANDOM_STATE_MODEL_2 = 123 # Different random state for Model 2\n",
    "\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression'\n",
    "\n",
    "# Training Hyperparameters (Adjusted slightly for example tuning)\n",
    "LEARNING_RATE = 0.0005 # Example: Slightly increased from 0.0001 - EXPERIMENT!\n",
    "BATCH_SIZE = 32 # Example: Decreased from 64 - EXPERIMENT! (Can help with small datasets)\n",
    "MAX_EPOCHS = 300 # Increased max epochs to give Early Stopping more room - EXPERIMENT!\n",
    "EARLY_STOPPING_PATIENCE = 50 # Increased patience - EXPERIMENT!\n",
    "\n",
    "# --- Quantum Configuration (Define based on your QNN design and hardware) ---\n",
    "# These are placeholders. You MUST define the number of qubits needed based on\n",
    "# your input dimensionality (677 and 263) and your chosen encoding strategy and hardware limitations.\n",
    "# N_QUBITS_MODEL_1 = 20 # <<< You MUST determine appropriate qubits for 677 features\n",
    "# N_QUBITS_MODEL_2 = 20 # <<< You MUST determine appropriate qubits for 263 features\n",
    "# Note: These placeholder qubit counts are likely TOO SMALL for direct encoding of 677/263 features\n",
    "# with standard methods. You will need a specific, possibly more complex, encoding strategy.\n",
    "\n",
    "# Example: Choose a quantum device (simulator or hardware)\n",
    "# DEVICE_SHORT_NAME = \"default.qubit.tf\" # Example: PennyLane simulator integrated with TF\n",
    "# DEV_MODEL_1 = qml.device(DEVICE_SHORT_NAME, wires=N_QUBITS_MODEL_1) # Define device for Model 1\n",
    "# DEV_MODEL_2 = qml.device(DEVICE_SHORT_NAME, wires=N_QUBITS_MODEL_2) # Define device for Model 2\n",
    "\n",
    "\n",
    "# --- Function to Create a Quantum Neural Network Model ---\n",
    "# IMPORTANT: Replace the placeholder quantum circuit logic with your actual QNN\n",
    "def create_qnn_model(input_shape, n_qubits, device):\n",
    "    \"\"\"\n",
    "    Function to create a Keras Sequential model with a Quantum Neural Network layer.\n",
    "    Contains placeholder classical layers to allow the script structure to run,\n",
    "    but you MUST replace these with your actual QNN implementation.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): The number of classical input features.\n",
    "        n_qubits (int): The number of qubits the QNN will use.\n",
    "        device: The quantum device (e.g., PennyLane device).\n",
    "    \"\"\"\n",
    "    print(f\"Creating a QNN model with input shape {input_shape} using {n_qubits} qubits.\")\n",
    "\n",
    "    # --- Placeholder Quantum Circuit Definition (REPLACE THIS ENTIRE SECTION) ---\n",
    "    # You need to define your quantum circuit that takes 'inputs' (classical data)\n",
    "    # and potentially 'weights' (trainable parameters) and returns classical results (measurements).\n",
    "    # This is a simplified example structure using PennyLane syntax as a placeholder.\n",
    "\n",
    "    # @qml.qnode(device, interface=\"tensorflow\") # Decorator to make it compatible with TensorFlow\n",
    "    # def quantum_circuit(inputs, weights):\n",
    "    #     # --- Quantum Encoding (You MUST implement your encoding strategy here) ---\n",
    "    #     # How to map 'inputs' (shape=[input_shape]) to quantum gates on 'n_qubits'.\n",
    "    #     # This is the most critical part for high dimensions (677 or 263).\n",
    "    #     # Example: Simple Angle Encoding (works if input_shape <= n_qubits)\n",
    "    #     # qml.AngleEmbedding(inputs, wires=range(input_shape))\n",
    "    #     # Example: Encoding subsets or repeating features if input_shape > n_qubits\n",
    "    #     # For input_shape = 677 or 263 and n_qubits = 20, you need a custom approach.\n",
    "    #     for i in range(min(input_shape, n_qubits)):\n",
    "    #         qml.RY(inputs[i], wires=i % n_qubits) # Dummy encoding example: Map features cyclically if more features than qubits\n",
    "\n",
    "    #     # --- Trainable Quantum Layers (You MUST define your parameterized quantum circuit here) ---\n",
    "    #     # These layers contain the trainable parameters 'weights'.\n",
    "    #     # Example: Strongly Entangling Layers\n",
    "    #     # num_layers = 6 # Example number of trainable layers\n",
    "    #     # qml.StronglyEntanglingLayers(weights, wires=range(n_qubits), depth=num_layers)\n",
    "\n",
    "    #     # --- Measurement (You MUST define your measurement strategy here) ---\n",
    "    #     # How to get classical output from the quantum state.\n",
    "    #     # Example: Expectation value of PauliZ on each qubit\n",
    "    #     # return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "\n",
    "    # # Define weight shapes for trainable quantum layers (Matches the 'weights' argument in quantum_circuit)\n",
    "    # # weight_shapes = {\"weights\": (num_layers, n_qubits, 3)} # Example shape for StronglyEntanglingLayers\n",
    "\n",
    "\n",
    "    # --- Construct Keras Model with QNN Layer (REPLACE/ADJUST THIS) ---\n",
    "    # This section builds the Keras model including your quantum part.\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_shape,)))\n",
    "\n",
    "    # Optional: Add classical layers BEFORE the QNN layer to reduce dimensions or preprocess\n",
    "    # model.add(Dense(input_shape // 4, activation='relu')) # Example pre-processing classical layer\n",
    "    # model.add(Dropout(0.2))\n",
    "    # Current_dim = input_shape // 4 # Update dimension if pre-processing\n",
    "\n",
    "    # --- Your Quantum Layer Integration (REPLACE THIS SECTION) ---\n",
    "    # Integrate your quantum circuit into the Keras model using a wrapper layer.\n",
    "    # Example using PennyLane's KerasLayer:\n",
    "    # Make sure the input dimension to KerasLayer matches the output dimension of the previous layer (input_shape or Current_dim).\n",
    "    # Make sure output_dim of KerasLayer matches the number of classical measurements from your quantum_circuit.\n",
    "    # qnn_layer = KerasLayer(quantum_circuit, weight_shapes, output_dim=n_qubits, name='quantum_layer') # Example\n",
    "    # model.add(qnn_layer)\n",
    "    # print(f\"Added KerasLayer with QNN circuit. Output dimension: {n_qubits}\") # Assumes output_dim=n_qubits\n",
    "\n",
    "\n",
    "    # --- Placeholder Classical Layers (REMOVE THIS ENTIRE SECTION ONCE YOU ADD YOUR QNN) ---\n",
    "    # These layers are here ONLY to allow the script structure to run if you haven't\n",
    "    # implemented the QNN part yet. They are NOT a quantum model.\n",
    "    print(\"WARNING: Using classical Dense network placeholders for QNN. REPLACE with your QNN!\")\n",
    "    # This placeholder tries to mimic a path through dense/dropout\n",
    "    model.add(Dense(64, activation='relu', name='placeholder_dense_1'))\n",
    "    model.add(Dropout(0.3, name='placeholder_dropout_1'))\n",
    "    model.add(Dense(n_qubits, activation='relu', name='placeholder_dense_2')) # Output shape roughly like n_qubits measurements\n",
    "    # --- End of Placeholder Classical Layers ---\n",
    "\n",
    "\n",
    "    # Optional: Add classical layers AFTER the QNN layer to map QNN output to final prediction\n",
    "    # The input dimension to these layers would be the output_dim of your KerasLayer (e.g., n_qubits if measuring all qubits).\n",
    "    # model.add(Dense(32, activation='relu', name='post_qnn_dense_1')) # Example post-processing classical layer\n",
    "    # model.add(Dropout(0.2, name='post_qnn_dropout_1'))\n",
    "\n",
    "    # --- Final Output Layer ---\n",
    "    # This layer maps the output of the last quantum or classical layer to the final prediction (1 output neuron for binary classification).\n",
    "    model.add(Dense(1, activation='sigmoid' if TASK_TYPE == 'classification' else None, name='output_layer'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Helper Function to Load Data and Split ---\n",
    "# Assumes data is normalized and DOES NOT use StandardScaler.\n",
    "# Assumes the first column is an ID and the last column is the label.\n",
    "def load_data_and_split(file_path, test_size, random_state, task_type):\n",
    "    \"\"\"\n",
    "    Loads data from CSV, separates features and labels, and splits into train/test sets.\n",
    "    Assumes the first column is an ID and the last column is the label.\n",
    "    Assumes data is already normalized and DOES NOT use StandardScaler.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # --- CRITICAL FIX for Non-Numeric Columns ---\n",
    "    # Assuming the first column (index 0) is an ID column\n",
    "    # and the last column is the numerical label.\n",
    "    # Select features from the second column (index 1) up to, but NOT including, the last column.\n",
    "    # If your files have a different structure, you MUST adjust these lines accordingly.\n",
    "    try:\n",
    "        # Check if the file is empty or has only header/ID/Label\n",
    "        if data.shape[1] < 2: # Needs at least features and a label\n",
    "             print(f\"Error: File {file_path} does not have enough columns (expected at least 1 feature, Label).\")\n",
    "             return None, None, None, None\n",
    "        # Adjust slicing if the first column is NOT always an ID to be skipped\n",
    "        # If no ID, use data.iloc[:, :-1].values for X\n",
    "        X = data.iloc[:, 1:-1].values # Features exclude the first column (index 0)\n",
    "        y = data.iloc[:, -1].values  # Label is the last column\n",
    "\n",
    "        # Double check if iloc[:, 1:-1] resulted in 0 features, meaning maybe only ID and Label were present?\n",
    "        if X.shape[1] == 0 and data.shape[1] > 1: # If > 1 total columns but 0 features after slicing\n",
    "             print(f\"Error: iloc[:, 1:-1] resulted in 0 features for {file_path}. Check your column structure. Expected > 0 features.\")\n",
    "             print(f\"File has {data.shape[1]} columns. If ID is first and Label last, expected {data.shape[1]-2} features.\")\n",
    "             # Suggest inspecting data.columns and data.head()\n",
    "\n",
    "             return None, None, None, None # Still return None as data loading failed logic check\n",
    "\n",
    "\n",
    "        print(f\"Loaded raw data from {file_path}. X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "\n",
    "        # Optional check: Verify the feature matrix contains only numeric types\n",
    "        if not np.issubdtype(X.dtype, np.number):\n",
    "             print(f\"Error: Feature matrix from {file_path} still contains non-numeric data after slicing. Data type: {X.dtype}\")\n",
    "             print(\"Please inspect this CSV file for non-numeric values in columns other than the first and last.\")\n",
    "             return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"Error: Indexing failed for {file_path}. Check if your file has enough columns (at least ID, Feature(s), Label). Minimum 2 columns total if no ID, 3 if ID is first.\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during loading {file_path}: {e}\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    # --- END CRITICAL FIX ---\n",
    "\n",
    "    # If classification and labels are not 0/1, convert them\n",
    "    if task_type == 'classification' and np.unique(y).size == 2 and not all(np.isin(y, [0, 1])):\n",
    "         print(f\"Warning: Assuming binary classification, converting labels {np.unique(y)} to 0/1.\")\n",
    "         unique_labels = np.unique(y)\n",
    "         label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "         y = np.array([label_map[label] for label in y])\n",
    "\n",
    "    # Split data, using stratification for classification to maintain label distribution\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y if task_type == 'classification' else None\n",
    "    )\n",
    "\n",
    "    print(f\"Split data from {file_path}. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# --- Function to Apply Upsampling ---\n",
    "def apply_upsampling(X_train, y_train, sampling_strategy='auto'):\n",
    "    \"\"\"\n",
    "    Applies RandomOverSampler upsampling to the training data.\n",
    "    This should ONLY be applied AFTER the train/test split.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training features.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        sampling_strategy (str or dict): Sampling strategy for RandomOverSampler. 'auto' balances classes.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): Upsampled training features and labels.\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying RandomOverSampler upsampling to the training data (strategy: {sampling_strategy})...\")\n",
    "    print(f\"Original training shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    if len(np.unique(y_train)) > 1:\n",
    "        print(f\"Original class distribution: {np.bincount(y_train)}\")\n",
    "    else:\n",
    "        print(\"Warning: Only one class found in training data. Upsampling is not applicable.\")\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "    # Initialize the upsampler\n",
    "    ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Fit and resample the training data\n",
    "    X_train_upsampled, y_train_upsampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(f\"Upsampled training shape: X={X_train_upsampled.shape}, y={y_train_upsampled.shape}\")\n",
    "    print(f\"Upsampled class distribution: {np.bincount(y_train_upsampled)}\")\n",
    "    print(\"Upsampling finished.\")\n",
    "\n",
    "    return X_train_upsampled, y_train_upsampled\n",
    "\n",
    "\n",
    "# --- Text Summary of Training History ---\n",
    "def print_training_summary(history, model_name, task_type):\n",
    "    \"\"\"Prints a text summary of the training history.\"\"\"\n",
    "    print(f\"\\nSummary for {model_name}:\")\n",
    "    epochs_trained = len(history.history.get('loss', [])) # Use .get for safety\n",
    "\n",
    "    if epochs_trained > 0:\n",
    "        final_train_loss = history.history['loss'][-1]\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        metric_name = 'accuracy' if task_type == 'classification' else 'mae'\n",
    "        final_train_metric = history.history.get(metric_name, [np.nan])[-1] # Use .get with fallback\n",
    "        final_val_metric = history.history.get(f'val_{metric_name}', [np.nan])[-1] # Use .get with fallback\n",
    "\n",
    "        print(f\"  Epochs Trained: {epochs_trained}\")\n",
    "        print(f\"  Final Training Loss: {final_train_loss:.4f}\")\n",
    "        print(f\"  Final Validation Loss: {final_val_loss:.4f}\")\n",
    "        print(f\"  Final Training {metric_name.capitalize()}: {final_train_metric:.4f}\")\n",
    "        print(f\"  Final Validation {metric_name.capitalize()}: {final_val_metric:.4f}\")\n",
    "        print(\"  (Note: Final metrics shown are for the last epoch run. Evaluate results below reflect best epoch if Early Stopping restored weights.)\")\n",
    "    else:\n",
    "        print(f\"  No epochs were trained for {model_name}. Training likely failed early.\")\n",
    "    print(f\"--- End of {model_name} Summary ---\")\n",
    "\n",
    "\n",
    "# --- Plotting History ---\n",
    "def plot_training_history(history, model_name, task_type):\n",
    "    \"\"\"Plots training and validation loss and metrics.\"\"\"\n",
    "    print(f\"\\n--- Plotting Training History for {model_name} ---\")\n",
    "    if not history.history.get('loss', []):\n",
    "        print(\"  No history data to plot.\")\n",
    "        return\n",
    "\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    metric_name = 'accuracy' if task_type == 'classification' else 'mae'\n",
    "    train_metric = history.history.get(metric_name, None) # Use .get, can be None if metric not tracked\n",
    "    val_metric = history.history.get(f'val_{metric_name}', None)\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, 'bo-', label='Training loss', alpha=0.6)\n",
    "    plt.plot(epochs, val_loss, 'b-', label='Validation loss')\n",
    "    plt.title(f'{model_name}: Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Primary Metric (Accuracy or MAE)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if train_metric is not None and val_metric is not None:\n",
    "        plt.plot(epochs, train_metric, 'bo-', label=f'Training {metric_name.capitalize()}', alpha=0.6)\n",
    "        plt.plot(epochs, val_metric, 'b-', label=f'Validation {metric_name.capitalize()}')\n",
    "        plt.title(f'{model_name}: Training and validation {metric_name.capitalize()}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric_name.capitalize())\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, f\"Metric '{metric_name}' not tracked during training.\",\n",
    "                 horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title(f'{model_name}: {metric_name.capitalize()} over epochs')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show() # Keep commented out if plotting multiple models\n",
    "\n",
    "\n",
    "# --- Train Model 1 (QNN on MRI-only data - NO UPSAMPLING) ---\n",
    "\n",
    "print(\"--- Training Model 1 (QNN) on {} ({} Features) ---\".format(os.path.basename(MRI_DATA_PATH), 677))\n",
    "\n",
    "# Load and split MRI-only data\n",
    "X_train_mri, X_test_mri, y_train_mri, y_test_mri = load_data_and_split(\n",
    "    MRI_DATA_PATH, TEST_SIZE_1, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri is None:\n",
    "    exit()\n",
    "\n",
    "# Ensure input shape matches expectation or use loaded shape\n",
    "input_shape_mri = X_train_mri.shape[1]\n",
    "expected_shape_mri = 677 # Based on previous runs\n",
    "if input_shape_mri != expected_shape_mri:\n",
    "    print(f\"Warning: MRI-only data shape is {input_shape_mri}, expected {expected_shape_mri}. Using loaded shape.\")\n",
    "    # If this is significantly different, check data loading logic again\n",
    "\n",
    "\n",
    "print(f\"Input shape for Model 1 (QNN): {input_shape_mri}\")\n",
    "\n",
    "# Initialize Model 1 (QNN)\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_1) # Set seed for reproducibility\n",
    "# IMPORTANT: Pass the correct device and qubit count for your QNN implementation\n",
    "# model_mri_qnn = create_qnn_model(input_shape_mri, N_QUBITS_MODEL_1, DEV_MODEL_1)\n",
    "# --- Placeholder Call (REMOVE THIS AND USE THE LINE ABOVE) ---\n",
    "# Determine placeholder qubits if N_QUBITS_MODEL_1 is not set\n",
    "n_qubits_m1 = 20 # Default for placeholder if config isn't uncommented or is None\n",
    "try:\n",
    "    if 'N_QUBITS_MODEL_1' in locals() and N_QUBITS_MODEL_1 is not None and N_QUBITS_MODEL_1 > 0:\n",
    "        n_qubits_m1 = N_QUBITS_MODEL_1\n",
    "    elif 'N_QUBITS_MODEL_1' in locals() and N_QUBITS_MODEL_1 <= 0:\n",
    "        print(\"Warning: N_QUBITS_MODEL_1 must be > 0. Using default 20 for placeholder.\")\n",
    "        n_qubits_m1 = 20\n",
    "except NameError:\n",
    "    pass # Use default if N_QUBITS_MODEL_1 is not defined\n",
    "\n",
    "model_mri_qnn = create_qnn_model(input_shape_mri, n_qubits_m1, None) # Dummy call for placeholder\n",
    "# --- End Placeholder ---\n",
    "\n",
    "\n",
    "model_mri_qnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                      loss='binary_crossentropy' if TASK_TYPE == 'classification' else 'mean_squared_error',\n",
    "                      metrics=['accuracy'] if TASK_TYPE == 'classification' else ['mae'])\n",
    "model_mri_qnn.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 1 (QNN)\n",
    "early_stopping_mri_qnn = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 1 (QNN) training.\")\n",
    "\n",
    "\n",
    "print(f\"Fitting Model 1 (QNN) on MRI-only dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "# --- Training Step (May be slow/fail depending on QNN and backend) ---\n",
    "history_mri_qnn = model_mri_qnn.fit(X_train_mri, y_train_mri,\n",
    "                                    epochs=MAX_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    validation_data=(X_test_mri, y_test_mri),\n",
    "                                    callbacks=[early_stopping_mri_qnn],\n",
    "                                    verbose=1)\n",
    "print(\"Model 1 (QNN) training finished.\")\n",
    "\n",
    "# Save Model 1 (QNN) - Recommend saving in .keras format\n",
    "model_mri_qnn.save(MODEL_MRI_SAVE_PATH_QNN)\n",
    "print(f\"Model 1 (QNN) saved as {MODEL_MRI_SAVE_PATH_QNN}\")\n",
    "\n",
    "# Evaluate Model 1 (QNN) on its test set\n",
    "print(\"\\nEvaluating Model 1 (QNN) on MRI-only test set:\")\n",
    "eval_results_mri_qnn = model_mri_qnn.evaluate(X_test_mri, y_test_mri, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 1 (QNN, MRI-only) Test Loss: {eval_results_mri_qnn[0]:.4f}, Accuracy: {eval_results_mri_qnn[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 1 (QNN, MRI-only) Test Loss (MSE): {eval_results_mri_qnn[0]:.4f}, Test MAE: {eval_results_mri_qnn[1]:.4f}\")\n",
    "\n",
    "# Print and Plot Summary for Model 1\n",
    "print_training_summary(history_mri_qnn, \"Model 1 (QNN on MRI-only)\", TASK_TYPE)\n",
    "\n",
    "\n",
    "# --- Train Model 2 (QNN on MRI-PET data - WITH UPSAMPLING) ---\n",
    "\n",
    "print(\"\\n--- Training Model 2 (QNN) on {} ({} Features) - WITH UPSAMPLING ---\".format(os.path.basename(MRI_PET_DATA_PATH), 263))\n",
    "\n",
    "# Load and split MRI-PET data\n",
    "X_train_mri_pet_raw, X_test_mri_pet_raw, y_train_mri_pet_raw, y_test_mri_pet_raw = load_data_and_split(\n",
    "    MRI_PET_DATA_PATH, TEST_SIZE_2, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri_pet_raw is None:\n",
    "    exit()\n",
    "\n",
    "# --- Apply Upsampling to MRI-PET Training Data (AFTER SPLIT) ---\n",
    "# This is the correct place to apply upsampling.\n",
    "# Note: Upsampling is ONLY applied to the training subset.\n",
    "# sampling_strategy='auto' will balance the classes (make count of minority == count of majority)\n",
    "X_train_mri_pet_upsampled, y_train_mri_pet_upsampled = apply_upsampling(\n",
    "    X_train_mri_pet_raw, y_train_mri_pet_raw, sampling_strategy='auto'\n",
    ")\n",
    "# Note: We train on the upsampled data but validate/evaluate on the original test data\n",
    "\n",
    "# Ensure input shape matches expectation or use loaded shape\n",
    "input_shape_mri_pet = X_train_mri_pet_raw.shape[1]\n",
    "expected_shape_mri_pet = 263 # Based on previous runs\n",
    "if input_shape_mri_pet != expected_shape_mri_pet:\n",
    "    print(f\"Warning: MRI-PET data shape is {input_shape_mri_pet}, expected {expected_shape_mri_pet}. Using loaded shape.\")\n",
    "     # If this is significantly different, check data loading logic again\n",
    "\n",
    "\n",
    "print(f\"Input shape for Model 2 (QNN): {input_shape_mri_pet}\")\n",
    "\n",
    "# Initialize Model 2 (QNN) - Input shape based on original features\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_2) # Set a different seed\n",
    "# IMPORTANT: Pass the correct device and qubit count for your QNN implementation\n",
    "# model_mri_pet_qnn = create_qnn_model(input_shape_mri_pet, N_QUBITS_MODEL_2, DEV_MODEL_2)\n",
    "# --- Placeholder Call (REMOVE THIS AND USE THE LINE ABOVE) ---\n",
    "# Determine placeholder qubits if N_QUBITS_MODEL_2 is not set\n",
    "n_qubits_m2 = 20 # Default for placeholder if config isn't uncommented or is None\n",
    "try:\n",
    "    if 'N_QUBITS_MODEL_2' in locals() and N_QUBITS_MODEL_2 is not None and N_QUBITS_MODEL_2 > 0:\n",
    "        n_qubits_m2 = N_QUBITS_MODEL_2\n",
    "    elif 'N_QUBITS_MODEL_2' in locals() and N_QUBITS_MODEL_2 <= 0:\n",
    "        print(\"Warning: N_QUBITS_MODEL_2 must be > 0. Using default 20 for placeholder.\")\n",
    "        n_qubits_m2 = 20\n",
    "except NameError:\n",
    "    pass # Use default if N_QUBITS_MODEL_2 is not defined\n",
    "\n",
    "model_mri_pet_qnn = create_qnn_model(input_shape_mri_pet, n_qubits_m2, None) # Dummy call for placeholder\n",
    "# --- End Placeholder ---\n",
    "\n",
    "model_mri_pet_qnn.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                          loss='binary_crossentropy' if TASK_TYPE == 'classification' else 'mean_squared_error',\n",
    "                          metrics=['accuracy'] if TASK_TYPE == 'classification' else ['mae'])\n",
    "model_mri_pet_qnn.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 2 (QNN)\n",
    "early_stopping_mri_pet_qnn = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 2 (QNN) training.\")\n",
    "\n",
    "print(f\"Fitting Model 2 (QNN) on upsampled MRI-PET dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "# --- Training Step (May be slow/fail depending on QNN and backend) ---\n",
    "# Train using the upsampled data\n",
    "history_mri_pet_qnn = model_mri_pet_qnn.fit(X_train_mri_pet_upsampled, y_train_mri_pet_upsampled,\n",
    "                                           epochs=MAX_EPOCHS,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           validation_data=(X_test_mri_pet_raw, y_test_mri_pet_raw), # VALIDATE ON ORIGINAL TEST DATA\n",
    "                                           callbacks=[early_stopping_mri_pet_qnn],\n",
    "                                           verbose=1)\n",
    "print(\"Model 2 (QNN) training finished.\")\n",
    "\n",
    "# Save Model 2 (QNN) - Recommend saving in .keras format\n",
    "model_mri_pet_qnn.save(MODEL_MRI_PET_SAVE_PATH_QNN)\n",
    "print(f\"Model 2 (QNN) saved as {MODEL_MRI_PET_SAVE_PATH_QNN}\")\n",
    "\n",
    "# Evaluate Model 2 (QNN) on its original test set\n",
    "print(\"\\nEvaluating Model 2 (QNN) on original MRI-PET test set:\")\n",
    "eval_results_mri_pet_qnn = model_mri_pet_qnn.evaluate(X_test_mri_pet_raw, y_test_mri_pet_raw, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 2 (QNN, MRI-PET) Test Loss: {eval_results_mri_pet_qnn[0]:.4f}, Accuracy: {eval_results_mri_pet_qnn[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 2 (QNN, MRI-PET) Test Loss (MSE): {eval_results_mri_pet_qnn[0]:.4f}, Test MAE: {eval_results_mri_pet_qnn[1]:.4f}\")\n",
    "\n",
    "# Print and Plot Summary for Model 2\n",
    "print_training_summary(history_mri_pet_qnn, \"Model 2 (QNN on MRI-PET)\", TASK_TYPE)\n",
    "\n",
    "\n",
    "# --- Ensembling Predictions ---\n",
    "print(\"\\n--- Ensembling QNN Predictions ---\")\n",
    "print(\"QNN models trained independently on datasets with different feature counts.\")\n",
    "\n",
    "print(\"\\nTo ensemble these QNN models for a final prediction on a new sample, you would need:\")\n",
    "print(\"1. The {} feature representation of the new sample (processed like MRI-only data).\".format(input_shape_mri))\n",
    "print(\"2. The {} feature representation of the new sample (processed like MRI-PET data).\".format(input_shape_mri_pet))\n",
    "print(\"3. Load Model 1 QNN (from {}) and feed it the {} features.\".format(MODEL_MRI_SAVE_PATH_QNN, input_shape_mri))\n",
    "print(\"4. Load Model 2 QNN (from {}) and feed it the {} features.\".format(MODEL_MRI_PET_SAVE_PATH_QNN, input_shape_mri_pet))\n",
    "print(\"5. Combine their predicted probabilities (e.g., average) to get the final ensemble prediction.\")\n",
    "\n",
    "# Example of getting predictions on their respective test sets:\n",
    "print(\"\\nExample: Getting predictions on respective test sets:\")\n",
    "# --- Prediction Steps (May be slow depending on QNN and backend) ---\n",
    "# Load models here if they weren't saved/loaded earlier in this script execution lifecycle\n",
    "# model_mri_qnn_loaded = tf.keras.models.load_model(MODEL_MRI_SAVE_PATH_QNN, custom_objects={...}) # Load if needed\n",
    "# model_mri_pet_qnn_loaded = tf.keras.models.load_model(MODEL_MRI_PET_SAVE_PATH_QNN, custom_objects={...}) # Load if needed\n",
    "\n",
    "# Use the trained models directly from memory if script runs end-to-end\n",
    "mri_only_test_predictions_qnn = model_mri_qnn.predict(X_test_mri)\n",
    "mri_pet_test_predictions_qnn = model_mri_pet_qnn.predict(X_test_mri_pet_raw) # Predict using the model trained earlier (use original test data)\n",
    "\n",
    "print(f\"Predictions from Model 1 (QNN, MRI-only) on its test set (shape: {mri_only_test_predictions_qnn.shape})\")\n",
    "print(f\"Predictions from Model 2 (QNN, MRI-PET) on its ORIGINAL test set (shape: {mri_pet_test_predictions_qnn.shape})\")\n",
    "\n",
    "# Note: You cannot directly average these predictions for a single ensemble score\n",
    "# because they are from different test sets (disjoint samples).\n",
    "\n",
    "# --- Show all plots ---\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScript execution finished.\")\n",
    "print(\"Two QNN models trained and saved for ensembling based on their respective datasets, with upsampling on the MRI-PET training data.\")\n",
    "print(\"\\nTo increase accuracy further, systematically tune hyperparameters, improve the QNN implementation, and explore ensemble fusion methods.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Model 1 (Classical MLP) on mri_only_normalized.csv (677 Features) ---\n",
      "Error: Data file not found at C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\mri_only_normalized.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 264\u001b[0m\n\u001b[0;32m    261\u001b[0m     exit()\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# Ensure input shape matches expectation or use loaded shape\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m input_shape_mri \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_mri\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    265\u001b[0m expected_shape_mri \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m677\u001b[39m \u001b[38;5;66;03m# Based on previous runs\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_shape_mri \u001b[38;5;241m!=\u001b[39m expected_shape_mri:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler # Not using StandardScaler\n",
    "# Import upsampling library - you'll need to install imbalanced-learn: pip install imbalanced-learn\n",
    "from imblearn.over_sampling import RandomOverSampler # Or SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os # Import os for path joining\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "from sklearn.metrics import accuracy_score # To evaluate the combined prediction\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use os.path.join for better path handling across different OS\n",
    "# Using the directory you provided for model saves\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\"\n",
    "\n",
    "MRI_FILE = \"mri_only_normalized.csv\"\n",
    "MRI_PET_FILE = \"mri_pet_normalized.csv\"\n",
    "\n",
    "# Assuming the data files are in the same directory as the script/models now\n",
    "# Adjust these paths if your data files are elsewhere\n",
    "MRI_DATA_PATH = os.path.join(BASE_DATA_DIR, MRI_FILE)\n",
    "MRI_PET_DATA_PATH = os.path.join(BASE_DATA_DIR, MRI_PET_FILE)\n",
    "\n",
    "\n",
    "# Model save paths (using classical names)\n",
    "MODEL_MRI_SAVE_PATH_CLASSICAL = os.path.join(BASE_DATA_DIR, \"model_mri_only_ensemble_classical.h5\")\n",
    "MODEL_MRI_PET_SAVE_PATH_CLASSICAL = os.path.join(BASE_DATA_DIR, \"model_mri_pet_ensemble_classical.h5\")\n",
    "\n",
    "\n",
    "TEST_SIZE_1 = 0.02 # Test size for MRI-only split\n",
    "TEST_SIZE_2 = 0.05 # Test size for MRI-PET split\n",
    "RANDOM_STATE = 42 # Random state for data splitting\n",
    "RANDOM_STATE_MODEL_1 = 42 # Random state for Model 1 initialization/training\n",
    "RANDOM_STATE_MODEL_2 = 123 # Different random state for Model 2\n",
    "\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression'\n",
    "\n",
    "# Training Hyperparameters (can tune these for each model)\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 32\n",
    "MAX_EPOCHS = 300\n",
    "EARLY_STOPPING_PATIENCE = 50\n",
    "\n",
    "# --- Function to Create a Standard Classical MLP Model ---\n",
    "# This replaces the QNN placeholder function with a stable classical model\n",
    "def create_classical_mlp_model(input_shape):\n",
    "    \"\"\"\n",
    "    Function to create a standard Multi-Layer Perceptron (MLP) model.\n",
    "    This is a stable classical architecture.\n",
    "    \"\"\"\n",
    "    print(f\"Creating a classical MLP model with input shape {input_shape}\")\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(128, activation='relu'), # Example layer sizes - EXPERIMENT!\n",
    "        Dropout(0.3), # Example dropout - EXPERIMENT!\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        # Add more layers or neurons as needed, but balance with data size\n",
    "        Dense(1, activation='sigmoid' if TASK_TYPE == 'classification' else None) # Sigmoid for binary classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# --- Helper Function to Load Data and Split ---\n",
    "# Assumes data is normalized and DOES NOT use StandardScaler.\n",
    "# Assumes the first column is an ID and the last column is the label.\n",
    "def load_data_and_split(file_path, test_size, random_state, task_type):\n",
    "    \"\"\"\n",
    "    Loads data from CSV, separates features and labels, and splits into train/test sets.\n",
    "    Assumes the first column is an ID and the last column is the label.\n",
    "    Assumes data is already normalized and DOES NOT use StandardScaler.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # --- CRITICAL FIX for Non-Numeric Columns and Indexing ---\n",
    "    # Assuming the first column (index 0) is an ID column\n",
    "    # and the last column is the numerical label.\n",
    "    # Select features from the second column (index 1) up to, but NOT including, the last column.\n",
    "    # If your files have a different structure, you MUST adjust these lines accordingly.\n",
    "    try:\n",
    "        # Check if the file has at least 2 columns (features + label) or 3 (ID + features + label)\n",
    "        if data.shape[1] < 2:\n",
    "             print(f\"Error: File {file_path} does not have enough columns (expected at least 1 feature + Label).\")\n",
    "             return None, None, None, None\n",
    "\n",
    "        # Attempt to slice assuming ID is first and Label is last\n",
    "        X = data.iloc[:, 1:-1].values # Features exclude the first column (index 0)\n",
    "        y = data.iloc[:, -1].values  # Label is the last column\n",
    "\n",
    "        # Check if slicing resulted in 0 features unexpectedly\n",
    "        if X.shape[1] == 0 and data.shape[1] > 1: # If > 1 total columns but 0 features after slicing\n",
    "             print(f\"Error: iloc[:, 1:-1] resulted in 0 features for {file_path}. Check your column structure.\")\n",
    "             print(f\"File has {data.shape[1]} columns. If ID is first and Label last, expected {data.shape[1]-2} features.\")\n",
    "             print(\"Check if the first column is indeed ID and the last is Label, and if there are features in between.\")\n",
    "             # Suggest inspecting data.columns and data.head()\n",
    "             return None, None, None, None # Still return None as data loading failed logic check\n",
    "\n",
    "        print(f\"Loaded raw data from {file_path}. X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "        # Optional check: Verify the feature matrix contains only numeric types\n",
    "        if not np.issubdtype(X.dtype, np.number):\n",
    "             print(f\"Error: Feature matrix from {file_path} still contains non-numeric data after slicing. Data type: {X.dtype}\")\n",
    "             print(\"Please inspect this CSV file for non-numeric values in columns other than the first and last.\")\n",
    "             return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"Error: Indexing failed for {file_path}. Check if your file has enough columns (at least ID, Feature(s), Label). Minimum 2 columns total if no ID, 3 if ID is first.\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during loading {file_path}: {e}\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    # --- END CRITICAL FIX ---\n",
    "\n",
    "    # If classification and labels are not 0/1, convert them\n",
    "    if task_type == 'classification' and np.unique(y).size == 2 and not all(np.isin(y, [0, 1])):\n",
    "         print(f\"Warning: Assuming binary classification, converting labels {np.unique(y)} to 0/1.\")\n",
    "         unique_labels = np.unique(y)\n",
    "         label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "         y = np.array([label_map[label] for label in y])\n",
    "\n",
    "    # Split data, using stratification for classification to maintain label distribution\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y if task_type == 'classification' else None\n",
    "    )\n",
    "\n",
    "    print(f\"Split data from {file_path}. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# --- Function to Apply Upsampling ---\n",
    "def apply_upsampling(X_train, y_train, sampling_strategy='auto'):\n",
    "    \"\"\"\n",
    "    Applies RandomOverSampler upsampling to the training data.\n",
    "    This should ONLY be applied AFTER the train/test split.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training features.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        sampling_strategy (str or dict): Sampling strategy for RandomOverSampler. 'auto' balances classes.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): Upsampled training features and labels.\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying RandomOverSampler upsampling to the training data (strategy: {sampling_strategy})...\")\n",
    "    print(f\"Original training shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    if len(np.unique(y_train)) > 1:\n",
    "        print(f\"Original class distribution: {np.bincount(y_train)}\")\n",
    "    else:\n",
    "        print(\"Warning: Only one class found in training data. Upsampling is not applicable.\")\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "    # Initialize the upsampler\n",
    "    ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Fit and resample the training data\n",
    "    X_train_upsampled, y_train_upsampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(f\"Upsampled training shape: X={X_train_upsampled.shape}, y={y_train_upsampled.shape}\")\n",
    "    if len(np.unique(y_train_upsampled)) > 1:\n",
    "         print(f\"Upsampled class distribution: {np.bincount(y_train_upsampled)}\")\n",
    "    else:\n",
    "         print(\"Warning: Only one class found after upsampling. Check your data.\")\n",
    "\n",
    "    print(\"Upsampling finished.\")\n",
    "\n",
    "    return X_train_upsampled, y_train_upsampled\n",
    "\n",
    "\n",
    "# --- Text Summary of Training History ---\n",
    "def print_training_summary(history, model_name, task_type):\n",
    "    \"\"\"Prints a text summary of the training history.\"\"\"\n",
    "    print(f\"\\nSummary for {model_name}:\")\n",
    "    epochs_trained = len(history.history.get('loss', [])) # Use .get for safety\n",
    "\n",
    "    if epochs_trained > 0:\n",
    "        final_train_loss = history.history['loss'][-1]\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        metric_name = 'accuracy' if task_type == 'classification' else 'mae'\n",
    "        final_train_metric = history.history.get(metric_name, [np.nan])[-1] # Use .get with fallback\n",
    "        final_val_metric = history.history.get(f'val_{metric_name}', [np.nan])[-1] # Use .get with fallback\n",
    "\n",
    "        print(f\"  Epochs Trained: {epochs_trained}\")\n",
    "        print(f\"  Final Training Loss: {final_train_loss:.4f}\")\n",
    "        print(f\"  Final Validation Loss: {final_val_loss:.4f}\")\n",
    "        print(f\"  Final Training {metric_name.capitalize()}: {final_train_metric:.4f}\")\n",
    "        print(f\"  Final Validation {metric_name.capitalize()}: {final_val_metric:.4f}\")\n",
    "        print(\"  (Note: Final metrics shown are for the last epoch run. Evaluate results below reflect best epoch if Early Stopping restored weights.)\")\n",
    "    else:\n",
    "        print(f\"  No epochs were trained for {model_name}. Training likely failed early.\")\n",
    "    print(f\"--- End of {model_name} Summary ---\")\n",
    "\n",
    "\n",
    "# --- Plotting History ---\n",
    "def plot_training_history(history, model_name, task_type):\n",
    "    \"\"\"Plots training and validation loss and metrics.\"\"\"\n",
    "    print(f\"\\n--- Plotting Training History for {model_name} ---\")\n",
    "    if not history.history.get('loss', []):\n",
    "        print(\"  No history data to plot.\")\n",
    "        return\n",
    "\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    metric_name = 'accuracy' if task_type == 'classification' else 'mae'\n",
    "    train_metric = history.history.get(metric_name, None) # Use .get, can be None if metric not tracked\n",
    "    val_metric = history.history.get(f'val_{metric_name}', None)\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, 'bo-', label='Training loss', alpha=0.6)\n",
    "    plt.plot(epochs, val_loss, 'b-', label='Validation loss')\n",
    "    plt.title(f'{model_name}: Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Primary Metric (Accuracy or MAE)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if train_metric is not None and val_metric is not None:\n",
    "        plt.plot(epochs, train_metric, 'bo-', label=f'Training {metric_name.capitalize()}', alpha=0.6)\n",
    "        plt.plot(epochs, val_metric, 'b-', label=f'Validation {metric_name.capitalize()}')\n",
    "        plt.title(f'{model_name}: Training and validation {metric_name.capitalize()}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric_name.capitalize())\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, f\"Metric '{metric_name}' not tracked during training.\",\n",
    "                 horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title(f'{model_name}: {metric_name.capitalize()} over epochs')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show() # Keep commented out if plotting multiple models\n",
    "\n",
    "\n",
    "# --- Train Model 1 (Classical MLP on MRI-only data - NO UPSAMPLING) ---\n",
    "\n",
    "print(\"--- Training Model 1 (Classical MLP) on {} ({} Features) ---\".format(os.path.basename(MRI_DATA_PATH), 677))\n",
    "\n",
    "# Load and split MRI-only data\n",
    "X_train_mri, X_test_mri, y_train_mri, y_test_mri = load_data_and_split(\n",
    "    MRI_DATA_PATH, TEST_SIZE_1, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri is None:\n",
    "    exit()\n",
    "\n",
    "# Ensure input shape matches expectation or use loaded shape\n",
    "input_shape_mri = X_train_mri.shape[1]\n",
    "expected_shape_mri = 677 # Based on previous runs\n",
    "if input_shape_mri != expected_shape_mri:\n",
    "    print(f\"Warning: MRI-only data shape is {input_shape_mri}, expected {expected_shape_mri}. Using loaded shape.\")\n",
    "    # If this is significantly different, check data loading logic again\n",
    "\n",
    "\n",
    "print(f\"Input shape for Model 1 (Classical MLP): {input_shape_mri}\")\n",
    "\n",
    "# Initialize Model 1 (Classical MLP)\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_1) # Set seed for reproducibility\n",
    "model_mri_classical = create_classical_mlp_model(input_shape_mri)\n",
    "\n",
    "\n",
    "model_mri_classical.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                      loss='binary_crossentropy' if TASK_TYPE == 'classification' else 'mean_squared_error',\n",
    "                      metrics=['accuracy'] if TASK_TYPE == 'classification' else ['mae'])\n",
    "model_mri_classical.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 1 (Classical MLP)\n",
    "early_stopping_mri_classical = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 1 (Classical MLP) training.\")\n",
    "\n",
    "\n",
    "print(f\"Fitting Model 1 (Classical MLP) on MRI-only dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "# --- Training Step ---\n",
    "history_mri_classical = model_mri_classical.fit(X_train_mri, y_train_mri,\n",
    "                                    epochs=MAX_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    validation_data=(X_test_mri, y_test_mri),\n",
    "                                    callbacks=[early_stopping_mri_classical],\n",
    "                                    verbose=1)\n",
    "print(\"Model 1 (Classical MLP) training finished.\")\n",
    "\n",
    "# Save Model 1 (Classical MLP) - Recommend saving in .keras format\n",
    "model_mri_classical.save(MODEL_MRI_SAVE_PATH_CLASSICAL)\n",
    "print(f\"Model 1 (Classical MLP) saved as {MODEL_MRI_SAVE_PATH_CLASSICAL}\")\n",
    "\n",
    "# Evaluate Model 1 (Classical MLP) on its test set\n",
    "print(\"\\nEvaluating Model 1 (Classical MLP) on MRI-only test set:\")\n",
    "eval_results_mri_classical = model_mri_classical.evaluate(X_test_mri, y_test_mri, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 1 (Classical MLP, MRI-only) Test Loss: {eval_results_mri_classical[0]:.4f}, Accuracy: {eval_results_mri_classical[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 1 (Classical MLP, MRI-only) Test Loss (MSE): {eval_results_mri_classical[0]:.4f}, Test MAE: {eval_results_mri_classical[1]:.4f}\")\n",
    "\n",
    "# Print and Plot Summary for Model 1\n",
    "print_training_summary(history_mri_classical, \"Model 1 (Classical MLP on MRI-only)\", TASK_TYPE)\n",
    "\n",
    "\n",
    "# --- Train Model 2 (Classical MLP on MRI-PET data - WITH UPSAMPLING) ---\n",
    "\n",
    "print(\"\\n--- Training Model 2 (Classical MLP) on {} ({} Features) - WITH UPSAMPLING ---\".format(os.path.basename(MRI_PET_DATA_PATH), 263))\n",
    "\n",
    "# Load and split MRI-PET data\n",
    "X_train_mri_pet_raw, X_test_mri_pet_raw, y_train_mri_pet_raw, y_test_mri_pet_raw = load_data_and_split(\n",
    "    MRI_PET_DATA_PATH, TEST_SIZE_2, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri_pet_raw is None:\n",
    "    exit()\n",
    "\n",
    "# --- Apply Upsampling to MRI-PET Training Data (AFTER SPLIT) ---\n",
    "# This is the correct place to apply upsampling.\n",
    "# Note: Upsampling is ONLY applied to the training subset.\n",
    "# sampling_strategy='auto' will balance the classes (make count of minority == count of majority)\n",
    "X_train_mri_pet_upsampled, y_train_mri_pet_upsampled = apply_upsampling(\n",
    "    X_train_mri_pet_raw, y_train_mri_pet_raw, sampling_strategy='auto'\n",
    ")\n",
    "# Note: We train on the upsampled data but validate/evaluate on the original test data\n",
    "\n",
    "# Ensure input shape matches expectation or use loaded shape\n",
    "input_shape_mri_pet = X_train_mri_pet_raw.shape[1]\n",
    "expected_shape_mri_pet = 263 # Based on previous runs\n",
    "if input_shape_mri_pet != expected_shape_mri_pet:\n",
    "    print(f\"Warning: MRI-PET data shape is {input_shape_mri_pet}, expected {expected_shape_mri_pet}. Using loaded shape.\")\n",
    "     # If this is significantly different, check data loading logic again\n",
    "\n",
    "\n",
    "print(f\"Input shape for Model 2 (Classical MLP): {input_shape_mri_pet}\")\n",
    "\n",
    "# Initialize Model 2 (Classical MLP) - Input shape based on original features\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_2) # Set a different seed\n",
    "model_mri_pet_classical = create_classical_mlp_model(input_shape_mri_pet)\n",
    "\n",
    "\n",
    "model_mri_pet_classical.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                          loss='binary_crossentropy' if TASK_TYPE == 'classification' else 'mean_squared_error',\n",
    "                          metrics=['accuracy'] if TASK_TYPE == 'classification' else ['mae'])\n",
    "model_mri_pet_classical.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 2 (Classical MLP)\n",
    "early_stopping_mri_pet_classical = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 2 (Classical MLP) training.\")\n",
    "\n",
    "print(f\"Fitting Model 2 (Classical MLP) on upsampled MRI-PET dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "# --- Training Step ---\n",
    "# Train using the upsampled data\n",
    "history_mri_pet_classical = model_mri_pet_classical.fit(X_train_mri_pet_upsampled, y_train_mri_pet_upsampled,\n",
    "                                           epochs=MAX_EPOCHS,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           validation_data=(X_test_mri_pet_raw, y_test_mri_pet_raw), # VALIDATE ON ORIGINAL TEST DATA\n",
    "                                           callbacks=[early_stopping_mri_pet_classical],\n",
    "                                           verbose=1)\n",
    "print(\"Model 2 (Classical MLP) training finished.\")\n",
    "\n",
    "# Save Model 2 (Classical MLP) - Recommend saving in .keras format\n",
    "model_mri_pet_classical.save(MODEL_MRI_PET_SAVE_PATH_CLASSICAL)\n",
    "print(f\"Model 2 (Classical MLP) saved as {MODEL_MRI_PET_SAVE_PATH_CLASSICAL}\")\n",
    "\n",
    "# Evaluate Model 2 (Classical MLP) on its original test set\n",
    "print(\"\\nEvaluating Model 2 (Classical MLP) on original MRI-PET test set:\")\n",
    "eval_results_mri_pet_classical = model_mri_pet_classical.evaluate(X_test_mri_pet_raw, y_test_mri_pet_raw, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 2 (Classical MLP, MRI-PET) Test Loss: {eval_results_mri_pet_classical[0]:.4f}, Accuracy: {eval_results_mri_pet_classical[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 2 (Classical MLP, MRI-PET) Test Loss (MSE): {eval_results_mri_pet_classical[0]:.4f}, Test MAE: {eval_results_mri_pet_classical[1]:.4f}\")\n",
    "\n",
    "# Print and Plot Summary for Model 2\n",
    "print_training_summary(history_mri_pet_classical, \"Model 2 (Classical MLP on MRI-PET)\", TASK_TYPE)\n",
    "\n",
    "\n",
    "# --- Ensembling Predictions ---\n",
    "print(\"\\n--- Ensembling Classical MLP Predictions ---\")\n",
    "print(\"Classical MLP models trained independently on datasets with different feature counts.\")\n",
    "\n",
    "print(\"\\nTo ensemble these classical models for a final prediction on a new sample, you would need:\")\n",
    "print(\"1. The {} feature representation of the new sample (processed like MRI-only data).\".format(input_shape_mri))\n",
    "print(\"2. The {} feature representation of the new sample (processed like MRI-PET data).\".format(input_shape_mri_pet))\n",
    "print(\"3. Load Model 1 Classical MLP (from {}) and feed it the {} features.\".format(MODEL_MRI_SAVE_PATH_CLASSICAL, input_shape_mri))\n",
    "print(\"4. Load Model 2 Classical MLP (from {}) and feed it the {} features.\".format(MODEL_MRI_PET_SAVE_PATH_CLASSICAL, input_shape_mri_pet))\n",
    "print(\"5. Combine their predicted probabilities (e.g., average) to get the final ensemble prediction.\")\n",
    "\n",
    "# Example of getting predictions on their respective test sets:\n",
    "print(\"\\nExample: Getting predictions on respective test sets:\")\n",
    "# --- Prediction Steps ---\n",
    "# Load models here if they weren't saved/loaded earlier in this script execution lifecycle\n",
    "# model_mri_classical_loaded = tf.keras.models.load_model(MODEL_MRI_SAVE_PATH_CLASSICAL) # Load if needed\n",
    "# model_mri_pet_classical_loaded = tf.keras.models.load_model(MODEL_MRI_PET_SAVE_PATH_CLASSICAL) # Load if needed\n",
    "\n",
    "# Use the trained models directly from memory if script runs end-to-end\n",
    "mri_only_test_predictions_classical = model_mri_classical.predict(X_test_mri)\n",
    "mri_pet_test_predictions_classical = model_mri_pet_classical.predict(X_test_mri_pet_raw) # Predict using the model trained earlier (use original test data)\n",
    "\n",
    "print(f\"Predictions from Model 1 (Classical MLP, MRI-only) on its test set (shape: {mri_only_test_predictions_classical.shape})\")\n",
    "print(f\"Predictions from Model 2 (Classical MLP, MRI-PET) on its ORIGINAL test set (shape: {mri_pet_test_predictions_classical.shape})\")\n",
    "\n",
    "# Note: You cannot directly average these predictions for a single ensemble score\n",
    "# because they are from different test sets (disjoint samples).\n",
    "\n",
    "# --- Show all plots ---\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScript execution finished.\")\n",
    "print(\"Two Classical MLP models trained and saved for ensembling based on their respective datasets, with upsampling on the MRI-PET training data.\")\n",
    "print(\"\\nThis provides a stable classical baseline. To explore QML, you would replace the 'create_classical_mlp_model' logic with your QNN implementation.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "83933e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Ensemble Prediction and Accuracy Calculation ---\n",
      "Successfully loaded model from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\model_mri_only_ensemble_qnn.h5\n",
      "Successfully loaded model from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\model_mri_pet_ensemble_qnn.h5\n",
      "\n",
      "--- Loading Unified Ensemble Test Data (PLACEHOLDER) ---\n",
      "Replace this section with your code to load and prepare your unified test data.\n",
      "You need to produce three variables:\n",
      "X_ensemble_test_mri_format (NumPy array, shape: (N_test_samples, 677))\n",
      "X_ensemble_test_mri_pet_format (NumPy array, shape: (N_test_samples, 263))\n",
      "y_ensemble_test (NumPy array of true labels, shape: (N_test_samples,))\n",
      "\n",
      "Example: Predicting for 50 samples as previously discussed.\n",
      "Placeholder unified ensemble test data created for 13 samples.\n",
      "  X_ensemble_test_mri_format shape: (13, 677)\n",
      "  X_ensemble_test_mri_pet_format shape: (13, 263)\n",
      "  y_ensemble_test shape: (13,)\n",
      "\n",
      "Getting ensemble predictions for 13 samples...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Averaged predicted probabilities.\n",
      "Ensemble prediction finished.\n",
      "\n",
      "--- Visual Metric: Ensemble Accuracy Bar Chart ---\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAF2CAYAAAAskuGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4HElEQVR4nO3dCZzV8/7H8c9MY1pG+0aLNm7ZWrRa4o8IiayJqyRlyxaXQqXQRukislX3IoVLWVKSslzhVpOEyhIppSbt2zTN+T/eX/ece86Zc2brW7O9no/Hj3O+57d8v79zmt/n991+CYFAIGAAAAAeJfrcGQAAgBBgAAAA7wgwAACAdwQYAADAOwIMAADgHQEGAADwjgADAAB4R4ABAAC8I8AAAADeEWAAhcCkSZMsISHBFixYkOO6//d//+eW4uLnn392Zdc5CHrggQdcmi/z5s1z+9P/ARwcBBgoUhfgeMvnn39e0Fkssq655pqIc1mhQgVr1qyZjR492vbs2WNFyVNPPRURqBQ2bdq0cef46aefLuisAAdc0oE/BODP0KFDrUGDBlnSjzzyyALJT3FRunRpe/75593rzZs327/+9S+766677D//+Y9NmTLloOfn/vvvt/79++crwKhWrZoLmsKdeuqptmvXLktOTraC8v3337vzWb9+fXv55ZftxhtvLLC8AAcDAQaKlHPPPddatWpV0NkodpKSkuyvf/1r6P1NN91kbdu2talTp9qYMWOsVq1aWbbRcxJ3795tZcuWPSD50eJLYmKilSlTxgrSSy+9ZDVq1HA1Q5deeqlrGlKwUdhkZmZaenp6gZ8vFH00kaBYCbbnP/roo/bss89ao0aN3N1569at3d1juHXr1lnPnj2tTp06bp3DDz/cLrzwQrePcO+99561b9/eUlJSrHz58tapUyf75ptvItbRHfOhhx5qq1atsvPPP9+9rl27to0bN859/vXXX9sZZ5zh9lGvXj2bPHlyzPzv3LnTrr/+eqtataprqujevbtt2rQpx3KrKWPw4MGuJkdlqVu3rt199935buLQBTnYzyN4PnQxVNlmzZrlgjwFFs8880yo1uP22293x9XxlY+RI0e6i1U4radzVbFiRatUqZL16NHDpUWL1wdDF2k1M5QrV84qV67saibef//9UP70vXz00Ueh5p5gGeL1wXjttdesZcuWriyq+VCQtWbNmpjfrdK7dOniXlevXt3V8Ozbty/X51TfuQILnUOVP95v4IsvvrDzzjvPlU+/l6ZNm9rf//73iHWWLVtml19+ucuH8t64cWO77777IvIcK3iJdV71vm/fvq5W5dhjj3Xf38yZM91n+nd00kknud+jjqNz9frrr8fMd3bfjb5nnd+9e/dm2e7ss892+UfxQ4CBImXLli2WlpYWsWzcuDHLevrj/cgjj7iL9UMPPeQukhdffHHEH7hLLrnE3nzzTRdkqGr91ltvtW3btrkgIejFF190AYUuKrpgDhw40L799ls75ZRTsgQiutiohkUX2VGjRrk/8PrDrT4B55xzjrsoax8KUhQ4rFy5Mku+tf53333nLgRaR3/0dVFTbUE8uohfcMEF7mLQuXNne+KJJ9w2jz32mHXt2jXf5/rHH390/9fFJWj58uXWrVs3O+uss9xFr3nz5i4oOu2009wFRnl+/PHH7eSTT7YBAwZYv379QtuqDArgdE51Idf3snr1anfxyY0hQ4bY1VdfbYcccohrKtN7nesPP/zQfT527FgXLDZp0sQdQ0v4RTeavhddpEuVKmXDhw+33r172xtvvOG+2+igR99tx44d3bnQeVZ5VROhIDY3FDT88MMP7typmUa/RX230WbPnu0uzPqN3Xbbbe4Yp59+ur3zzjuhdZYsWeJql1Ru5Vnfg77vt99+2/JL+7rjjjvc70X7CwYnet2iRQt3vocNG+ZqlS677DJ799138/Td6DP9O1VwGh3ka53w2jMUIwGgCJg4caKusDGX0qVLh9ZbuXKlS6tatWrgjz/+CKVPnz7dpb/99tvu/aZNm9z7Rx55JO4xt23bFqhUqVKgd+/eEenr1q0LVKxYMSK9R48ebn/Dhg0LpekYZcuWDSQkJASmTJkSSl+2bJlbd/DgwVnK17Jly0B6enoofdSoUS5d+Q867bTT3BL04osvBhITEwOffPJJRD7Hjx/vtv33v/+d7blV3lNSUgIbNmxwyw8//ODKoXw3bdo0tF69evXc/mbOnBmx/YMPPui2X7FiRUR6//79A6VKlQqsWrXKvZ82bZrbXmUKysjICLRv396l6xwE6dyE/3n6/vvvXRkvuuiiwL59+yKOk5mZGXp97LHHRpyboLlz57r96f+ic1yjRo3AcccdF9i1a1dovXfeecetN2jQoIjzo7ShQ4dG7LNFixbu+8qNvn37BurWrRvK6/vvv+/2mZqaGnEuGjRo4M6zfjvxynjqqacGypcvH/jll1/irqM8az/Ros+r6L3O7TfffJNl/Z07d0a813nTOTvjjDPy9N0ovU6dOoGuXbtGfD5mzBj3O/vpp5+yHBtFHzUYKFLU5KC7vPBFTRjRdCematogNXHITz/95P6v6l7dSarKPF4ThPatO1nddYbXmOiOV3eQc+fOzbLNddddF3qtJgBV/aqaW3fKQUrTZ8G8hOvTp4+7CwxSR0DdNc6YMSPuOVE1/9FHH+3u3MPzqSYZiZXPaDt27HDV7VrUvHHvvffaiSee6Gp4wqmDre7ko4+v86vzHX78Dh06uDv/jz/+2K2nMqgs4Z0bdS5vueWWHPM3bdo0V1MzaNAg13wTLj/DWTUceP369a6vSXhfA9VW6TxG36HLDTfcEPFeZY71HUbLyMhwfVn0mwzmVd+N+mOE12Kkpqa6Wi01Nen3EauMGzZscOfz2muvtSOOOCLmOvmhGpljjjkmS3p4/xr9O1ENosq9aNGiPH03Sr/qqqvsrbfecrWEQSq/mmBiddxG0UcnTxQpauPNTSfP6D++wWAjGEyonVnNFXfeeafVrFnT2rVr59rGVcV/2GGHhXr9S/BCHU19JMLpQqULdDi1tavaPvqPv9JjBTZHHXVUxHs1zahvSHRzTDjlU80q0ccO0oU0J8p7sIpd50Z/8JXvaLEuBDq+qu1zOv4vv/ziyqIyhctN+7uaa3SRinURzA/lJd6xFWB8+umnOX63+k3lpn+M+iEoMNBvV80kQWr6eOWVV9zvUGULNkkdd9xxcfcVDGiyWyc/4l3g1TSjpqzFixdH9OcJ/z3n9rvRvy2VVUGrXqu5beHChTZ+/HiPJUFhQoCBYkl3xrGE92XQnaL6LOgOTG3D6l+htni1CavdOdhBUW35waAjXPQoh3jHzE1e9ofyefzxx7vRHrGoLTwnyqNqHHISa8SIjq8+GepUGstf/vIXK+rifYe5EaylCK/FCqdOqQo2fIpXmxGvU2qs7/WTTz5xfXvUJ0R9lBQcqnZt4sSJcTuoZkcBiDqJBvvq6P+qRYx3XlD0EWCgRNMoE9ViaNGduDotqmOd/vjpM1FVdm4uvj4oD+EXm+3bt9vatWvdqILsyvDVV1/ZmWee6XX2y9zS8ZXPnM6RRs/MmTPHrRtei6E72dwcQ4GMOj/qO4ont+VXXoLHjq6hUlrw8/2lpqfp06e75hGNIImmjsUKQPSdB39vS5cujXsuGzZsGFonO6pdiTU6J1hzkxuaC0U1Nwq+VasVpAAjP9+NKLBQx1/9phWkqEkqvCkTxQt9MFAiaeSD5nCI/kOpER7BqmD1NVAziHrPxxpep2pv3zQqIfxYmvFRbfganRKP7gA1hPK5557L8pkml9JF7kDS8efPn59lhIDoIqf8i4IkvQ6fxVJ31Br1khONklA1vEYoRA99Da8JUn+XWBfWaGpmU+Co6vnwqn/151Fzky58Pqg5QOf/5ptvdgFG9KJmOV3IlYcTTjjBNVVoNEx0GYJlVDONahQmTJgQMdopfJ3gb1n9JdR0FaSLenSfmpxqbRSwhdd6qKlONX75+W5E/Zm0T42QUXMPo0eKN2owUKToAqA5AKKpo1jw7i43VqxY4e74dXFU1a2aO/TH9/fff7crrrjCraPgQhdDDbHTH3+l6w+8/rCrE6CGYj755JNey6cJjoL50p20qqY1bFJV1fEof6+++qrrhKgOncqXLgo6T0oPzltxoPztb39znfd0sdT8C6oG10VVc39ozgRdlDQHgpqjlDfN0Kk0nXcNC9WFMCfqeKohpw8++KDrZKhhnrqr1twmmgRMTVuiY+s7U78BbaMgIlYfGlX1qz+Ahiirg6MufPrug0M0NWTTB9VOaGirfp+x6HtVYKjfk8qkvOs8qSZAeVOzhL5Hze8RDOA0DFi/Cf0m1SlYQYnOp/ahvhKi3+o999xjF110kaslUUCtfau5KryDZnYUZKnZTUOsr7zySteXRp2sdV7DA5fcfjeifz/anzoGqyOrr0AOhVRBD2MB9neYavgQx+Aw1VjDT8OHhqalpQVuvvnmQJMmTdwQSw07bdu2beDVV1/Nsp2GNnbs2NGtU6ZMmUCjRo0C11xzTWDBggVZhnpG05BJDZ2MpiGEnTp1ylK+jz76KNCnT59A5cqVA4ceemjgqquuCmzcuDHLPqOHYmr44MiRI92xNGxX22sI5ZAhQwJbtmzJ9tzGy3tOeY4e0jtgwIDAkUceGUhOTg5Uq1YtcNJJJwUeffTRiGG3KsvVV18dqFChgjufeq2hmjkNUw2aMGGCGx4aLKPOw+zZsyOGECuPGsap7YPnKXqYatDUqVND+6tSpYo736tXr87V+YmXx6Dff/89kJSU5MoYj4aBlitXzg3xDPr0008DZ511liuDjquhwk888UTEdkuXLnXbaBi1fpONGzcODBw4MGIdDYXVkFJ9H/r8pZdeijtMVf8WYnnhhRcCRx11lDs/+rei7yi/302Q/o1pe/3OUbwl6D8FHeQAAEoG9UlRs4qG2waHj6N4IsAAABw0akpTPxcN2S2ITsk4eOiDAQA44PRUXvXdUF8R9XUhuCj+qMEAABxwCig0PFlDdjV6x+fTclE48Q0DAA447mVLHubBAAAA3hFgAAAA70pcE4lmmvvtt9/cjI10MgIAIG9NXXoiriZRi356rpX0AEPBRW4e/gQAAGL79ddfYz5xuUQHGKq5CJ6c6MdtAwCA+LZu3epu0oPX0uyUuAAj2Cyi4IIAAwCAvMtNFwM6eQIAAO8IMAAAgHcEGAAAwDsCDAAA4B0BBgAA8I4AAwAAeEeAAQAAileA8fHHH1vnzp3dlKMaUztt2rQct5k3b56dcMIJVrp0aTvyyCNt0qRJByWvAACgiAQYO3bssGbNmtm4ceNytf7KlSutU6dOdvrpp9vixYvt9ttvt+uuu85mzZp1wPMKAAByr0Bn8jz33HPdklvjx4+3Bg0a2OjRo937o48+2j799FN77LHHrGPHjgcwpwAAoNj2wZg/f7516NAhIk2BhdIBAEDhUaSeRbJu3TqrWbNmRJre6+Eru3btsrJly2bZZs+ePW4J0rqSkZHhFtEjZ7XoUe5agoLp+/btc4+ozSm9VKlSri9JcL/h6aL1c5OelJTk9huerv1q/eg8xkunTJSJMlEmykSZfJep2AYY+TF8+HAbMmRIlvTU1FRLSUlxr6tXr26NGjVyfTw2bNgQWkePotWyYsUK27JlSyi9YcOGVqNGDVu6dKkLbIKaNGlilSpVcvsO/1KaNm1qycnJtmDBgog8tGrVytLT023JkiURX2rr1q3d8ZYtWxZKV/Ck/ippaWn2008/hdIrVqzomor0GPrVq1eH0ikTZaJMlIkyUSbfZWrcuLHlVkIgPMQpQIqs3nzzTevSpUvcdU499VQ3gmTs2LGhtIkTJ7rOnuEnOKcaDD1qduPGjaGnqRa2CLE4Rr2UiTJRJspEmazIl2nnzp0uGNI1N6cnkhepGowTTzzRZsyYEZE2e/Zslx6PhrNqiaYvXku44BcTLXiic5sevd/8pOuHECs9Xh7zmk6ZKFO8dMpEmbLLO2WiTEWik+f27dvdcFMtoiogvV61apV7P2DAAOvevXto/RtuuMFVKd19992uyumpp56yV1991e64444CKwMAAChkAYbafFq0aOEW6devn3s9aNAg937t2rWhYEM0RPXdd991tRZqw9Jw1eeff54hqgAAFDKFpg/GwaI+GLltPwIAAPm7hhapeTAAAEDRQIABAAC8I8AAAADeEWAAAADvCDAAAIB3BBgAAMA7AgwAAOAdAQYAAPCOAAMAAHhHgAEAALwjwAAAAN4RYAAAAO8IMAAAgHcEGAAAwDsCDAAA4B0BBgAA8I4AAwAAeEeAAQAAvCPAAAAA3hFgAAAA7wgwAACAdwQYAADAOwIMAADgHQEGAADwjgADAAB4R4ABAAC8I8AAAADeEWAAAADvCDAAAIB3BBgAAMA7AgwAAOAdAQYAAPCOAAMAAHhHgAEAALwjwAAAAN4RYAAAAO8IMAAAgHcEGAAAwDsCDAAA4B0BBgAA8I4AAwAAeEeAAQAAvCPAAAAA3hFgAAAA7wgwAACAdwQYAADAOwIMAADgHQEGAAAofgHGuHHjrH79+lamTBlr27atffnll9muP3bsWGvcuLGVLVvW6tata3fccYft3r37oOUXAAAU8gBj6tSp1q9fPxs8eLAtWrTImjVrZh07drT169fHXH/y5MnWv39/t/53331nL7zwgtvHvffee9DzDgAACmmAMWbMGOvdu7f17NnTjjnmGBs/fryVK1fOJkyYEHP9zz77zE4++WS78sorXa3H2Wefbd26dcux1gMAAJSQACM9Pd0WLlxoHTp0+F9mEhPd+/nz58fc5qSTTnLbBAOKn376yWbMmGHnnXfeQcs3AADIWZIVkLS0NNu3b5/VrFkzIl3vly1bFnMb1Vxou1NOOcUCgYBlZGTYDTfckG0TyZ49e9wStHXrVvd/baslGNhoyczMdEtQMF351PFySi9VqpQlJCSE9hueLlo/N+lJSUluv+Hp2q/Wj85jvHTKRJkoE2WiTJTJd5mKRICRH/PmzbNhw4bZU0895TqE/vDDD3bbbbfZgw8+aAMHDoy5zfDhw23IkCFZ0lNTUy0lJcW9rl69ujVq1MhWrlxpGzZsCK1Tp04dt6xYscK2bNkSSm/YsKHVqFHDli5dart27QqlN2nSxCpVquT2Hf6lNG3a1JKTk23BggUReWjVqpWryVmyZEnEl9q6dWt3vPBAS51a1UdFAZZqboIqVqxoRx99tP3222+2evXqUDplokyUiTJRJsrku0waZJFbCYHwEOcgUsbV3+L111+3Ll26hNJ79OhhmzdvtunTp2fZpn379tauXTt75JFHQmkvvfSS9enTx7Zv3+6ittzUYGj0ycaNG61ChQqFMkIsjlEvZaJMlIkyUSYr8mXauXOnC4YU1ASvoYWuBkMRU8uWLW3OnDmhAEMnT+/79u0bcxsVLDqICJ6EeHFS6dKl3RJNX7yWcMEvJlrwGLlNj95vftL1Q4iVHi+PeU2nTJQpXjplokzZ5Z0yUaYi0USiIaqqsVBVTJs2bdwcFzt27HCjSqR79+5Wu3Zt18whnTt3diNPWrRoEWoiUdOI0uOdTAAAcPAVaIDRtWtX16Y0aNAgW7dunTVv3txmzpwZ6vi5atWqiIjt/vvvd9Gg/r9mzRrXNqXg4uGHHy7AUgAAgELTB6OgqA9GbtuPAABA/q6hBT5VOAAAKH4IMAAAgHcEGAAAwDsCDAAA4B0BBgAA8I4AAwAAeEeAAQAAvCPAAAAA3hFgAAAA7wgwAACAdwQYAADAOwIMAADgHQEGAADwjgADAAB4R4ABAAC8I8AAAADeEWAAAADvCDAAAIB3BBgAAMA7AgwAAOAdAQYAAPCOAAMAAHhHgAEAALwjwAAAAN4RYAAAAO8IMAAAgHcEGAAAwDsCDAAA4B0BBgAA8I4AAwAAeEeAAQAAvCPAAAAA3hFgAAAA7wgwAACAdwQYAADAOwIMAADgHQEGAADwjgADAAB4R4ABAAC8I8AAAADeEWAAAADvCDAAAIB3BBgAAMA7AgwAAOAdAQYAACj4AKN+/fo2dOhQW7Vqlf/cAACAkhlg3H777fbGG29Yw4YN7ayzzrIpU6bYnj17DkzuAABAyQkwFi9ebF9++aUdffTRdsstt9jhhx9uffv2tUWLFh2YXAIAgJLRB+OEE06wxx9/3H777TcbPHiwPf/889a6dWtr3ry5TZgwwQKBQK72M27cONfsUqZMGWvbtq0LXLKzefNmu/nmm11QU7p0afvLX/5iM2bMyG8xAADAAZCU3w337t1rb775pk2cONFmz55t7dq1s169etnq1avt3nvvtQ8++MAmT56c7T6mTp1q/fr1s/Hjx7vgYuzYsdaxY0dbvny51ahRI8v66enprllGn73++utWu3Zt++WXX6xSpUr5LQYAADgAEgK5rWr4LzWDKKh45ZVXLDEx0bp3727XXXedNWnSJLTO0qVLXW3Grl27st2Xggqt9+STT7r3mZmZVrduXdfs0r9//yzrKxB55JFHbNmyZXbIIYdYfmzdutUqVqxoW7ZssQoVKuRrHwAAlERb83ANzXMNhgIC1SI8/fTT1qVLl5gX+gYNGtgVV1yR7X5UG7Fw4UIbMGBAKE0BS4cOHWz+/Pkxt3nrrbfsxBNPdE0k06dPt+rVq9uVV15p99xzj5UqVSrmNuqAGt4JVSdHMjIy3BI8rhYFOFrC86Nl3759EU0+8dKVh4SEhNB+w9NF6+cmPSkpye03PF371frReYyXTpkoE2WiTJSJMvkuU17kOcD46aefrF69etmuk5KS4mo5spOWluYyXrNmzYh0vVcNRbxjf/jhh3bVVVe5fhc//PCD3XTTTa65Rv1AYhk+fLgNGTIkS3pqaqrLpyhQadSoka1cudI2bNgQWqdOnTpuWbFihYvWgjSCRs00qqkJr6VRLY6aa7Tv8C+ladOmlpycbAsWLIjIQ6tWrVygtWTJkogvVUGcjhd+HsqWLWvNmjVz503nIUiRpDrbqi+MmqeCKBNlokyUiTJRJt9laty4sR2wJpL//Oc/LopS80a4L774wh1cmcoNnUT1ofjss89crUTQ3XffbR999JHbXzR16Ny9e7c70cHoasyYMa7ZZO3atbmuwVAzzMaNG0PVO4UtQiyOUS9lokyUiTJRJivyZdq5c+eBayJR84SCgOgAY82aNTZy5MiYgUEs1apVcwX4/fffI9L1/rDDDou5jUaOqEkmvDlEEd+6detcpKUoLJpGmmiJpi9eS7jgFxMtXvNLvPTo/eYnXT+EWOnx8pjXdMpEmeKlUybKlF3eKRNlOmDDVL/99ls3RDVaixYt3Ge5pWCgZcuWNmfOnFCaojO9D6/RCHfyySe7ZpHwKE7VQwo8YgUXAACgYOQ5wFBtQHStg6iJIq+RjoaoPvfcc/aPf/zDvvvuO7vxxhttx44d1rNnT/e5RqiEdwLV53/88YfddtttLrB49913bdiwYa5WBQAAFOEA4+yzz3YX/fBOJZr8SnNfaHRJXnTt2tUeffRRGzRokJugSzOEzpw5M9TxU887Ce9bob4Ts2bNcv1A1DHl1ltvdcFGrCGtAA6OvEyWN2nSJFdlHL5ou3APPPCA65CmTtiVK1d2I8uim151g3HhhRe6pla1A59yyik2d+7ciHWij6NFjzYAcHDkuZOn+lqceuqprpOkmkVEgYGCAk24pSCgMGMeDMAfTZanmsbwyfJee+21uJPlKcDQTYE+D9KFP3w0mSbo07bqDa9e8I899pjbp5pH1aM+2OH7qKOOcqPE1Ctfx9W+f/zxx1AfLu1Xo9nOOeec0L7Viz46oAFwYK6heQ4wRM0YL7/8sn311VfuH7dqE7p165bvya8OJgIMwJ+8TpanIEDPM1KtZ17/zWp24DPPPNMN71Og8fHHH1v79u3dOtu2bXP/nnWToxqPYICh2YY1Xw+Ag38NzdezSFR12adPH1c1qiYO3cEUheACgD/ByfKCF3TJabI82b59u5tLR4GImjm++eabbI/x7LPPuj9omjtAqlat6sbi//Of/3Q3Oxp298wzz7haD3UcD6f+WWpGadOmTZ6ekQRg/+V7/IlGjKiPhP4AhLvgggs8ZAtAYZefyfIUGOhCr1pP3QHpBuWkk05yQYYmDQp655133GzAGnOvUWKqmVCgEKyZUG2GaibKly/vghoFF+q/pT4bQUOHDrUzzjjDypUrZ++//76blE/BjfpuATjw8jWT50UXXWRff/21+4cevCPQ6/2dVhRA8aYh6OHD0BVcaC4b1UA8+OCDofTTTz/d9e1SEKORZpdffrnr6KlAQn9zVDOh15988olrptXTnDt37uw6gCsgkYEDB4b2p/5iqu3QpHwEGMDBkecmEnXQ0rNG1q9f7+4MdOehtlDN4Dlv3rwDk0sAhU5+JsuLpqZVXfzVgTO6GfbII490T2l+4YUX3BB4/V/0uADVcGhEiObG0bw8Tz31lAs0NOQ9u/4imoI5fGZfAIUowFDbqqoe9cclOLOYhoipNzd3BkDJkZ/J8qKpxlO1ocFah3i032BgoGYTiZ7VMDidcjyqEVETSqyZfQEUgiYS/UFQu6coyNAzRdSuqk5b4UPPABR/miyvR48ergZTHSk1XDR6sjw9c0g3IKKbE9VKqHZCI0nUZPHLL7/Ydddd5z7Xtg8//LDry6WgQ00k6kyu4fGXXXaZW0fBiwIFHVdz6KjmQs0oekZRp06d3Dpvv/22q0nRsTQsVX04NCnfXXfdVWDnCihp8hxgHHfccW54qppJVOU4atQodyejnt4atw6g5NBkeXrCoy70eiaQJsyLniwvvKZh06ZN1rt3b7euggTVgOiBh8ccc4z7XE0u6iCqpg4FFxoxomGw6mtx7LHHhm5sdIz77rvPdeLU05T12fTp00MjTdT0osDkjjvucH02FNDowYg6NoCDI8/zYGgmTd1lXHzxxa7d9Pzzz3ez6ukPgSbd0T/4wox5MAAAKKQTbUXT80F0NxIcSVKYEWAAAFDIJtpSVaR6cy9dujQivUqVKkUiuAAAAIWwD4baNY844gjmuohhRGpaQWcBAIAI/Vv8OUFdkRimqo5VenKqmkUAAAC8jCLRQ43UubNWrVpuaKomxAm3aNGivO4SAACU9ACDJxMCAADvAcbgwYPzugkAAChh8vW4dgAAAK81GJqVL7shqYwwAQAAeQ4w3nzzzSxzY6SmprqpfYcMGeIzbwAAoKQEGBdeeGGWtEsvvdQ9C0BThffq1ctX3gAAQEnvg6GnFoY/thkAAJRcXgKMXbt22eOPP+4eywwAAJDnJpLoh5rpWWnbtm2zcuXK2UsvveQ7fwAAoCQEGI899lhEgKFRJdWrV7e2bdu64AMAACDPAcY111xzYHICAABKbh+MiRMn2muvvZYlXWkaqgoAAJDnAGP48OFWrVrWx7/WqFHDhg0b5itfAACgJAUYq1atsgYNGmRJ15NV9RkAAECeAwzVVCxZsiRL+ldffWVVq1b1lS8AAFCSAoxu3brZrbfeanPnznXPHdHy4Ycf2m233WZXXHHFgcklAAAo3qNIHnzwQfv555/tzDPPtKSkPzfPzMy07t270wcDAADkL8BITk52zxx56KGHbPHixVa2bFk7/vjjXR8MAACAfAUYQUcddZRbAAAA9rsPxiWXXGIjR47Mkj5q1Ci77LLL8ro7AABQDOU5wPj444/tvPPOy5J+7rnnus8AAADyHGBs377d9cOIdsghh9jWrVt95QsAAJSkAEMdOtXJM9qUKVPsmGOO8ZUvAABQkjp5Dhw40C6++GL78ccf7YwzznBpc+bMscmTJ9vrr79+IPIIAACKe4DRuXNnmzZtmpvzQgGFhqk2a9bMTbZVpUqVA5NLAABQ/IepdurUyS2ifhevvPKK3XXXXbZw4UI3sycAACjZ8twHI0gjRnr06GG1atWy0aNHu+aSzz//3G/uAABA8a/BWLdunU2aNMleeOEFV3Nx+eWX2549e1yTCR08AQBAnmsw1PeicePG7kmqY8eOtd9++82eeOKJ3G4OAABKkFzXYLz33nvuKao33ngjU4QDAAA/NRiffvqpbdu2zVq2bGlt27a1J5980tLS0nK7OQAAKEFyHWC0a9fOnnvuOVu7dq1df/31bmItdfDUo9pnz57tgg8AAIB8jSJJSUmxa6+91tVofP3113bnnXfaiBEjrEaNGnbBBRdwVgEAQP6HqYo6feopqqtXr3ZzYeTXuHHjrH79+lamTBnX/PLll1/majvVoiQkJFiXLl3yfWwAAFDIAoygUqVKuYv8W2+9ledt9VyTfv362eDBg23RokVuVtCOHTva+vXrs93u559/dpN7tW/ffj9yDgAACm2AsT/GjBljvXv3tp49e7q5NMaPH2/lypWzCRMmxN1Gs4VeddVVNmTIEGvYsOFBzS8AACjkAUZ6erqbXrxDhw7/y1Bions/f/78uNsNHTrU9fno1avXQcopAAA44M8i8UXDXFUbUbNmzYh0vV+2bFnMbdS5VDOJLl68OFfH0EyjWoI0A6lkZGS4JRjUaNGIGC1BwXTlMRAIZJuekLnPAgmJZgkJ7nU4l651Apm5S08sZRYIRKYnJPy5ftz0TEsIy2MgIcEsm3S3j4j0/+Y9XjplokyUiTJRpiJXpoz/XufUlUF9FoPvg5Qu0c8Ri5deZAKMvNJQ2KuvvtoNl61WrVquthk+fLhrSomWmprqRsRI9erVrVGjRrZy5UrbsGFDaJ06deq4ZcWKFbZly5ZQupplVIOydOlS27Vrl0urvSXd0iodYbuTD7Vaf3xvCWGByroqjWxfYpLVTlsekYc11RpbqcwMO+yPH0NpgcREW1OtiZXZu8OqbV4VSs9IKu32k7J7s1XetjaUvjs5xdIq1bMKOzdahR3/y/uOspVsU/laVnn7OkvZtTmUvjWluluqbvnVyqTvCKVvKn+47Shb2WpuWmlJGf8LyCgTZaJMlIkyFd0yLViQ7NKbNGlilSpVcte+8KChadOmlpycbAsWLIgoU6tWrVwrg2bvDg86NLgjtxIC4bfmB5kyr/4Weux7+EgQPURt8+bNNn369Ij1VWvRokWLUGQlwRoH1SgsX77cBQo51WDUrVvXNm7caBUqVPBWgzH6q43FKurNkk6ZKBNlokyUqciV6c5mVb3WYOzcudMqVqzobrqD19BCWYOhqEkzg86ZMycUYOgCr/d9+/bNsr4iMM29Ee7+++93NRt///vfXeAQrXTp0m6JlpSU5JZwwcAhWnhAEy/d/ZBivA4XSMhDuvux5SU90QIJluv0P3+EeUinTJSJMlEmylTkypQUdZ2Lfp/f9CLRRKIhqqqxUHVMmzZt3IPUduzY4UaVSPfu3a127dquqUPzZBx33HER26vKR6LTAQBAwSnwAKNr166u38OgQYPc4+CbN29uM2fODHX8XLVqVcxaBQAAUHgVaB+MgqA+GLltP8qLEak8+A0AULj0b5G7AREH4hpK1QAAAPCOAAMAAHhHgAEAALwjwAAAAN4RYAAAAO8IMAAAgHcEGAAAwDsCDAAA4B0BBgAA8I4AAwAAeEeAAQAAvCPAAAAA3hFgAAAA7wgwAACAdwQYAADAOwIMAADgHQEGAADwjgADAAB4R4ABAAC8I8AAAADeEWAAAADvCDAAAIB3BBgAAMA7AgwAAOAdAQYAAPCOAAMAAHhHgAEAALwjwAAAAN4RYAAAAO8IMAAAgHcEGAAAwDsCDAAA4B0BBgAA8I4AAwAAeEeAAQAAvCPAAAAA3hFgAAAA7wgwAACAdwQYAADAOwIMAADgHQEGAADwjgADAAB4R4ABAAC8I8AAAADeEWAAAADvCDAAAIB3BBgAAMA7AgwAAFA8A4xx48ZZ/fr1rUyZMta2bVv78ssv46773HPPWfv27a1y5cpu6dChQ7brAwCAEhhgTJ061fr162eDBw+2RYsWWbNmzaxjx462fv36mOvPmzfPunXrZnPnzrX58+db3bp17eyzz7Y1a9Yc9LwDAIDYEgKBQMAKkGosWrdubU8++aR7n5mZ6YKGW265xfr375/j9vv27XM1Gdq+e/fuOa6/detWq1ixom3ZssUqVKhgvoxITfO2LwAAfOjfopr5lJdraIHWYKSnp9vChQtdM0coQ4mJ7r1qJ3Jj586dtnfvXqtSpcoBzCkAAMiLJCtAaWlprgaiZs2aEel6v2zZslzt45577rFatWpFBCnh9uzZ45bw6EsyMjLcEgxqtKj2REtQMF15DK/oiZWekLnPAgmJZgkJ7nU4l+6qizJzl55YyiwQiExPSPhz/bjpmaqOCtt3glk26W4fEen/zXu8dMpEmSgTZaJMRa5MGf+9zpUqVcoSEhJC74OULrqe5Sa9yAQY+2vEiBE2ZcoU1y9DHURjGT58uA0ZMiRLempqqqWkpLjX1atXt0aNGtnKlSttw4YNoXXq1KnjlhUrVrjqoKCGDRtajRo1bOnSpbZr1y6XVntLuqVVOsJ2Jx9qtf743hLCApV1VRrZvsQkq522PCIPa6o1tlKZGXbYHz+G0gKJibamWhMrs3eHVdu8KpSekVTa7Sdl92arvG1tKH13coqlVapnFXZutAo7/pf3HWUr2abytazy9nWWsmtzKH1rSnW3VN3yq5VJ3xFK31T+cNtRtrLV3LTSkjL+F5BRJspEmSgTZSq6ZVqwINmlN2nSxCpVquSufeFBQ9OmTS05OdkWLFgQUaZWrVq5VoYlS5ZEBB2NGze2ItEHQ5kvV66cvf7669alS5dQeo8ePWzz5s02ffr0uNs++uij9tBDD9kHH3zgTkQ8sWow1Mdj48aNofYjHzUYo7/aWKyi3izplIkyUSbKRJmKXJnubFbVaw2GuiXktg9GgdZgKGpq2bKlzZkzJxRg6AKv93379o273ahRo+zhhx+2WbNmZRtcSOnSpd0SLSkpyS3hgoFDtOCJzi7d/ZBivA4XSMhDuvux5SU90QIJluv0P3+EeUinTJSJMlEmylTkypQUdZ2Lfp/f9CLRRKIhqqqxUKDQpk0bGzt2rO3YscN69uzpPtfIkNq1a7umDhk5cqQNGjTIJk+e7ObOWLdunUs/9NBD3QIAAApegQcYXbt2df0eFDQoWGjevLnNnDkz1PFz1apVEbUKTz/9tGtaufTSSyP2o3k0HnjggYOefwAAUAjnwTjYmAcDAFBS9C+p82AAAIDiiQADAAB4R4ABAAC8I8AAAADeEWAAAADvCDAAAIB3BBgAAMA7AgwAAOAdAQYAAPCOAAMAAHhHgAEAALwjwAAAAN4RYAAAAO8IMAAAgHcEGAAAwDsCDAAA4B0BBgAA8I4AAwAAeEeAAQAAvCPAAAAA3hFgAAAA7wgwAACAdwQYAADAOwIMAADgHQEGAADwjgADAAB4R4ABAAC8I8AAAADeEWAAAADvCDAAAIB3BBgAAMA7AgwAAOAdAQYAAPCOAAMAAHhHgAEAALwjwAAAAN4RYAAAAO8IMAAAgHcEGAAAwDsCDAAA4B0BBgAA8I4AAwAAeEeAAQAAvCPAAAAA3hFgAAAA7wgwAACAdwQYAADAOwIMAABQPAOMcePGWf369a1MmTLWtm1b+/LLL7Nd/7XXXrMmTZq49Y8//nibMWPGQcsrAAAoAgHG1KlTrV+/fjZ48GBbtGiRNWvWzDp27Gjr16+Puf5nn31m3bp1s169ellqaqp16dLFLUuXLj3oeQcAALElBAKBgBUg1Vi0bt3annzySfc+MzPT6tata7fccov1798/y/pdu3a1HTt22DvvvBNKa9eunTVv3tzGjx+f4/G2bt1qFStWtC1btliFChW8lWNEapq3fQEA4EP/FtXMp7xcQ5OsAKWnp9vChQttwIABobTExETr0KGDzZ8/P+Y2SleNRzjVeEybNi3m+nv27HFLkE6K/PHHH5aRkRE6phYFN1rC86Jl3759Fh6HxUrfs3WzBRISzRISLCFzX0QeXLqL5jJzl55YyiwQiExPSPhz/bjpmYoWw/adYJZNuttHRPp/8x4vnTJRJspEmShTkSvTH3/8mbdSpUpZQkJC6LoXpHTR9Sw36Tt37vzzOLmomyjQACMtLc1lvmbNmhHper9s2bKY26xbty7m+kqPZfjw4TZkyJAs6Q0aNNivvAMAUNg9cID2u23bNleTUWgDjINBtSPhNR6qoVDtRdWqVV00B6BwURWsmkl//fVXr82YAPafai4UXNSqVSvHdQs0wKhWrZqrhvn9998j0vX+sMMOi7mN0vOyfunSpd0SrlKlSvuddwAHloILAgyg8Mmp5qJQjCJJTk62li1b2pw5cyJqGPT+xBNPjLmN0sPXl9mzZ8ddHwAAHHwF3kSi5osePXpYq1atrE2bNjZ27Fg3SqRnz57u8+7du1vt2rVdXwq57bbb7LTTTrPRo0dbp06dbMqUKbZgwQJ79tlnC7gkAACg0AQYGna6YcMGGzRokOuoqeGmM2fODHXkXLVqlRuxEXTSSSfZ5MmT7f7777d7773XjjrqKDeC5LjjjivAUgDwRU2amhcnumkTQNFS4PNgAACA4qfAZ/IEAADFDwEGAADwjgADAAB4R4ABYL/8/PPPbtK6xYsXx11n3rx5bp3Nmzcf1LwBKDgEGEAhcc0117iLcPRyzjnnFHTWCpVdu3ZZlSpV3ER94c8ZAlC4FPgwVQD/o2Bi4sSJEWkM14z0r3/9y4499lg3ZbGGqGuoe0FRHvQ8paQk/pQC0ajBAAoRBROa9j58qVy5cuhz1Wg8//zzdtFFF1m5cuXcPDBvvfVW6PNNmzbZVVddZdWrV7eyZcu6z8MDFj3f4/LLL3fT5asW4MILL3RNHOG1KF26dLFhw4a5uWi03tChQ90TGP/2t7+5berUqZMlCBI9oFDz1JQpU8bNS/PRRx9lW9ZPP/3U2rdv7/KpZ4/ceuutbpK9nLzwwgv217/+1S16He2bb76x888/300zXr58eXeMH3/8MfT5hAkTXICic3344Ydb37594zb1qElHaWriCW/qee+999wsxNqHyqH961zqnB166KHWunVr++CDDyLypdqWe+65x5VV2x155JEu/wpS9PrRRx+NWF/50LF++OGHHM8JUBgRYABFjJ4OrCBhyZIldt5557mAQg/wk4EDB9q3337rLoDfffedPf30064pQfbu3WsdO3Z0F91PPvnE/v3vf7uLoWpN0tPTQ/v/8MMP7bfffrOPP/7YxowZ4ya90gVbgc4XX3xhN9xwg11//fW2evXqiHwpALnzzjstNTXVTd3fuXNn27hxY8wy6IKs415yySWuHFOnTnUX6uDFPh5tN3/+fFd+LSrHL7/8Evp8zZo1duqpp7oLuMqxcOFCu/baa0OPqNb5uPnmm61Pnz729ddfu+BMF/e86t+/v40YMcKd46ZNm9r27dvdd6HHGKj8KpvKr4kCgzQr8SuvvGKPP/642+6ZZ55x519BhPIYHbTpvcqSn/wBhYIm2gJQ8Hr06BEoVapUICUlJWJ5+OGHQ+von+z9998fer99+3aX9t5777n3nTt3DvTs2TPm/l988cVA48aNA5mZmaG0PXv2BMqWLRuYNWtWKA/16tUL7Nu3L7SOtmnfvn3ofUZGhsvXK6+84t6vXLnS5WHEiBGhdfbu3RuoU6dOYOTIke793Llz3TqbNm1y73v16hXo06dPRP4++eSTQGJiYmDXrl1xz9G9994b6NKlS+j9hRdeGBg8eHDo/YABAwINGjQIpKenx9y+Vq1agfvuuy/mZ8FypKamhtKUX6Up/+HlmDZtWiAnxx57bOCJJ55wr5cvX+62mz17dsx116xZ4777L774wr1X/qtVqxaYNGlSjscBCitqMIBC5PTTT3dV4+GLagzC6Y45KCUlxTUFrF+/3r2/8cYb3fN5NOX+3XffbZ999llo3a+++spVt6sGQ3fOWtTksXv37ogmBDUfhE/Pr2r/448/PvReT0CuWrVq6JhB4Q8cVJ8EPV9Id+qxKC+TJk0K5UOLalf0sMOVK1fG3EZ9Hf7xj3+4ppEgvdZ+tJ3ofKlJ5JBDDsmyvfKrmpkzzzzT9pfKFk41GHfddZcdffTRrllJ5VHZgzUYypfOm56jFIsefa1nK6n5Rt5++23XpHLZZZftd16BgkLPJKAQUcCQU5V49MVTVezBC+y5557rmgxmzJjhnjKsi6maBNS+r4ug+g28/PLLWfapPhvZ7T+7Y+aH8qJmFvW7iHbEEUfE3GbWrFmuCSS6U6cCDzVNnHXWWa4/RzzZfSbBoCr86QlqVor3PYVTcKHzrfOs70/HuvTSS0NNTzkdW6677jq7+uqr7bHHHnPNIyqn+tkARRU1GEAxo2BBTyh+6aWX3NOJg08aPuGEE+z777+3GjVquItg+FKxYsX9Pu7nn38eeq0+D+r/oDv6WJQX9RWJzoeW5OTkmNuoQ+QVV1yRpYZHacHOnqrdUb+MWIGBam7q16/vgpFYgkHW2rVrQ2nZze0RTv1Z1EFWnW9V26POueGdZ5WmgCy7jq/qw6HARf1E9MBH9csAijICDKAQUbW4niocvqSlpeV6ez2VePr06a4pRKMp3nnnndBFXp1B1eFTox10EVZThEZFqBYhusNmfowbN87efPNNN5pEtSYa0RLvIqnRFGq+UadOXcQV+Cjf8Tp56onLajZQ4KQRKuGLOk9quKo6umr7rVu3uqBjwYIFbr8vvviiLV++3O3ngQcesNGjR7uOlvps0aJF9sQTT4RqGdq1axfqvKlgQE9tzg2N1nnjjTdcWdT8c+WVV0bU8CiwUd51PpTX4Ll/9dVXQ+uoCUVByoABA9z+wpucgKKIAAMoRHTnqqGT4cspp5yS6+11968LlO7kNQJBFy31yRBVt2tkiJogLr74Yhd49OrVy/XBUD+O/aULs5ZmzZq5ESEaoREcwRJN+dMFfMWKFa7PRIsWLVxwpL4Isfzzn/90d/ex+k8oTcGBamzUN0SjR9QEo/4OahJ67rnnQk08usirVuepp55yfU00OkaBRpD6QKj2Rdvdfvvt9tBDD+Wq7Bpto1E2Gqar0SPqT6JamnCqmVCzyU033WRNmjSx3r17ZxmWq+9DzSo9e/bM1XGBwozHtQNAIaGaJQVMmq9EnWuBoowAAwAKQdOYmoFUw6L+G7E64gJFDU0kAFDANAFXvXr13Myho0aNKujsAF5QgwEAALyjBgMAAHhHgAEAALwjwAAAAN4RYAAAAO8IMAAAgHcEGAAAwDsCDAAA4B0BBgAA8I4AAwAAmG//D0O/0f43u5amAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar chart displayed showing the ensemble accuracy.\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Ensemble Accuracy Calculation Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "# You need to import any custom layers (like KerasLayer from PennyLane)\n",
    "# that were used when saving the model if saving in HDF5 format (.h5).\n",
    "# If you saved in the native Keras format (.keras), custom_objects might not be needed.\n",
    "\n",
    "# --- Import Quantum Libraries and Custom Layers ---\n",
    "# from pennylane.keras import KerasLayer # Example if using PennyLane\n",
    "# from pennylane.templates.tensor import DenseLayer # Example if your QNN used this\n",
    "# from pennylane.templates.layers import StronglyEntanglingLayers # Example if your QNN used this\n",
    "\n",
    "# If your QNN circuit was defined within a function, you might need to import that too,\n",
    "# or ensure your KerasLayer definition is available in the current scope.\n",
    "# Example:\n",
    "# def quantum_circuit_placeholder(...):\n",
    "#    ... your quantum circuit logic ...\n",
    "#    return measurements\n",
    "\n",
    "# --- Define Custom Objects Dictionary (NEEDED IF USING .h5 SAVE WITH CUSTOM LAYERS) ---\n",
    "# custom_objects = {\n",
    "#     \"KerasLayer\": KerasLayer, # Map the class name string to the actual class\n",
    "#     # Add any other custom layers/functions used in your model definition/save\n",
    "#     # \"DenseLayer\": DenseLayer,\n",
    "#     # \"StronglyEntanglingLayers\": StronglyEntanglingLayers,\n",
    "#     # \"quantum_circuit_placeholder\": quantum_circuit_placeholder, # If needed\n",
    "# }\n",
    "\n",
    "# --- Configuration ---\n",
    "# Update BASE_DATA_DIR to where your models are saved (based on your previous input)\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\"\n",
    "MODEL_MRI_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_only_ensemble_qnn.h5\")\n",
    "MODEL_MRI_PET_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_pet_ensemble_qnn.h5\")\n",
    "\n",
    "TASK_TYPE = 'classification' # Assuming classification\n",
    "# Accuracy calculation is typically for classification\n",
    "\n",
    "# --- Function to Load Models ---\n",
    "def load_trained_model(model_path, custom_objects=None):\n",
    "    \"\"\"Loads a trained Keras model from a file.\"\"\"\n",
    "    try:\n",
    "        # If you saved as .keras, you usually don't need custom_objects\n",
    "        if model_path.lower().endswith('.keras'):\n",
    "             model = tf.keras.models.load_model(model_path) # custom_objects is often not needed here\n",
    "        else: # Assuming .h5 format requires custom_objects for custom layers\n",
    "             model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {model_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        print(\"If using .h5 with custom layers (like KerasLayer for QNN), ensure 'custom_objects' dictionary is correct.\")\n",
    "        print(\"Consider saving models in the native Keras format (.keras) in the future.\")\n",
    "        return None\n",
    "\n",
    "# --- Function to Perform Ensemble Prediction ---\n",
    "def get_ensemble_predictions(model_mri, model_mri_pet, X_data_mri_format, X_data_mri_pet_format, task_type='classification'):\n",
    "    \"\"\"\n",
    "    Gets predictions from two models and averages them.\n",
    "\n",
    "    Args:\n",
    "        model_mri: The trained model expecting MRI-only shaped features (677).\n",
    "        model_mri_pet: The trained model expecting MRI-PET shaped features (263).\n",
    "        X_data_mri_format: NumPy array of features for prediction, shaped (N_samples, 677).\n",
    "        X_data_mri_pet_format: NumPy array of features for prediction, shaped (N_samples, 263).\n",
    "        task_type: 'classification' or 'regression'.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of ensemble predictions.\n",
    "    \"\"\"\n",
    "    if model_mri is None or model_mri_pet is None:\n",
    "        print(\"Error: One or both models failed to load. Cannot perform ensemble prediction.\")\n",
    "        return None\n",
    "\n",
    "    if X_data_mri_format.shape[0] != X_data_mri_pet_format.shape[0]:\n",
    "         print(\"Error: The number of samples in the two feature arrays must be the same for ensembling.\")\n",
    "         return None\n",
    "\n",
    "    # Optional shape checks based on loaded models' input shapes (more robust)\n",
    "    # Fallback to expected shapes if input_shape is None (can happen with old HDF5 saves or certain model configs)\n",
    "    expected_mri_shape = model_mri.input_shape[1] if (model_mri.input_shape and len(model_mri.input_shape) > 1) else 677\n",
    "    expected_mri_pet_shape = model_mri_pet.input_shape[1] if (model_mri_pet.input_shape and len(model_mri_pet.input_shape) > 1) else 263\n",
    "\n",
    "    if X_data_mri_format.shape[1] != expected_mri_shape:\n",
    "        print(f\"Error: MRI-only data features shape mismatch. Expected {expected_mri_shape}, got {X_data_mri_format.shape[1]}.\")\n",
    "        return None\n",
    "\n",
    "    if X_data_mri_pet_format.shape[1] != expected_mri_pet_shape:\n",
    "        print(f\"Error: MRI-PET data features shape mismatch. Expected {expected_mri_pet_shape}, got {X_data_mri_pet_format.shape[1]}.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    print(f\"\\nGetting ensemble predictions for {X_data_mri_format.shape[0]} samples...\")\n",
    "\n",
    "    # Get predictions from each model\n",
    "    # For classification, sigmoid output is already probabilities (0-1)\n",
    "    predictions_mri = model_mri.predict(X_data_mri_format)\n",
    "    predictions_mri_pet = model_mri_pet.predict(X_data_mri_pet_format)\n",
    "\n",
    "    # Combine predictions by averaging\n",
    "    if task_type == 'classification':\n",
    "        # Averaging probabilities for classification\n",
    "        ensemble_predictions = (predictions_mri + predictions_mri_pet) / 2.0\n",
    "        print(\"Averaged predicted probabilities.\")\n",
    "    else: # Regression\n",
    "        # Averaging predicted values for regression\n",
    "        ensemble_predictions = (predictions_mri + predictions_mri_pet) / 2.0\n",
    "        print(\"Averaged predicted values.\")\n",
    "\n",
    "    print(\"Ensemble prediction finished.\")\n",
    "    return ensemble_predictions\n",
    "\n",
    "# --- Main Execution for Ensemble Prediction and Accuracy ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting Ensemble Prediction and Accuracy Calculation ---\")\n",
    "\n",
    "    # Load the trained models\n",
    "    # Make sure custom_objects is defined if needed for .h5 files with custom layers\n",
    "    model_mri = load_trained_model(MODEL_MRI_SAVE_PATH_QNN) # , custom_objects=custom_objects\n",
    "    model_mri_pet = load_trained_model(MODEL_MRI_PET_SAVE_PATH_QNN) # , custom_objects=custom_objects\n",
    "\n",
    "    if model_mri is None or model_mri_pet is None:\n",
    "        print(\"Failed to load one or both models. Cannot calculate ensemble accuracy. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # --- PLACEHOLDER FOR UNIFIED ENSEMBLE TEST DATA ---\n",
    "    # This section is where you load and prepare the data for your ensemble test set.\n",
    "    # This test set must be DIFFERENT from the data used for training/validation\n",
    "    # of the individual models. Each sample in this test set must have:\n",
    "    # 1. Its 677-feature representation (X_ensemble_test_mri_format)\n",
    "    # 2. Its 263-feature representation (X_ensemble_test_mri_pet_format)\n",
    "    # 3. Its true label (y_ensemble_test)\n",
    "\n",
    "    print(\"\\n--- Loading Unified Ensemble Test Data (PLACEHOLDER) ---\")\n",
    "    print(\"Replace this section with your code to load and prepare your unified test data.\")\n",
    "    print(\"You need to produce three variables:\")\n",
    "    print(\"X_ensemble_test_mri_format (NumPy array, shape: (N_test_samples, 677))\")\n",
    "    print(\"X_ensemble_test_mri_pet_format (NumPy array, shape: (N_test_samples, 263))\")\n",
    "    print(\"y_ensemble_test (NumPy array of true labels, shape: (N_test_samples,))\")\n",
    "    print(\"\\nExample: Predicting for 50 samples as previously discussed.\")\n",
    "\n",
    "\n",
    "    # Example placeholder data (replace with your actual unified test data for 50 samples)\n",
    "    num_samples_ensemble_test = 13\n",
    "     # Set this to the number of samples in your ensemble test set\n",
    "\n",
    "    # These need to be derived from your actual UNIFIED test data, not random.\n",
    "    # Ensure these are DIFFERENT samples than those used to train Model 1 and Model 2.\n",
    "    X_ensemble_test_mri_format = np.random.rand(num_samples_ensemble_test, 677) # REPLACE THIS LINE\n",
    "    X_ensemble_test_mri_pet_format = np.random.rand(num_samples_ensemble_test, 263) # REPLACE THIS LINE\n",
    "    y_ensemble_test = np.random.randint(0, 2, num_samples_ensemble_test) # REPLACE THIS LINE with your actual true labels (ensure dtype is int)\n",
    "\n",
    "    print(f\"Placeholder unified ensemble test data created for {num_samples_ensemble_test} samples.\")\n",
    "    print(f\"  X_ensemble_test_mri_format shape: {X_ensemble_test_mri_format.shape}\")\n",
    "    print(f\"  X_ensemble_test_mri_pet_format shape: {X_ensemble_test_mri_pet_format.shape}\")\n",
    "    print(f\"  y_ensemble_test shape: {y_ensemble_test.shape}\")\n",
    "\n",
    "    if X_ensemble_test_mri_format.shape[0] != y_ensemble_test.shape[0]:\n",
    "         print(\"Error: Number of samples in features and labels do not match. Cannot calculate accuracy.\")\n",
    "         exit()\n",
    "    # --- END PLACEHOLDER ---\n",
    "\n",
    "\n",
    "    # Perform the ensemble prediction on the unified test data\n",
    "    ensemble_predictions_prob = get_ensemble_predictions(\n",
    "        model_mri,\n",
    "        model_mri_pet,\n",
    "        X_ensemble_test_mri_format, # Feed 677 features to Model 1\n",
    "        X_ensemble_test_mri_pet_format,  # Feed 263 features to Model 2\n",
    "        TASK_TYPE\n",
    "    )\n",
    "\n",
    "    if ensemble_predictions_prob is not None:\n",
    "        # Calculate ensemble accuracy (for classification)\n",
    "        if TASK_TYPE == 'classification':\n",
    "            # Convert probabilities to binary labels (e.g., threshold at 0.5)\n",
    "            ensemble_predicted_labels = (ensemble_predictions_prob > 0.5).astype(int)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            # Ensure y_ensemble_test is the same shape as ensemble_predicted_labels for accuracy_score\n",
    "            # ensemble_predicted_labels is (N_samples, 1), y_ensemble_test is (N_samples,)\n",
    "            ensemble_accuracy = accuracy_score(y_ensemble_test, ensemble_predicted_labels.flatten()) # Use flatten for consistency\n",
    "\n",
    "            # print(f\"\\n--- Ensemble Accuracy on 50 Samples ---\")\n",
    "            # print(f\"Ensemble Test Accuracy: {ensemble_accuracy:.4f}\")\n",
    "\n",
    "            # --- Visual Metric: Bar Chart for Accuracy ---\n",
    "            print(\"\\n--- Visual Metric: Ensemble Accuracy Bar Chart ---\")\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.bar(['Ensemble Accuracy'], [ensemble_accuracy], color='skyblue')\n",
    "            plt.ylim(0, 1.05) # Set y-axis limit for accuracy (0 to slightly above 1)\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Ensemble Prediction Accuracy')\n",
    "            # Add the accuracy value text on the bar\n",
    "            plt.text('Ensemble Accuracy', ensemble_accuracy, f'{ensemble_accuracy:.4f}', ha='center', va='bottom')\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "            print(\"Bar chart displayed showing the ensemble accuracy.\")\n",
    "\n",
    "\n",
    "            print(\"-------------------------------------------------\")\n",
    "\n",
    "        else: # Regression - you'd typically evaluate R2, MAE, MSE instead of accuracy\n",
    "             print(\"\\n--- Ensemble Regression Evaluation ---\")\n",
    "             print(\"For regression, you would calculate metrics like MAE, MSE, R2 here using y_ensemble_test and ensemble_predictions_prob\")\n",
    "             # from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "             # mae = mean_absolute_error(y_ensemble_test, ensemble_predictions_prob)\n",
    "             # mse = mean_squared_error(y_ensemble_test, ensemble_predictions_prob)\n",
    "             # r2 = r2_score(y_ensemble_test, ensemble_predictions_prob)\n",
    "             # print(f\"Ensemble Test MAE: {mae:.4f}\")\n",
    "             # print(f\"Ensemble Test MSE: {mse:.4f}\")\n",
    "             # print(f\"Ensemble Test R2: {r2:.4f}\")\n",
    "             print(\"------------------------------------\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Ensemble Accuracy Calculation Script Finished ---\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59b13a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Ensemble Prediction and Accuracy Calculation ---\n",
      "Successfully loaded model from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\model_mri_only_ensemble_qnn.h5\n",
      "Successfully loaded model from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\model_mri_pet_ensemble_qnn.h5\n",
      "\n",
      "--- Loading Unified Ensemble Test Data (PLACEHOLDER) ---\n",
      "Replace this section with your code to load and prepare your unified test data.\n",
      "You need to produce three variables:\n",
      "X_ensemble_test_mri_format (NumPy array, shape: (N_test_samples, 677))\n",
      "X_ensemble_test_mri_pet_format (NumPy array, shape: (N_test_samples, 263))\n",
      "y_ensemble_test (NumPy array of true labels, shape: (N_test_samples,))\n",
      "\n",
      "Example: Predicting for 13 samples as specified in your last output.\n",
      "Placeholder unified ensemble test data created for 13 samples.\n",
      "  X_ensemble_test_mri_format shape: (13, 677)\n",
      "  X_ensemble_test_mri_pet_format shape: (13, 263)\n",
      "  y_ensemble_test shape: (13,)\n",
      "\n",
      "Getting ensemble predictions for 13 samples...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Averaged predicted probabilities.\n",
      "Ensemble prediction finished.\n",
      "\n",
      "--- Ensemble Accuracy on 13 Samples ---\n",
      "Ensemble Test Accuracy: 0.5385\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Visual Metric: Ensemble Accuracy Bar Chart ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar chart displayed showing the ensemble accuracy.\n",
      "\n",
      "--- Ensemble Prediction and Accuracy Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt # Import for plotting\n",
    "\n",
    "# You need to import any custom layers (like KerasLayer from PennyLane)\n",
    "# that were used when saving the model if saving in HDF5 format (.h5).\n",
    "# If you saved in the native Keras format (.keras), custom_objects might not be needed.\n",
    "\n",
    "# --- Import Quantum Libraries and Custom Layers ---\n",
    "# from pennylane.keras import KerasLayer # Example if using PennyLane\n",
    "# from pennylane.templates.tensor import DenseLayer # Example if your QNN used this\n",
    "# from pennylane.templates.layers import StronglyEntanglingLayers # Example if your QNN used this\n",
    "\n",
    "# If your QNN circuit was defined within a function, you might need to import that too,\n",
    "# or ensure your KerasLayer definition is available in the current scope.\n",
    "# Example:\n",
    "# def quantum_circuit_placeholder(...):\n",
    "#    ... your quantum circuit logic ...\n",
    "#    return measurements\n",
    "\n",
    "# --- Define Custom Objects Dictionary (NEEDED IF USING .h5 SAVE WITH CUSTOM LAYERS) ---\n",
    "# custom_objects = {\n",
    "#     \"KerasLayer\": KerasLayer, # Map the class name string to the actual class\n",
    "#     # Add any other custom layers/functions used in your model definition/save\n",
    "#     # \"DenseLayer\": DenseLayer,\n",
    "#     # \"StronglyEntanglingLayers\": StronglyEntanglingLayers,\n",
    "#     # \"quantum_circuit_placeholder\": quantum_circuit_placeholder, # If needed\n",
    "# }\n",
    "\n",
    "# --- Configuration ---\n",
    "# Update BASE_DATA_DIR to where your models are saved (based on your previous input)\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\" # Assuming models are here now\n",
    "MODEL_MRI_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_only_ensemble_qnn.h5\")\n",
    "MODEL_MRI_PET_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_pet_ensemble_qnn.h5\")\n",
    "\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression'\n",
    "# Accuracy calculation is typically for classification\n",
    "\n",
    "# --- Function to Load Models ---\n",
    "def load_trained_model(model_path, custom_objects=None):\n",
    "    \"\"\"Loads a trained Keras model from a file.\"\"\"\n",
    "    try:\n",
    "        # If you saved as .keras, you usually don't need custom_objects\n",
    "        if model_path.lower().endswith('.keras'):\n",
    "             model = tf.keras.models.load_model(model_path) # custom_objects is often not needed here\n",
    "        else: # Assuming .h5 format requires custom_objects for custom layers\n",
    "             model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {model_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        print(\"If using .h5 with custom layers (like KerasLayer for QNN), ensure 'custom_objects' dictionary is correct.\")\n",
    "        print(\"Consider saving models in the native Keras format (.keras) in the future.\")\n",
    "        return None\n",
    "\n",
    "# --- Function to Perform Ensemble Prediction ---\n",
    "def get_ensemble_predictions(model_mri, model_mri_pet, X_data_mri_format, X_data_mri_pet_format, task_type='classification'):\n",
    "    \"\"\"\n",
    "    Gets predictions from two models and averages them.\n",
    "\n",
    "    Args:\n",
    "        model_mri: The trained model expecting MRI-only shaped features (677).\n",
    "        model_mri_pet: The trained model expecting MRI-PET shaped features (263).\n",
    "        X_data_mri_format: NumPy array of features for prediction, shaped (N_samples, 677).\n",
    "        X_data_mri_pet_format: NumPy array of features for prediction, shaped (N_samples, 263).\n",
    "        task_type: 'classification' or 'regression'.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of ensemble predictions.\n",
    "    \"\"\"\n",
    "    if model_mri is None or model_mri_pet is None:\n",
    "        print(\"Error: One or both models failed to load. Cannot perform ensemble prediction.\")\n",
    "        return None\n",
    "\n",
    "    if X_data_mri_format.shape[0] != X_data_mri_pet_format.shape[0]:\n",
    "         print(\"Error: The number of samples in the two feature arrays must be the same for ensembling.\")\n",
    "         return None\n",
    "\n",
    "    # Optional shape checks based on loaded models' input shapes (more robust)\n",
    "    # Fallback to expected shapes if input_shape is None (can happen with old HDF5 saves or certain model configs)\n",
    "    expected_mri_shape = model_mri.input_shape[1] if (model_mri.input_shape and len(model_mri.input_shape) > 1) else 677\n",
    "    expected_mri_pet_shape = model_mri_pet.input_shape[1] if (model_mri_pet.input_shape and len(model_mri_pet.input_shape) > 1) else 263\n",
    "\n",
    "    if X_data_mri_format.shape[1] != expected_mri_shape:\n",
    "        print(f\"Error: MRI-only data features shape mismatch. Expected {expected_mri_shape}, got {X_data_mri_format.shape[1]}.\")\n",
    "        return None\n",
    "\n",
    "    if X_data_mri_pet_format.shape[1] != expected_mri_pet_shape:\n",
    "        print(f\"Error: MRI-PET data features shape mismatch. Expected {expected_mri_pet_shape}, got {X_data_mri_pet_format.shape[1]}.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    print(f\"\\nGetting ensemble predictions for {X_data_mri_format.shape[0]} samples...\")\n",
    "\n",
    "    # Get predictions from each model\n",
    "    # For classification, sigmoid output is already probabilities (0-1)\n",
    "    predictions_mri = model_mri.predict(X_data_mri_format)\n",
    "    predictions_mri_pet = model_mri_pet.predict(X_data_mri_pet_format)\n",
    "\n",
    "    # Combine predictions by averaging\n",
    "    if task_type == 'classification':\n",
    "        # Averaging probabilities for classification\n",
    "        ensemble_predictions = (predictions_mri + predictions_mri_pet) / 2.0\n",
    "        print(\"Averaged predicted probabilities.\")\n",
    "    else: # Regression\n",
    "        # Averaging predicted values for regression\n",
    "        ensemble_predictions = (predictions_mri + predictions_mri_pet) / 2.0\n",
    "        print(\"Averaged predicted values.\")\n",
    "\n",
    "    print(\"Ensemble prediction finished.\")\n",
    "    return ensemble_predictions\n",
    "\n",
    "# --- Main Execution for Ensemble Prediction and Accuracy ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting Ensemble Prediction and Accuracy Calculation ---\")\n",
    "\n",
    "    # Load the trained models\n",
    "    # Make sure custom_objects is defined if needed for .h5 files with custom layers\n",
    "    model_mri = load_trained_model(MODEL_MRI_SAVE_PATH_QNN) # , custom_objects=custom_objects\n",
    "    model_mri_pet = load_trained_model(MODEL_MRI_PET_SAVE_PATH_QNN) # , custom_objects=custom_objects\n",
    "\n",
    "    if model_mri is None or model_mri_pet is None:\n",
    "        print(\"Failed to load one or both models. Cannot calculate ensemble accuracy. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # --- PLACEHOLDER FOR UNIFIED ENSEMBLE TEST DATA ---\n",
    "    # This section is where you load and prepare the data for your ensemble test set.\n",
    "    # This test set must be DIFFERENT from the data used for training/validation\n",
    "    # of the individual models. Each sample in this test set must have:\n",
    "    # 1. Its 677-feature representation (X_ensemble_test_mri_format)\n",
    "    # 2. Its 263-feature representation (X_ensemble_test_mri_pet_format)\n",
    "    # 3. Its true label (y_ensemble_test)\n",
    "\n",
    "    print(\"\\n--- Loading Unified Ensemble Test Data (PLACEHOLDER) ---\")\n",
    "    print(\"Replace this section with your code to load and prepare your unified test data.\")\n",
    "    print(\"You need to produce three variables:\")\n",
    "    print(\"X_ensemble_test_mri_format (NumPy array, shape: (N_test_samples, 677))\")\n",
    "    print(\"X_ensemble_test_mri_pet_format (NumPy array, shape: (N_test_samples, 263))\")\n",
    "    print(\"y_ensemble_test (NumPy array of true labels, shape: (N_test_samples,))\")\n",
    "    print(\"\\nExample: Predicting for 13 samples as specified in your last output.\")\n",
    "\n",
    "\n",
    "    # Example placeholder data (replace with your actual unified test data for the correct number of samples)\n",
    "    num_samples_ensemble_test = 13 # Set this to the number of samples in your ensemble test set\n",
    "\n",
    "    # These need to be derived from your actual UNIFIED test data, not random.\n",
    "    # Ensure these are DIFFERENT samples than those used to train Model 1 and Model 2.\n",
    "    # If your actual data loading yields slightly different shapes, adjust the expected shapes in get_ensemble_predictions accordingly,\n",
    "    # or fix your data loading to match the expected shapes.\n",
    "    X_ensemble_test_mri_format = np.random.rand(num_samples_ensemble_test, 677) # REPLACE THIS LINE\n",
    "    X_ensemble_test_mri_pet_format = np.random.rand(num_samples_ensemble_test, 263) # REPLACE THIS LINE\n",
    "    y_ensemble_test = np.random.randint(0, 2, num_samples_ensemble_test) # REPLACE THIS LINE with your actual true labels (ensure dtype is int)\n",
    "\n",
    "\n",
    "    print(f\"Placeholder unified ensemble test data created for {num_samples_ensemble_test} samples.\")\n",
    "    print(f\"  X_ensemble_test_mri_format shape: {X_ensemble_test_mri_format.shape}\")\n",
    "    print(f\"  X_ensemble_test_mri_pet_format shape: {X_ensemble_test_mri_pet_format.shape}\")\n",
    "    print(f\"  y_ensemble_test shape: {y_ensemble_test.shape}\")\n",
    "\n",
    "    if X_ensemble_test_mri_format.shape[0] != y_ensemble_test.shape[0]:\n",
    "         print(\"Error: Number of samples in features and labels do not match. Cannot calculate accuracy.\")\n",
    "         exit()\n",
    "    # --- END PLACEHOLDER ---\n",
    "\n",
    "\n",
    "    # Perform the ensemble prediction on the unified test data\n",
    "    ensemble_predictions_prob = get_ensemble_predictions(\n",
    "        model_mri,\n",
    "        model_mri_pet,\n",
    "        X_ensemble_test_mri_format, # Feed 677 features to Model 1\n",
    "        X_ensemble_test_mri_pet_format,  # Feed 263 features to Model 2\n",
    "        TASK_TYPE\n",
    "    )\n",
    "\n",
    "    if ensemble_predictions_prob is not None:\n",
    "        # Calculate ensemble accuracy (for classification)\n",
    "        if TASK_TYPE == 'classification':\n",
    "            # Convert probabilities to binary labels (e.g., threshold at 0.5)\n",
    "            ensemble_predicted_labels = (ensemble_predictions_prob > 0.5).astype(int)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            # Ensure y_ensemble_test is the same shape as ensemble_predicted_labels for accuracy_score\n",
    "            # ensemble_predicted_labels is (N_samples, 1), y_ensemble_test is (N_samples,)\n",
    "            ensemble_accuracy = accuracy_score(y_ensemble_test, ensemble_predicted_labels.flatten()) # Use flatten for consistency\n",
    "\n",
    "            print(f\"\\n--- Ensemble Accuracy on {num_samples_ensemble_test} Samples ---\")\n",
    "            print(f\"Ensemble Test Accuracy: {ensemble_accuracy:.4f}\")\n",
    "            print(\"-------------------------------------------------\")\n",
    "\n",
    "            # --- Visual Metric: Bar Chart for Accuracy ---\n",
    "            print(\"\\n--- Visual Metric: Ensemble Accuracy Bar Chart ---\")\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.bar(['Ensemble Accuracy'], [ensemble_accuracy], color='skyblue')\n",
    "            plt.ylim(0, 1.05) # Set y-axis limit for accuracy (0 to slightly above 1)\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Ensemble Prediction Accuracy')\n",
    "            # Add the accuracy value text on the bar\n",
    "            plt.text('Ensemble Accuracy', ensemble_accuracy, f'{ensemble_accuracy:.4f}', ha='center', va='bottom')\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "            print(\"Bar chart displayed showing the ensemble accuracy.\")\n",
    "\n",
    "\n",
    "        else: # Regression - you'd typically evaluate R2, MAE, MSE instead of accuracy\n",
    "             print(\"\\n--- Ensemble Regression Evaluation ---\")\n",
    "             print(\"For regression, you would calculate metrics like MAE, MSE, R2 here using y_ensemble_test and ensemble_predictions_prob\")\n",
    "            #  from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "            #  mae = mean_absolute_error(y_ensemble_test, ensemble_predictions_prob)\n",
    "            #  mse = mean_squared_error(y_ensemble_test, ensemble_predictions_prob)\n",
    "            #  r2 = r2_score(y_ensemble_test, ensemble_predictions_prob)\n",
    "            #  print(f\"Ensemble Test MAE: {mae:.4f}\")\n",
    "            #  print(f\"Ensemble Test MSE: {mse:.4f}\")\n",
    "            #  print(f\"Ensemble Test R2: {r2:.4f}\")\n",
    "             print(\"------------------------------------\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Ensemble Prediction and Accuracy Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c3938a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Ensemble Prediction and Accuracy Calculation ---\n",
      "Successfully loaded model from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\model_mri_only_ensemble_qnn.h5\n",
      "Successfully loaded model from C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\\model_mri_pet_ensemble_qnn.h5\n",
      "\n",
      "--- Loading Unified Ensemble Test Data (PLACEHOLDER) ---\n",
      "Replace this section with your code to load and prepare your unified test data.\n",
      "You need to produce three variables:\n",
      "X_ensemble_test_mri_format (NumPy array, shape: (N_test_samples, 677))\n",
      "X_ensemble_test_mri_pet_format (NumPy array, shape: (N_test_samples, 263))\n",
      "y_ensemble_test (NumPy array of true labels, shape: (N_test_samples,))\n",
      "\n",
      "Example: Predicting for 13 samples as specified in your last output.\n",
      "Placeholder unified ensemble test data created for 18 samples.\n",
      "  X_ensemble_test_mri_format shape: (18, 677)\n",
      "  X_ensemble_test_mri_pet_format shape: (18, 263)\n",
      "  y_ensemble_test shape: (18,)\n",
      "\n",
      "Getting ensemble predictions for 18 samples...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Averaged predicted probabilities.\n",
      "Ensemble prediction finished.\n",
      "\n",
      "--- Ensemble Accuracy on 18 Samples ---\n",
      "Ensemble Test Accuracy: 0.3889\n",
      "-------------------------------------------------\n",
      "\n",
      "--- Visual Metric: Ensemble Accuracy Bar Chart ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar chart displayed showing the ensemble accuracy.\n",
      "\n",
      "--- Ensemble Prediction and Accuracy Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt # Import for plotting\n",
    "\n",
    "# You need to import any custom layers (like KerasLayer from PennyLane)\n",
    "# that were used when saving the model if saving in HDF5 format (.h5).\n",
    "# If you saved in the native Keras format (.keras), custom_objects might not be needed.\n",
    "\n",
    "# --- Import Quantum Libraries and Custom Layers ---\n",
    "# from pennylane.keras import KerasLayer # Example if using PennyLane\n",
    "# from pennylane.templates.tensor import DenseLayer # Example if your QNN used this\n",
    "# from pennylane.templates.layers import StronglyEntanglingLayers # Example if your QNN used this\n",
    "\n",
    "# If your QNN circuit was defined within a function, you might need to import that too,\n",
    "# or ensure your KerasLayer definition is available in the current scope.\n",
    "# Example:\n",
    "# def quantum_circuit_placeholder(...):\n",
    "#    ... your quantum circuit logic ...\n",
    "#    return measurements\n",
    "\n",
    "# --- Define Custom Objects Dictionary (NEEDED IF USING .h5 SAVE WITH CUSTOM LAYERS) ---\n",
    "# custom_objects = {\n",
    "#     \"KerasLayer\": KerasLayer, # Map the class name string to the actual class\n",
    "#     # Add any other custom layers/functions used in your model definition/save\n",
    "#     # \"DenseLayer\": DenseLayer,\n",
    "#     # \"StronglyEntanglingLayers\": StronglyEntanglingLayers,\n",
    "#     # \"quantum_circuit_placeholder\": quantum_circuit_placeholder, # If needed\n",
    "# }\n",
    "\n",
    "# --- Configuration ---\n",
    "# Update BASE_DATA_DIR to where your models are saved (based on your previous input)\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\Desktop\\QML_Project\\src\" # Assuming models are here now\n",
    "MODEL_MRI_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_only_ensemble_qnn.h5\")\n",
    "MODEL_MRI_PET_SAVE_PATH_QNN = os.path.join(BASE_DATA_DIR, \"model_mri_pet_ensemble_qnn.h5\")\n",
    "\n",
    "TASK_TYPE = 'classification' # Assuming classification\n",
    "# Accuracy calculation is typically for classification\n",
    "\n",
    "# --- Function to Load Models ---\n",
    "def load_trained_model(model_path, custom_objects=None):\n",
    "    \"\"\"Loads a trained Keras model from a file.\"\"\"\n",
    "    try:\n",
    "        # If you saved as .keras, you usually don't need custom_objects\n",
    "        if model_path.lower().endswith('.keras'):\n",
    "             model = tf.keras.models.load_model(model_path) # custom_objects is often not needed here\n",
    "        else: # Assuming .h5 format requires custom_objects for custom layers\n",
    "             model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {model_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        print(\"If using .h5 with custom layers (like KerasLayer for QNN), ensure 'custom_objects' dictionary is correct.\")\n",
    "        print(\"Consider saving models in the native Keras format (.keras) in the future.\")\n",
    "        return None\n",
    "\n",
    "# --- Function to Perform Ensemble Prediction ---\n",
    "def get_ensemble_predictions(model_mri, model_mri_pet, X_data_mri_format, X_data_mri_pet_format, task_type='classification'):\n",
    "    \"\"\"\n",
    "    Gets predictions from two models and averages them.\n",
    "\n",
    "    Args:\n",
    "        model_mri: The trained model expecting MRI-only shaped features (677).\n",
    "        model_mri_pet: The trained model expecting MRI-PET shaped features (263).\n",
    "        X_data_mri_format: NumPy array of features for prediction, shaped (N_samples, 677).\n",
    "        X_data_mri_pet_format: NumPy array of features for prediction, shaped (N_samples, 263).\n",
    "        task_type: 'classification' or 'regression'.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of ensemble predictions.\n",
    "    \"\"\"\n",
    "    if model_mri is None or model_mri_pet is None:\n",
    "        print(\"Error: One or both models failed to load. Cannot perform ensemble prediction.\")\n",
    "        return None\n",
    "\n",
    "    if X_data_mri_format.shape[0] != X_data_mri_pet_format.shape[0]:\n",
    "         print(\"Error: The number of samples in the two feature arrays must be the same for ensembling.\")\n",
    "         return None\n",
    "\n",
    "    # Optional shape checks based on loaded models' input shapes (more robust)\n",
    "    # Fallback to expected shapes if input_shape is None (can happen with old HDF5 saves or certain model configs)\n",
    "    expected_mri_shape = model_mri.input_shape[1] if (model_mri.input_shape and len(model_mri.input_shape) > 1) else 677\n",
    "    expected_mri_pet_shape = model_mri_pet.input_shape[1] if (model_mri_pet.input_shape and len(model_mri_pet.input_shape) > 1) else 263\n",
    "\n",
    "    if X_data_mri_format.shape[1] != expected_mri_shape:\n",
    "        print(f\"Error: MRI-only data features shape mismatch. Expected {expected_mri_shape}, got {X_data_mri_format.shape[1]}.\")\n",
    "        return None\n",
    "\n",
    "    if X_data_mri_pet_format.shape[1] != expected_mri_pet_shape:\n",
    "        print(f\"Error: MRI-PET data features shape mismatch. Expected {expected_mri_pet_shape}, got {X_data_mri_pet_format.shape[1]}.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    print(f\"\\nGetting ensemble predictions for {X_data_mri_format.shape[0]} samples...\")\n",
    "\n",
    "    # Get predictions from each model\n",
    "    # For classification, sigmoid output is already probabilities (0-1)\n",
    "    predictions_mri = model_mri.predict(X_data_mri_format)\n",
    "    predictions_mri_pet = model_mri_pet.predict(X_data_mri_pet_format)\n",
    "\n",
    "    # Combine predictions by averaging\n",
    "    if task_type == 'classification':\n",
    "        # Averaging probabilities for classification\n",
    "        ensemble_predictions = (predictions_mri + predictions_mri_pet) / 2.0\n",
    "        print(\"Averaged predicted probabilities.\")\n",
    "    else: # Regression\n",
    "        # Averaging predicted values for regression\n",
    "        ensemble_predictions = (predictions_mri + predictions_mri_pet) / 2.0\n",
    "        print(\"Averaged predicted values.\")\n",
    "\n",
    "    print(\"Ensemble prediction finished.\")\n",
    "    return ensemble_predictions\n",
    "\n",
    "# --- Main Execution for Ensemble Prediction and Accuracy ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting Ensemble Prediction and Accuracy Calculation ---\")\n",
    "\n",
    "    # Load the trained models\n",
    "    # Make sure custom_objects is defined if needed for .h5 files with custom layers\n",
    "    model_mri = load_trained_model(MODEL_MRI_SAVE_PATH_QNN) # , custom_objects=custom_objects\n",
    "    model_mri_pet = load_trained_model(MODEL_MRI_PET_SAVE_PATH_QNN) # , custom_objects=custom_objects\n",
    "\n",
    "    if model_mri is None or model_mri_pet is None:\n",
    "        print(\"Failed to load one or both models. Cannot calculate ensemble accuracy. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # --- PLACEHOLDER FOR UNIFIED ENSEMBLE TEST DATA ---\n",
    "    # This section is where you load and prepare the data for your ensemble test set.\n",
    "    # This test set must be DIFFERENT from the data used for training/validation\n",
    "    # of the individual models. Each sample in this test set must have:\n",
    "    # 1. Its 677-feature representation (X_ensemble_test_mri_format)\n",
    "    # 2. Its 263-feature representation (X_ensemble_test_mri_pet_format)\n",
    "    # 3. Its true label (y_ensemble_test)\n",
    "\n",
    "    print(\"\\n--- Loading Unified Ensemble Test Data (PLACEHOLDER) ---\")\n",
    "    print(\"Replace this section with your code to load and prepare your unified test data.\")\n",
    "    print(\"You need to produce three variables:\")\n",
    "    print(\"X_ensemble_test_mri_format (NumPy array, shape: (N_test_samples, 677))\")\n",
    "    print(\"X_ensemble_test_mri_pet_format (NumPy array, shape: (N_test_samples, 263))\")\n",
    "    print(\"y_ensemble_test (NumPy array of true labels, shape: (N_test_samples,))\")\n",
    "    print(\"\\nExample: Predicting for 13 samples as specified in your last output.\")\n",
    "\n",
    "\n",
    "    # Example placeholder data (replace with your actual unified test data for the correct number of samples)\n",
    "    num_samples_ensemble_test = 18 # Set this to the number of samples in your ensemble test set\n",
    "\n",
    "    # These need to be derived from your actual UNIFIED test data, not random.\n",
    "    # Ensure these are DIFFERENT samples than those used to train Model 1 and Model 2.\n",
    "    # If your actual data loading yields slightly different shapes, adjust the expected shapes in get_ensemble_predictions accordingly,\n",
    "    # or fix your data loading to match the expected shapes.\n",
    "    X_ensemble_test_mri_format = np.random.rand(num_samples_ensemble_test, 677) # REPLACE THIS LINE\n",
    "    X_ensemble_test_mri_pet_format = np.random.rand(num_samples_ensemble_test, 263) # REPLACE THIS LINE\n",
    "    y_ensemble_test = np.random.randint(0, 2, num_samples_ensemble_test) # REPLACE THIS LINE with your actual true labels (ensure dtype is int)\n",
    "\n",
    "\n",
    "    print(f\"Placeholder unified ensemble test data created for {num_samples_ensemble_test} samples.\")\n",
    "    print(f\"  X_ensemble_test_mri_format shape: {X_ensemble_test_mri_format.shape}\")\n",
    "    print(f\"  X_ensemble_test_mri_pet_format shape: {X_ensemble_test_mri_pet_format.shape}\")\n",
    "    print(f\"  y_ensemble_test shape: {y_ensemble_test.shape}\")\n",
    "\n",
    "    if X_ensemble_test_mri_format.shape[0] != y_ensemble_test.shape[0]:\n",
    "         print(\"Error: Number of samples in features and labels do not match. Cannot calculate accuracy.\")\n",
    "         exit()\n",
    "    # --- END PLACEHOLDER ---\n",
    "\n",
    "\n",
    "    # Perform the ensemble prediction on the unified test data\n",
    "    ensemble_predictions_prob = get_ensemble_predictions(\n",
    "        model_mri,\n",
    "        model_mri_pet,\n",
    "        X_ensemble_test_mri_format, # Feed 677 features to Model 1\n",
    "        X_ensemble_test_mri_pet_format,  # Feed 263 features to Model 2\n",
    "        TASK_TYPE\n",
    "    )\n",
    "\n",
    "    if ensemble_predictions_prob is not None:\n",
    "        # Calculate ensemble accuracy (for classification)\n",
    "        if TASK_TYPE == 'classification':\n",
    "            # Convert probabilities to binary labels (e.g., threshold at 0.5)\n",
    "            ensemble_predicted_labels = (ensemble_predictions_prob > 0.5).astype(int)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            # Ensure y_ensemble_test is the same shape as ensemble_predicted_labels for accuracy_score\n",
    "            # ensemble_predicted_labels is (N_samples, 1), y_ensemble_test is (N_samples,)\n",
    "            ensemble_accuracy = accuracy_score(y_ensemble_test, ensemble_predicted_labels.flatten()) # Use flatten for consistency\n",
    "\n",
    "            print(f\"\\n--- Ensemble Accuracy on {num_samples_ensemble_test} Samples ---\")\n",
    "            print(f\"Ensemble Test Accuracy: {ensemble_accuracy:.4f}\")\n",
    "            print(\"-------------------------------------------------\")\n",
    "\n",
    "            # --- Visual Metric: Bar Chart for Accuracy ---\n",
    "            print(\"\\n--- Visual Metric: Ensemble Accuracy Bar Chart ---\")\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            # Create a bar at position 0, with height equal to accuracy\n",
    "            plt.bar(0, ensemble_accuracy, color='skyblue', width=0.5)\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Ensemble Prediction Accuracy')\n",
    "            plt.ylim(0, 1.05) # Set y-axis limit for accuracy (0 to slightly above 1)\n",
    "            plt.xticks([]) # Hide x-axis ticks as there's only one bar\n",
    "            # Add the accuracy value text on the bar\n",
    "            plt.text(0, ensemble_accuracy, f'{ensemble_accuracy:.4f}', ha='center', va='bottom')\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
    "            plt.show()\n",
    "            print(\"Bar chart displayed showing the ensemble accuracy.\")\n",
    "\n",
    "\n",
    "        else: # Regression - you'd typically evaluate R2, MAE, MSE instead of accuracy\n",
    "             print(\"\\n--- Ensemble Regression Evaluation ---\")\n",
    "             print(\"For regression, you would calculate metrics like MAE, MSE, R2 here using y_ensemble_test and ensemble_predictions_prob\")\n",
    "             # from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "             # mae = mean_absolute_error(y_ensemble_test, ensemble_predictions_prob)\n",
    "             # mse = mean_squared_error(y_ensemble_test, ensemble_predictions_prob)\n",
    "             # r2 = r2_score(y_ensemble_test, ensemble_predictions_prob)\n",
    "             # print(f\"Ensemble Test MAE: {mae:.4f}\")\n",
    "             # print(f\"Ensemble Test MSE: {mse:.4f}\")\n",
    "             # print(f\"Ensemble Test R2: {r2:.4f}\")\n",
    "             print(\"------------------------------------\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Ensemble Prediction and Accuracy Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42180730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Model 1 (Classical MLP) on mri_only_normalized.csv (677 Features) ---\n",
      "Error: Data file not found at C:\\Users\\ishsh\\OneDrive\\OneDrive\\Desktop\\QML_Project\\src\\mri_only_normalized.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 280\u001b[0m\n\u001b[0;32m    277\u001b[0m     exit()\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# Ensure input shape matches expectation or use loaded shape\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m input_shape_mri \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_mri\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    281\u001b[0m expected_shape_mri \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m677\u001b[39m \u001b[38;5;66;03m# Based on previous runs\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_shape_mri \u001b[38;5;241m!=\u001b[39m expected_shape_mri:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler # Not using StandardScaler if data is already normalized\n",
    "# Import upsampling library - you'll need to install imbalanced-learn: pip install imbalanced-learn\n",
    "from imblearn.over_sampling import RandomOverSampler # Or SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os # Import os for path joining\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "from sklearn.metrics import accuracy_score # To evaluate the combined prediction (used later with prediction script)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use os.path.join for better path handling across different OS\n",
    "# Using the directory you provided for model saves\n",
    "BASE_DATA_DIR = r\"C:\\Users\\ishsh\\OneDrive\\OneDrive\\Desktop\\QML_Project\\src\" # Adjusted BASE_DATA_DIR based on common user path\n",
    "\n",
    "MRI_FILE = \"mri_only_normalized.csv\"\n",
    "MRI_PET_FILE = \"mri_pet_normalized.csv\"\n",
    "\n",
    "# Assuming the data files are in the same directory as the script/models now\n",
    "# Adjust these paths if your data files are elsewhere\n",
    "MRI_DATA_PATH = os.path.join(BASE_DATA_DIR, MRI_FILE)\n",
    "MRI_PET_DATA_PATH = os.path.join(BASE_DATA_DIR, MRI_PET_FILE)\n",
    "\n",
    "# Model save paths (using classical names)\n",
    "MODEL_MRI_SAVE_PATH_CLASSICAL = os.path.join(BASE_DATA_DIR, \"model_mri_only_ensemble_classical.h5\")\n",
    "MODEL_MRI_PET_SAVE_PATH_CLASSICAL = os.path.join(BASE_DATA_DIR, \"model_mri_pet_ensemble_classical.h5\")\n",
    "\n",
    "\n",
    "TEST_SIZE_1 = 0.02 # Test size for MRI-only split\n",
    "TEST_SIZE_2 = 0.05 # Test size for MRI-PET split\n",
    "RANDOM_STATE = 42 # Random state for data splitting\n",
    "RANDOM_STATE_MODEL_1 = 42 # Random state for Model 1 initialization/training\n",
    "RANDOM_STATE_MODEL_2 = 123 # Different random state for Model 2\n",
    "\n",
    "TASK_TYPE = 'classification' # 'classification' or 'regression'\n",
    "\n",
    "# Training Hyperparameters (can tune these for each model)\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 32\n",
    "MAX_EPOCHS = 300\n",
    "EARLY_STOPPING_PATIENCE = 50\n",
    "\n",
    "# --- Function to Create a Standard Classical MLP Model ---\n",
    "# This provides a stable classical architecture\n",
    "def create_classical_mlp_model(input_shape):\n",
    "    \"\"\"\n",
    "    Function to create a standard Multi-Layer Perceptron (MLP) model.\n",
    "    This is a stable classical architecture.\n",
    "\n",
    "    Args:\n",
    "        input_shape (int): The number of classical input features.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.models.Sequential: The compiled Keras model.\n",
    "    \"\"\"\n",
    "    print(f\"Creating a classical MLP model with input shape {input_shape}\")\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,), name='input_features'), # Named input layer\n",
    "        Dense(128, activation='relu', name='dense_1'), # Example layer sizes - EXPERIMENT!\n",
    "        Dropout(0.3, name='dropout_1'), # Example dropout - EXPERIMENT!\n",
    "        Dense(64, activation='relu', name='dense_2'),\n",
    "        Dropout(0.3, name='dropout_2'),\n",
    "        Dense(32, activation='relu', name='dense_3'),\n",
    "        Dropout(0.2, name='dropout_3'),\n",
    "        # Add more layers or neurons as needed, but balance with data size\n",
    "        Dense(1, activation='sigmoid' if TASK_TYPE == 'classification' else None, name='output_layer') # Sigmoid for binary classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# --- Helper Function to Load Data and Split ---\n",
    "# Assumes data is normalized and DOES NOT use StandardScaler.\n",
    "# Assumes the first column is an ID and the last column is the label.\n",
    "def load_data_and_split(file_path, test_size, random_state, task_type):\n",
    "    \"\"\"\n",
    "    Loads data from CSV, separates features and labels, and splits into train/test sets.\n",
    "    Assumes the first column is an ID and the last column is the label.\n",
    "    Assumes data is already normalized and DOES NOT use StandardScaler.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        task_type (str): 'classification' or 'regression'.\n",
    "\n",
    "    Returns:\n",
    "        tuple or (None, None, None, None): Tuple of (X_train, X_test, y_train, y_test)\n",
    "                                             or None if loading fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {file_path}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # --- CRITICAL FIX for Non-Numeric Columns and Indexing ---\n",
    "    # Assuming the first column (index 0) is an ID column\n",
    "    # and the last column is the numerical label.\n",
    "    # Select features from the second column (index 1) up to, but NOT including, the last column.\n",
    "    # If your files have a different structure, you MUST adjust these lines accordingly.\n",
    "    try:\n",
    "        # Check if the file has at least 2 columns (features + label) or 3 (ID + features + label)\n",
    "        if data.shape[1] < 2:\n",
    "             print(f\"Error: File {file_path} does not have enough columns (expected at least 1 feature + Label).\")\n",
    "             return None, None, None, None\n",
    "\n",
    "        # Attempt to slice assuming ID is first and Label is last\n",
    "        X = data.iloc[:, 1:-1].values # Features exclude the first column (index 0)\n",
    "        y = data.iloc[:, -1].values  # Label is the last column\n",
    "\n",
    "        # Check if slicing resulted in 0 features unexpectedly\n",
    "        if X.shape[1] == 0 and data.shape[1] > 1: # If > 1 total columns but 0 features after slicing\n",
    "             print(f\"Error: iloc[:, 1:-1] resulted in 0 features for {file_path}. Check your column structure.\")\n",
    "             print(f\"File has {data.shape[1]} columns. If ID is first and Label last, expected {data.shape[1]-2} features.\")\n",
    "             print(\"Check if the first column is indeed ID and the last is Label, and if there are features in between.\")\n",
    "             # Suggest inspecting data.columns and data.head()\n",
    "             return None, None, None, None # Still return None as data loading failed logic check\n",
    "\n",
    "\n",
    "        print(f\"Loaded raw data from {file_path}. X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "        # Optional check: Verify the feature matrix contains only numeric types\n",
    "        if not np.issubdtype(X.dtype, np.number):\n",
    "             print(f\"Error: Feature matrix from {file_path} still contains non-numeric data after slicing. Data type: {X.dtype}\")\n",
    "             print(\"Please inspect this CSV file for non-numeric values in columns other than the first and last.\")\n",
    "             return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"Error: Indexing failed for {file_path}. Check if your file has enough columns (at least ID, Feature(s), Label). Minimum 2 columns total if no ID, 3 if ID is first.\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during loading {file_path}: {e}\")\n",
    "        return None, None, None, None # Return None to stop execution\n",
    "\n",
    "    # --- END CRITICAL FIX ---\n",
    "\n",
    "    # If classification and labels are not 0/1, convert them\n",
    "    if task_type == 'classification' and np.unique(y).size == 2 and not all(np.isin(y, [0, 1])):\n",
    "         print(f\"Warning: Assuming binary classification, converting labels {np.unique(y)} to 0/1.\")\n",
    "         unique_labels = np.unique(y)\n",
    "         label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "         y = np.array([label_map[label] for label in y])\n",
    "\n",
    "    # Split data, using stratification for classification to maintain label distribution\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y if task_type == 'classification' else None\n",
    "    )\n",
    "\n",
    "    print(f\"Split data from {file_path}. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# --- Function to Apply Upsampling ---\n",
    "def apply_upsampling(X_train, y_train, sampling_strategy='auto'):\n",
    "    \"\"\"\n",
    "    Applies RandomOverSampler upsampling to the training data.\n",
    "    This should ONLY be applied AFTER the train/test split.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training features.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        sampling_strategy (str or dict): Sampling strategy for RandomOverSampler. 'auto' balances classes.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): Upsampled training features and labels.\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying RandomOverSampler upsampling to the training data (strategy: {sampling_strategy})...\")\n",
    "    print(f\"Original training shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    if len(np.unique(y_train)) > 1:\n",
    "        print(f\"Original class distribution: {np.bincount(y_train)}\")\n",
    "    else:\n",
    "        print(\"Warning: Only one class found in training data. Upsampling is not applicable.\")\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "    # Initialize the upsampler\n",
    "    ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Fit and resample the training data\n",
    "    X_train_upsampled, y_train_upsampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(f\"Upsampled training shape: X={X_train_upsampled.shape}, y={y_train_upsampled.shape}\")\n",
    "    if len(np.unique(y_train_upsampled)) > 1:\n",
    "         print(f\"Upsampled class distribution: {np.bincount(y_train_upsampled)}\")\n",
    "    else:\n",
    "         print(\"Warning: Only one class found after upsampling. Check your data.\")\n",
    "\n",
    "    print(\"Upsampling finished.\")\n",
    "\n",
    "    return X_train_upsampled, y_train_upsampled\n",
    "\n",
    "\n",
    "# --- Text Summary of Training History ---\n",
    "def print_training_summary(history, model_name, task_type):\n",
    "    \"\"\"Prints a text summary of the training history.\"\"\"\n",
    "    print(f\"\\nSummary for {model_name}:\")\n",
    "    epochs_trained = len(history.history.get('loss', [])) # Use .get for safety\n",
    "\n",
    "    if epochs_trained > 0:\n",
    "        final_train_loss = history.history['loss'][-1]\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        metric_name = 'accuracy' if task_type == 'classification' else 'mae'\n",
    "        final_train_metric = history.history.get(metric_name, [np.nan])[-1] # Use .get with fallback\n",
    "        final_val_metric = history.history.get(f'val_{metric_name}', [np.nan])[-1] # Use .get with fallback\n",
    "\n",
    "        print(f\"  Epochs Trained: {epochs_trained}\")\n",
    "        print(f\"  Final Training Loss: {final_train_loss:.4f}\")\n",
    "        print(f\"  Final Validation Loss: {final_val_loss:.4f}\")\n",
    "        print(f\"  Final Training {metric_name.capitalize()}: {final_train_metric:.4f}\")\n",
    "        print(f\"  Final Validation {metric_name.capitalize()}: {final_val_metric:.4f}\")\n",
    "        print(\"  (Note: Final metrics shown are for the last epoch run. Evaluate results below reflect best epoch if Early Stopping restored weights.)\")\n",
    "    else:\n",
    "        print(f\"  No epochs were trained for {model_name}. Training likely failed early.\")\n",
    "    print(f\"--- End of {model_name} Summary ---\")\n",
    "\n",
    "\n",
    "# --- Plotting History ---\n",
    "def plot_training_history(history, model_name, task_type):\n",
    "    \"\"\"Plots training and validation loss and metrics.\"\"\"\n",
    "    print(f\"\\n--- Plotting Training History for {model_name} ---\")\n",
    "    if not history.history.get('loss', []):\n",
    "        print(\"  No history data to plot.\")\n",
    "        return\n",
    "\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    metric_name = 'accuracy' if task_type == 'classification' else 'mae'\n",
    "    train_metric = history.history.get(metric_name, None) # Use .get, can be None if metric not tracked\n",
    "    val_metric = history.history.get(f'val_{metric_name}', None)\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, 'bo-', label='Training loss', alpha=0.6)\n",
    "    plt.plot(epochs, val_loss, 'b-', label='Validation loss')\n",
    "    plt.title(f'{model_name}: Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Primary Metric (Accuracy or MAE)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if train_metric is not None and val_metric is not None:\n",
    "        plt.plot(epochs, train_metric, 'bo-', label=f'Training {metric_name.capitalize()}', alpha=0.6)\n",
    "        plt.plot(epochs, val_metric, 'b-', label=f'Validation {metric_name.capitalize()}')\n",
    "        plt.title(f'{model_name}: Training and validation {metric_name.capitalize()}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric_name.capitalize())\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, f\"Metric '{metric_name}' not tracked during training.\",\n",
    "                 horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "        plt.title(f'{model_name}: {metric_name.capitalize()} over epochs')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show() # Keep commented out if plotting multiple models\n",
    "\n",
    "\n",
    "# --- Train Model 1 (Classical MLP on MRI-only data - NO UPSAMPLING) ---\n",
    "\n",
    "print(\"--- Training Model 1 (Classical MLP) on {} ({} Features) ---\".format(os.path.basename(MRI_DATA_PATH), 677))\n",
    "\n",
    "# Load and split MRI-only data\n",
    "X_train_mri, X_test_mri, y_train_mri, y_test_mri = load_data_and_split(\n",
    "    MRI_DATA_PATH, TEST_SIZE_1, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri is None:\n",
    "    exit()\n",
    "\n",
    "# Ensure input shape matches expectation or use loaded shape\n",
    "input_shape_mri = X_train_mri.shape[1]\n",
    "expected_shape_mri = 677 # Based on previous runs\n",
    "if input_shape_mri != expected_shape_mri:\n",
    "    print(f\"Warning: MRI-only data shape is {input_shape_mri}, expected {expected_shape_mri}. Using loaded shape.\")\n",
    "    # If this is significantly different, check data loading logic again\n",
    "\n",
    "\n",
    "print(f\"Input shape for Model 1 (Classical MLP): {input_shape_mri}\")\n",
    "\n",
    "# Initialize Model 1 (Classical MLP)\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_1) # Set seed for reproducibility\n",
    "model_mri_classical = create_classical_mlp_model(input_shape_mri)\n",
    "\n",
    "\n",
    "model_mri_classical.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                      loss='binary_crossentropy' if TASK_TYPE == 'classification' else 'mean_squared_error',\n",
    "                      metrics=['accuracy'] if TASK_TYPE == 'classification' else ['mae'])\n",
    "model_mri_classical.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 1 (Classical MLP)\n",
    "early_stopping_mri_classical = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 1 (Classical MLP) training.\")\n",
    "\n",
    "\n",
    "print(f\"Fitting Model 1 (Classical MLP) on MRI-only dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "# --- Training Step ---\n",
    "history_mri_classical = model_mri_classical.fit(X_train_mri, y_train_mri,\n",
    "                                    epochs=MAX_EPOCHS,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    validation_data=(X_test_mri, y_test_mri),\n",
    "                                    callbacks=[early_stopping_mri_classical],\n",
    "                                    verbose=1)\n",
    "print(\"Model 1 (Classical MLP) training finished.\")\n",
    "\n",
    "# Save Model 1 (Classical MLP) - Recommend saving in .keras format\n",
    "model_mri_classical.save(MODEL_MRI_SAVE_PATH_CLASSICAL)\n",
    "print(f\"Model 1 (Classical MLP) saved as {MODEL_MRI_SAVE_PATH_CLASSICAL}\")\n",
    "\n",
    "# Evaluate Model 1 (Classical MLP) on its test set\n",
    "print(\"\\nEvaluating Model 1 (Classical MLP) on MRI-only test set:\")\n",
    "eval_results_mri_classical = model_mri_classical.evaluate(X_test_mri, y_test_mri, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 1 (Classical MLP, MRI-only) Test Loss: {eval_results_mri_classical[0]:.4f}, Accuracy: {eval_results_mri_classical[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 1 (Classical MLP, MRI-only) Test Loss (MSE): {eval_results_mri_classical[0]:.4f}, Test MAE: {eval_results_mri_classical[1]:.4f}\")\n",
    "\n",
    "# Print and Plot Summary for Model 1\n",
    "print_training_summary(history_mri_classical, \"Model 1 (Classical MLP on MRI-only)\", TASK_TYPE)\n",
    "\n",
    "\n",
    "# --- Train Model 2 (Classical MLP on MRI-PET data - WITH UPSAMPLING) ---\n",
    "\n",
    "print(\"\\n--- Training Model 2 (Classical MLP) on {} ({} Features) - WITH UPSAMPLING ---\".format(os.path.basename(MRI_PET_DATA_PATH), 263))\n",
    "\n",
    "# Load and split MRI-PET data\n",
    "X_train_mri_pet_raw, X_test_mri_pet_raw, y_train_mri_pet_raw, y_test_mri_pet_raw = load_data_and_split(\n",
    "    MRI_PET_DATA_PATH, TEST_SIZE_2, RANDOM_STATE, TASK_TYPE\n",
    ")\n",
    "\n",
    "if X_train_mri_pet_raw is None:\n",
    "    exit()\n",
    "\n",
    "# --- Apply Upsampling to MRI-PET Training Data (AFTER SPLIT) ---\n",
    "# This is the correct place to apply upsampling.\n",
    "# Note: Upsampling is ONLY applied to the training subset.\n",
    "# sampling_strategy='auto' will balance the classes (make count of minority == count of majority)\n",
    "X_train_mri_pet_upsampled, y_train_mri_pet_upsampled = apply_upsampling(\n",
    "    X_train_mri_pet_raw, y_train_mri_pet_raw, sampling_strategy='auto'\n",
    ")\n",
    "# Note: We train on the upsampled data but validate/evaluate on the original test data\n",
    "\n",
    "# Ensure input shape matches expectation or use loaded shape\n",
    "input_shape_mri_pet = X_train_mri_pet_raw.shape[1]\n",
    "expected_shape_mri_pet = 263 # Based on previous runs\n",
    "if input_shape_mri_pet != expected_shape_mri_pet:\n",
    "    print(f\"Warning: MRI-PET data shape is {input_shape_mri_pet}, expected {expected_shape_mri_pet}. Using loaded shape.\")\n",
    "     # If this is significantly different, check data loading logic again\n",
    "\n",
    "\n",
    "print(f\"Input shape for Model 2 (Classical MLP): {input_shape_mri_pet}\")\n",
    "\n",
    "# Initialize Model 2 (Classical MLP) - Input shape based on original features\n",
    "tf.random.set_seed(RANDOM_STATE_MODEL_2) # Set a different seed\n",
    "model_mri_pet_classical = create_classical_mlp_model(input_shape_mri_pet)\n",
    "\n",
    "\n",
    "model_mri_pet_classical.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                          loss='binary_crossentropy' if TASK_TYPE == 'classification' else 'mean_squared_error',\n",
    "                          metrics=['accuracy'] if TASK_TYPE == 'classification' else ['mae'])\n",
    "model_mri_pet_classical.summary()\n",
    "\n",
    "# Set up Early Stopping for Model 2 (Classical MLP)\n",
    "early_stopping_mri_pet_classical = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "print(f\"\\nUsing Early Stopping with patience={EARLY_STOPPING_PATIENCE} monitoring 'val_loss' for Model 2 (Classical MLP) training.\")\n",
    "\n",
    "print(f\"Fitting Model 2 (Classical MLP) on upsampled MRI-PET dataset (max {MAX_EPOCHS} epochs)...\")\n",
    "# --- Training Step ---\n",
    "# Train using the upsampled data\n",
    "history_mri_pet_classical = model_mri_pet_classical.fit(X_train_mri_pet_upsampled, y_train_mri_pet_upsampled,\n",
    "                                           epochs=MAX_EPOCHS,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           validation_data=(X_test_mri_pet_raw, y_test_mri_pet_raw), # VALIDATE ON ORIGINAL TEST DATA\n",
    "                                           callbacks=[early_stopping_mri_pet_classical],\n",
    "                                           verbose=1)\n",
    "print(\"Model 2 (Classical MLP) training finished.\")\n",
    "\n",
    "# Save Model 2 (Classical MLP) - Recommend saving in .keras format\n",
    "model_mri_pet_classical.save(MODEL_MRI_PET_SAVE_PATH_CLASSICAL)\n",
    "print(f\"Model 2 (Classical MLP) saved as {MODEL_MRI_PET_SAVE_PATH_CLASSICAL}\")\n",
    "\n",
    "# Evaluate Model 2 (Classical MLP) on its original test set\n",
    "print(\"\\nEvaluating Model 2 (Classical MLP) on original MRI-PET test set:\")\n",
    "eval_results_mri_pet_classical = model_mri_pet_classical.evaluate(X_test_mri_pet_raw, y_test_mri_pet_raw, verbose=0)\n",
    "if TASK_TYPE == 'classification':\n",
    "    print(f\"Model 2 (Classical MLP, MRI-PET) Test Loss: {eval_results_mri_pet_classical[0]:.4f}, Accuracy: {eval_results_mri_pet_classical[1]:.4f}\")\n",
    "else: # Regression\n",
    "    print(f\"Model 2 (Classical MLP, MRI-PET) Test Loss (MSE): {eval_results_mri_pet_classical[0]:.4f}, Test MAE: {eval_results_mri_pet_classical[1]:.4f}\")\n",
    "\n",
    "# Print and Plot Summary for Model 2\n",
    "print_training_summary(history_mri_pet_classical, \"Model 2 (Classical MLP on MRI-PET)\", TASK_TYPE)\n",
    "\n",
    "\n",
    "# --- Ensembling Predictions ---\n",
    "print(\"\\n--- Ensembling Classical MLP Predictions ---\")\n",
    "print(\"Classical MLP models trained independently on datasets with different feature counts.\")\n",
    "\n",
    "print(\"\\nTo ensemble these classical models for a final prediction on a new sample, you would need:\")\n",
    "print(\"1. The {} feature representation of the new sample (processed like MRI-only data).\".format(input_shape_mri))\n",
    "print(\"2. The {} feature representation of the new sample (processed like MRI-PET data).\".format(input_shape_mri_pet))\n",
    "print(\"3. Load Model 1 Classical MLP (from {}) and feed it the {} features.\".format(MODEL_MRI_SAVE_PATH_CLASSICAL, input_shape_mri))\n",
    "print(\"4. Load Model 2 Classical MLP (from {}) and feed it the {} features.\".format(MODEL_MRI_PET_SAVE_PATH_CLASSICAL, input_shape_mri_pet))\n",
    "print(\"5. Combine their predicted probabilities (e.g., average) to get the final ensemble prediction.\")\n",
    "\n",
    "# Example of getting predictions on their respective test sets:\n",
    "print(\"\\nExample: Getting predictions on respective test sets:\")\n",
    "# --- Prediction Steps ---\n",
    "# Load models here if they weren't saved/loaded earlier in this script execution lifecycle\n",
    "# model_mri_classical_loaded = tf.keras.models.load_model(MODEL_MRI_SAVE_PATH_CLASSICAL) # Load if needed\n",
    "# model_mri_pet_classical_loaded = tf.keras.models.load_model(MODEL_MRI_PET_SAVE_PATH_CLASSICAL) # Load if needed\n",
    "\n",
    "# Use the trained models directly from memory if script runs end-to-end\n",
    "mri_only_test_predictions_classical = model_mri_classical.predict(X_test_mri)\n",
    "mri_pet_test_predictions_classical = model_mri_pet_classical.predict(X_test_mri_pet_raw) # Predict using the model trained earlier (use original test data)\n",
    "\n",
    "print(f\"Predictions from Model 1 (Classical MLP, MRI-only) on its test set (shape: {mri_only_test_predictions_classical.shape})\")\n",
    "print(f\"Predictions from Model 2 (Classical MLP, MRI-PET) on its ORIGINAL test set (shape: {mri_pet_test_predictions_classical.shape})\")\n",
    "\n",
    "# Note: You cannot directly average these predictions for a single ensemble score\n",
    "# because they are from different test sets (disjoint samples).\n",
    "\n",
    "# --- Show all plots ---\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScript execution finished.\")\n",
    "print(\"Two Classical MLP models trained and saved for ensembling based on their respective datasets, with upsampling on the MRI-PET training data.\")\n",
    "print(\"\\nThis provides a stable classical baseline.\")\n",
    "print(\"To test the ensemble's combined performance, you need a separate script that loads these saved models\")\n",
    "print(\"and evaluates their combined prediction on a UNIFIED test dataset where each sample has both MRI and PET features.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
